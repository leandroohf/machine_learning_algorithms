{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Finder\n",
    "\n",
    "**TODO:** FInidh this study later\n",
    "\n",
    "refs:\n",
    "\n",
    "1. How reliable: https://blog.dataiku.com/the-learning-rate-finder-technique-how-reliable-is-it\n",
    "1. https://fastai1.fast.ai/callbacks.lr_finder.html\n",
    "1. Good discussion: https://forums.fast.ai/t/new-lr-finder-output/89236/3\n",
    "1. colab: https://walkwithfastai.com/lr_finder\n",
    "1. Small discussion regards implementation: https://aidancoco.medium.com/optimizing-the-learning-rate-of-your-neural-networks-32e38addd8a3\n",
    "\n",
    "**GOAL**\n",
    "\n",
    "* Understand learning rate finder better.\n",
    "* Why I can choose large values of lr than the lrf reccomendation?\n",
    "* Is this behavor also on fastai or tensorflow?\n",
    "* Implement increase by mini-batch and not epoch as mentioned in one of the ref above. DOes this make any different and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# required for iteractive plot\n",
    "%matplotlib notebook  \n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats \n",
    "\n",
    "from sklearn.datasets import make_classification, make_blobs, make_regression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation of learning rate finder\n",
    "\n",
    "https://github.com/WittmannF/LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self, min_lr, max_lr, mom=0.9, stop_multiplier=None, \n",
    "                 reload_weights=True, batches_lr_update=5):\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.mom = mom\n",
    "        self.reload_weights = reload_weights\n",
    "        self.batches_lr_update = batches_lr_update\n",
    "        if stop_multiplier is None:\n",
    "            self.stop_multiplier = -20*self.mom/3 + 10 # 4 if mom=0.9\n",
    "                                                       # 10 if mom=0\n",
    "        else:\n",
    "            self.stop_multiplier = stop_multiplier\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        p = self.params\n",
    "        try:\n",
    "            n_iterations = p['epochs']*p['samples']//p['batch_size']\n",
    "        except:\n",
    "            n_iterations = p['steps']*p['epochs']\n",
    "            \n",
    "        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n",
    "                                           num=n_iterations//self.batches_lr_update+1)\n",
    "        self.losses=[]\n",
    "        self.iteration=0\n",
    "        self.best_loss=0\n",
    "        if self.reload_weights:\n",
    "            self.model.save_weights('tmp.hdf5')\n",
    "        \n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "        \n",
    "        if self.iteration!=0: # Make loss smoother using momentum\n",
    "            loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n",
    "        \n",
    "        if self.iteration==0 or loss < self.best_loss: \n",
    "                self.best_loss = loss\n",
    "                \n",
    "        if self.iteration%self.batches_lr_update==0: # Evaluate each lr over 5 epochs\n",
    "            \n",
    "            if self.reload_weights:\n",
    "                self.model.load_weights('tmp.hdf5')\n",
    "          \n",
    "            lr = self.learning_rates[self.iteration//self.batches_lr_update]            \n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "            self.losses.append(loss)            \n",
    "\n",
    "        if loss > self.best_loss*self.stop_multiplier: # Stop criteria\n",
    "            self.model.stop_training = True\n",
    "                \n",
    "        self.iteration += 1\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.reload_weights:\n",
    "                self.model.load_weights('tmp.hdf5')\n",
    "                \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.learning_rates[:len(self.losses)], self.losses)\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xscale('log')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# 1. Input Data\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "mean, std = X_train.mean(), X_train.std()\n",
    "X_train, X_test = (X_train-mean)/std, (X_test-mean)/std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5263c551fab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mlr_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_finder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1162\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m       \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m       \u001b[0;31m# Handle fault-tolerance for multi-worker.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7fd4c7bd2c78>\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m   2211\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0munknown\u001b[0m \u001b[0mformat\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m     \"\"\"\n\u001b[0;32m-> 2213\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2214\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m     \u001b[0mfilepath_is_h5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;31m# When the graph has not been initialized, use the Model's implementation to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;31m# to check if the weights has been created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=bad-super-call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2669\u001b[0m       \u001b[0;31m# been invoked yet, this will cover both sequential and subclass model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m       \u001b[0;31m# Also make sure to exclude Model class itself which has build() defined.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m       raise ValueError('Weights for model %s have not yet been created. '\n\u001b[0m\u001b[1;32m   2672\u001b[0m                        \u001b[0;34m'Weights are created when the Model is first called on '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m                        \u001b[0;34m'inputs or `build()` is called with an `input_shape`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#                 tf.keras.layers.Dense(1, activation='relu',\n",
    "#                           kernel_initializer=W01,\n",
    "#                           bias_initializer=b01,\n",
    "#                           input_shape=(2,))\n",
    "#     ])\n",
    "\n",
    "# sgd = tf.keras.optimizers.SGD(learning_rate=lr, \n",
    "#                               momentum=0.0,\n",
    "#                               nesterov=False, name='SGD'\n",
    "#                              )\n",
    "\n",
    "\n",
    "# model.compile(loss='mse', optimizer=sgd, metrics=['mse'])\n",
    "\n",
    "# _ = model.fit(X, y, epochs=n_iter, batch_size=bs, verbose=0, validation_split=0.0)\n",
    "\n",
    "\n",
    "# 2. Define and Compile Model\n",
    "model = tf.keras.Sequential([tf.keras.layers.Flatten(),\n",
    "                    tf.keras.layers.Dense(512, activation='relu'),\n",
    "                    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "\n",
    "# 3. Fit using Callback\n",
    "lr_finder = LRFinder(min_lr=0.001, max_lr=1)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, callbacks=[lr_finder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cots functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linera regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: matrix nxd\n",
    "# y: column vector nx1\n",
    "# theta: column vector dx1\n",
    "\n",
    "def cost_func_linear_regression(theta: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # error: pred - y\n",
    "    z = np.matmul(X,theta) - y\n",
    "\n",
    "    return np.matmul(z.T,z)/2.0/n  # <== average of all loss MSE\n",
    "\n",
    "def gradient_linear_regression(theta: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # error: pred - y\n",
    "    z = np.matmul(X,theta) - y    \n",
    "    grad = np.matmul(X.T,z)\n",
    "        \n",
    "    return grad/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Sigmoid activation\n",
    "\n",
    "* vectorized formula of the cost function: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "\n",
    "* Cost function is also calles **Cross-entropy** or **log loss**\n",
    "\n",
    "**TODO:** Try to find a demonstartion of the vectorial version of the gradient using matrix derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# X: matrix nxd\n",
    "# y: column vector nx1\n",
    "# theta: column vector dx1\n",
    "\n",
    "def cost_func_logistic_regression(theta: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    z = np.matmul(X,theta)  # nx1\n",
    "    h = 1.0/(1.0 + np.exp(-z))  # activation: nx1\n",
    "    \n",
    "    cost = (-1.0/n)*(np.matmul(y.T,np.log(h)) + np.matmul(1-y.T,np.log(1-h))) # scalar\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def gradient_logistic_regression(theta: np.array, X: np.array, y: np.array)-> np.array:\n",
    "    \n",
    "    z = np.matmul(X,theta)\n",
    "    \n",
    "    # pred\n",
    "    h = 1.0/(1.0 + np.exp(-z))  # activation\n",
    "    \n",
    "    return np.matmul(X.T,h - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_2d_linear_data(slope: float, intercept: float, noise: float, n_sanples: int, random: int = 2021):\n",
    "    \n",
    "    x = np.linspace(-1.0, 1.0, n).reshape(n,1)\n",
    "    \n",
    "    np.random.seed(random)\n",
    "    e = np.random.normal(0.0,noise,n).reshape(n,1)\n",
    "    \n",
    "    y = slope*x + intercept + e\n",
    "    y = y.reshape(n,)\n",
    "\n",
    "    _ones = np.ones((n,1))\n",
    "\n",
    "    x.shape\n",
    "\n",
    "    _ones.shape\n",
    "\n",
    "    X = np.hstack((_ones, x))\n",
    "    \n",
    "    return X,y, e\n",
    "\n",
    "slope = 5.0 \n",
    "intercept = 4.0\n",
    "noise = 0.1\n",
    "n = 50\n",
    "\n",
    "X,y, e = make_2d_linear_data(slope,intercept, noise, n)\n",
    "theta_true = np.array([intercept,slope])\n",
    "\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "X[0:3]\n",
    "\n",
    "print(f\"True parameters: {theta_true}\")\n",
    "bayes_error = np.sum(np.matmul(e.T,e))/n  # <== mse\n",
    "print(f\"bayes error: {bayes_error:.2f}\")\n",
    "\n",
    "_ = plt.subplot(121);\n",
    "_ = plt.scatter(X[:,1],y);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUNING Learning rate \n",
    "\n",
    "https://miguel-data-sc.github.io/2017-11-05-first/\n",
    "https://arxiv.org/abs/1506.01186\n",
    "\n",
    "1. learning rate finding \n",
    "1. learnig rate find vs batchs size\n",
    "\n",
    "Needs to introduce learning rate shdedule $\\eta = \\eta(t)$ in mini batch algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LearningRateScheduler(ABC):\n",
    "\n",
    "    def __init__(self, lr0: float ):\n",
    "        \n",
    "        self._lr0 = lr0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def run(it: int) -> float: \n",
    "        return\n",
    "    \n",
    "class ConstLearningRateScheduler(LearningRateScheduler):\n",
    "    \n",
    "    def run(self, it: int) -> float: \n",
    "        return self._lr0\n",
    "    \n",
    "class FindLearningRateScheduler(LearningRateScheduler):\n",
    "    \n",
    "    def __init__(self, lr0: float, number_of_lr_per_order: int = 4 ):\n",
    "    \n",
    "        super().__init__(lr0)\n",
    "        \n",
    "        self._lr0 = lr0\n",
    "        self._number_of_lr_per_order = number_of_lr_per_order\n",
    "    \n",
    "    def run(self, it: int) -> float: \n",
    "        \n",
    "        lr = self._lr0*(10**(it/self._number_of_lr_per_order))\n",
    "        \n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ConstLearningRateScheduler(0.01)\n",
    "\n",
    "lr_scheduler.run(10)\n",
    "lr_scheduler.run(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 55\n",
    "\n",
    "lr_scheduler = FindLearningRateScheduler(1e-5,number_of_lr_per_order=10)\n",
    "\n",
    "lrs = np.zeros((n_iter,))\n",
    "for it in range(n_iter):\n",
    "    \n",
    "    lrs[it] = lr_scheduler.run(it)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2);\n",
    "fig.suptitle('learning rate scheduler');\n",
    "ax1.plot(range(n_iter),lrs,'b',label='');\n",
    "ax1.set_yscale('log');\n",
    "ax1.grid(True);\n",
    "ax1.set_title('log scale');\n",
    "ax2.plot(range(n_iter),lrs,'b',label='learning rate scheduler');\n",
    "ax2.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(data_shuffle, batch_size):\n",
    "    \n",
    "    n = data_shuffle.shape[0]\n",
    "    mini_batches = []\n",
    "    \n",
    "    for k in np.arange(0, n, batch_size):\n",
    "        \n",
    "        X_mini = data_shuffle[k:k+batch_size,:-1]\n",
    "        y_mini = data_shuffle[k:k+batch_size,-1]\n",
    "        \n",
    "        mini_batches.append((X_mini,y_mini))\n",
    "    return mini_batches\n",
    "\n",
    "def mini_batch_gradient_descent(X,y, cost_func, gradient_func, \n",
    "                                learning_rate_scheduler: Callable[[float,int], float],\n",
    "                                n_iterations: int,\n",
    "                                batch_size: int, \n",
    "                                init_guess: np.array,seed: int =1):\n",
    "    \n",
    "    n = X.shape[0] # training size\n",
    "    d = X.shape[1]  # number of dimensions or paramters\n",
    "    \n",
    "    theta = init_guess\n",
    "\n",
    "    Xy = np.hstack((X,y.reshape((n,1))))\n",
    "        \n",
    "    theta_history = []\n",
    "    grad_history = []\n",
    "    cost_history = []\n",
    " \n",
    "    np.random.seed(seed)\n",
    "    for it in range(0,n_iterations):\n",
    "           \n",
    "        data_shuffle = Xy.copy()\n",
    "        np.random.shuffle(data_shuffle)\n",
    "        \n",
    "        cost = 0.0\n",
    "        mini_batches = create_mini_batches(data_shuffle,batch_size)\n",
    "        \n",
    "        for X_mini, y_mini in mini_batches:\n",
    "\n",
    "            # compute the gradients and cost function\n",
    "            cost += cost_func(theta,X_mini,y_mini)\n",
    "            gradient_at_theta = gradient_func(theta,X_mini,y_mini)  # gradient approximation using 1 data point\n",
    "            \n",
    "\n",
    "            # log history for oploting\n",
    "            theta_history.append(theta.T)\n",
    "            grad_history.append(gradient_at_theta)\n",
    "            \n",
    "           # update parameters\n",
    "            learning_rate = learning_rate_scheduler.run(it)\n",
    "            dtheta = -learning_rate* gradient_at_theta\n",
    "\n",
    "            theta = theta + dtheta\n",
    "            \n",
    "        cost_history.append(cost/len(mini_batches)) # << average loss in the epoch \n",
    "\n",
    "    history = {'theta': theta_history, \"cost\": cost_history, \"grad\": grad_history}\n",
    "\n",
    "    return theta, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = 5.0 \n",
    "intercept = 4.0\n",
    "noise = 0.1\n",
    "n = 300\n",
    "\n",
    "X,y,_ = make_2d_linear_data(slope,intercept, noise, n)\n",
    "theta_true = np.array([intercept,slope])\n",
    "\n",
    "print(f\"True parameters: {theta_true}\")\n",
    "bayes_error = np.sum(np.matmul(e.T,e))/n  # <== mse\n",
    "print(f\"bayes error: {bayes_error:.2f}\")\n",
    "\n",
    "_ = plt.subplot(121);\n",
    "_ = plt.scatter(X[:,1],y);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 53\n",
    "bs = 18\n",
    "\n",
    "lr_scheduler = FindLearningRateScheduler(1e-6,number_of_lr_per_order=8)\n",
    "\n",
    "init_guess = np.array([1.0,0.5])\n",
    "\n",
    "\n",
    "theta, history = mini_batch_gradient_descent(X, y, cost_func_linear_regression, gradient_linear_regression, \n",
    "                                             lr_scheduler, n_iter,bs, init_guess)\n",
    "\n",
    "\n",
    "\n",
    "theta0 = history['theta'][0]\n",
    "grad0 =  history['grad'][0]\n",
    "cost0 = history['cost'][0]\n",
    "\n",
    "print(f\"Initial Guess it: 0 => theta: {theta0}; grad: {grad0}; cost: {cost0:.2f}\")\n",
    "\n",
    "it = n_iter -1\n",
    "theta = history['theta'][-1]\n",
    "grad =  history['grad'][-1]\n",
    "cost = history['cost'][-1]\n",
    "\n",
    "print(f\"it: {it} => theta: {theta}; grad: {grad}; cost: {cost:.2f}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.zeros((n_iter,))\n",
    "for it in range(n_iter):\n",
    "    \n",
    "    lrs[it] = lr_scheduler.run(it)\n",
    "\n",
    "cost = history['cost'] \n",
    "\n",
    "plt.subplots()\n",
    "plt.plot(lrs,cost,'b',label='cost');\n",
    "plt.xscale('log');\n",
    "plt.ylim(0.0,8.5);\n",
    "plt.legend();\n",
    "plt.title('Learning Rate Finder');\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W01 = tf.constant_initializer(0.5)\n",
    "b01 = tf.constant_initializer(1.0)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(1, activation='relu',\n",
    "                          kernel_initializer=W01,\n",
    "                          bias_initializer=b01,\n",
    "                          input_shape=(2,))\n",
    "    ])\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=lr, \n",
    "                              momentum=0.0,\n",
    "                              nesterov=False, name='SGD'\n",
    "                             )\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer=sgd, metrics=['mse'])\n",
    "\n",
    "#_ = model.fit(X, y, epochs=n_iter, batch_size=bs, verbose=0, validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit using Callback\n",
    "lr_finder = LRFinder(min_lr=1e-14, max_lr=0.001)\n",
    "\n",
    "\n",
    "model.fit(X, y, batch_size=bs, callbacks=[lr_finder], epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other options are also available\n",
    "X, y = make_classification(\n",
    "    n_samples=100, \n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    n_features=2,\n",
    "    n_redundant = 0,\n",
    "    n_informative = 2,\n",
    "    n_repeated = 0,\n",
    "    weights=[0.5,0.5], # balanced classes\n",
    "    flip_y=0.001, # add noisy the default value for flip_y is 0.1%\n",
    "    class_sep=1.7, # class_sep is 1.0. The lower the value, the harder classification is.\n",
    "    random_state=2020) \n",
    "\n",
    "ax = sns.scatterplot(X[:,0],X[:,1],hue=y);\n",
    "ax.set_title(\"Dataset\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "n_iter = 200\n",
    "bs = 100\n",
    "\n",
    "lr_scheduler = ConstLearningRateScheduler(lr)\n",
    "\n",
    "init_guess = np.array([1.0,0.5])\n",
    "\n",
    "theta, history = mini_batch_gradient_descent(X, y, cost_func_linear_regression, gradient_linear_regression, \n",
    "                                             lr_scheduler, n_iter,bs, init_guess)\n",
    "\n",
    "\n",
    "theta0 = history['theta'][0]\n",
    "grad0 =  history['grad'][0]\n",
    "cost0 = history['cost'][0]\n",
    "\n",
    "print(f\"Initial Guess it: 0 => theta: {theta0}; grad: {grad0}; cost: {cost0:.2f}\")\n",
    "\n",
    "it = n_iter -1\n",
    "theta = history['theta'][-1]\n",
    "grad =  history['grad'][-1]\n",
    "cost = history['cost'][-1]\n",
    "\n",
    "print(f\"it: {it} => theta: {theta}; grad: {grad}; cost: {cost:.2f}\")\n",
    "print(f\"Expected theta:{theta_true}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = history['cost']\n",
    "    \n",
    "    \n",
    "m = len(cost)\n",
    "x = np.linspace(0,n_iter,m)\n",
    "    \n",
    "fig,ax = plt.subplots()\n",
    "_ = ax.plot(x,cost,'b',label='cost');\n",
    "_ = ax.set_ylabel(\"Cost\")\n",
    "_ = ax.set_ylabel(\"Epoch\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 54\n",
    "bs = 100\n",
    "\n",
    "lr_scheduler = FindLearningRateScheduler(1e-5,number_of_lr_per_order=10)\n",
    "\n",
    "init_guess = np.array([1.0,0.5])\n",
    "\n",
    "theta, history = mini_batch_gradient_descent(X, y, cost_func_linear_regression, gradient_linear_regression, \n",
    "                                             lr_scheduler, n_iter,bs, init_guess)\n",
    "\n",
    "\n",
    "theta0 = history['theta'][0]\n",
    "grad0 =  history['grad'][0]\n",
    "cost0 = history['cost'][0]\n",
    "\n",
    "print(f\"Initial Guess it: 0 => theta: {theta0}; grad: {grad0}; cost: {cost0:.2f}\")\n",
    "\n",
    "it = n_iter -1\n",
    "theta = history['theta'][-1]\n",
    "grad =  history['grad'][-1]\n",
    "cost = history['cost'][-1]\n",
    "\n",
    "print(f\"it: {it} => theta: {theta}; grad: {grad}; cost: {cost:.2f}\")\n",
    "print(f\"Expected theta:{theta_true}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.zeros((n_iter,))\n",
    "for it in range(n_iter):\n",
    "    \n",
    "    lrs[it] = lr_scheduler.run(it)\n",
    "\n",
    "\n",
    "cost = history['cost'] \n",
    "print(f\"first 5 lrs: {lrs[0:5]}\")\n",
    "print(f\"last 5 lrs: {lrs[-5:]}\")\n",
    "print(f\"last 5 cost: {cost[-5:]}\")\n",
    "\n",
    "\n",
    "plt.subplots()\n",
    "plt.plot(lrs,cost,'b',label='cost');\n",
    "plt.xscale('log');\n",
    "plt.ylim(0.0,2.0);\n",
    "plt.legend();\n",
    "plt.title('Learning Rate Finder');\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs[-13:]\n",
    "cost[-13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = [('manual', 0.01),\n",
    "       ('tuned lr',0.2 )]\n",
    "\n",
    "n_iter = 300\n",
    "init_guess\n",
    "bs\n",
    "\n",
    "for name, lr in sim:\n",
    "\n",
    "    lr_scheduler = ConstLearningRateScheduler(lr)\n",
    "    \n",
    "    theta, history[name] = mini_batch_gradient_descent(X, y, cost_func_linear_regression, gradient_linear_regression, \n",
    "                                             lr_scheduler, n_iter,bs, init_guess)\n",
    "\n",
    "\n",
    "    print(f\"{name}, lr: {lr}\")\n",
    "    theta0 = history[name]['theta'][0]\n",
    "    grad0 =  history[name]['grad'][0]\n",
    "    cost0 = history[name]['cost'][0]\n",
    "\n",
    "    print(f\"Initial Guess it: 0 => theta: {theta0}; grad: {grad0}; cost: {cost0:.2f}\")\n",
    "\n",
    "    it = n_iter -1\n",
    "    theta = history[name]['theta'][-1]\n",
    "    grad =  history[name]['grad'][-1]\n",
    "    cost = history[name]['cost'][-1]\n",
    "\n",
    "    print(f\"it: {it} => theta: {theta}; grad: {grad}; cost: {cost:.2f}\")\n",
    "    \n",
    "    z = np.matmul(X,theta)  # nx1\n",
    "    h = 1.0/(1.0 + np.exp(-z))\n",
    "    y_pred = h > 0.5\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    print(f\"accuracy: {acc}\")\n",
    "    \n",
    "    #print(f\"Expected theta:{theta_true}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b','k','g']\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "for idx, hyper_params in enumerate(sim):\n",
    "    \n",
    "    name, lr = hyper_params= hyper_params\n",
    "    \n",
    "    cost = history[name]['cost']\n",
    "    \n",
    "    m = len(cost)\n",
    "    x = np.linspace(0,n_iter,m)\n",
    "    \n",
    "    _ = ax.plot(x,cost,colors[idx],label=name);\n",
    "    \n",
    "#_ = ax.set_ylim([6, 8])\n",
    "_ = ax.set_ylabel(\"Cost\")\n",
    "_ = ax.set_ylabel(\"Epoch\")\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "name": "intro_gradient_algos.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "383.75px",
    "width": "252.222px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201.111px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
