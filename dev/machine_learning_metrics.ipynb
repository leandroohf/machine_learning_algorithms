{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text summarization: ROUGE and BLEU\n",
    "\n",
    "refs:\n",
    "* https://towardsdatascience.com/automatic-text-summarization-evaluation-2e312f66893b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE: Recall-Oriented Understudy for Gisting Evaluation\n",
    "\n",
    "\n",
    "it is a metric to measure the quality of an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. \n",
    "\n",
    "* Aplications:\n",
    "    * text summarization\n",
    "    * language translation\n",
    "    \n",
    "    \n",
    "    \n",
    "$\n",
    "Recall = \\frac{\\text{#share n-grams}}{\\text{#n-grams in reference summary}} \\\\ \n",
    "Precision = \\frac{\\text{#share n-grams}}{\\text{#n-grams in ML summary}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU: BLEU (short for “Bilingual evaluation understudy” \n",
    "\n",
    "\n",
    "“the closer a machine translation is to a professional human translation, the better it is” \n",
    "\n",
    "It was one of the first metrics to claim a high correlation with human judgements of quality and remains one of the most popular automated and inexpensive metrics.\n",
    "\n",
    "\n",
    "$\n",
    "Precision = \\frac{\\text{#share n-grams}}{\\text{#n-grams in ML summary}}\n",
    "$\n",
    "\n",
    "\n",
    "Aplications:\n",
    "* Language translation\n",
    "* BERT [\"Attention is all you need\"](https://arxiv.org/pdf/1706.03762.pdf) (BERT paper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "\n",
    "score = sentence_bleu(reference, candidate)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "references = [[['this', 'is', 'a', 'test'], ['this', 'is' 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "\n",
    "score = corpus_bleu(references, candidates)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with BLEU\n",
    "\n",
    "It doesn’t consider meaning\n",
    "It doesn’t directly consider sentence structure\n",
    "It doesn’t handle morphologically rich languages well\n",
    "It doesn’t map well to human judgements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU and ROUGE are complementary\n",
    "\n",
    "* In both metrics the oreder is not taking \n",
    "\n",
    "* **BLEU measures precision**: how much the words (and/or n-grams) in the machine generated summaries appeared in the human reference summaries.\n",
    "\n",
    "* **ROUGE measures recall**: how much the words (and/or n-grams) in the human reference summaries appeared in the machine generated summaries \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation: METEOR\n",
    "\n",
    "\n",
    "It stands for Metric for Evaluation of Translation with Explicit ORdering, which is a metric for the evaluation of machine translation output.\n",
    "\n",
    "The metric is based on the harmonic mean of unigram precision and recall, with recall weighted higher than precision. The metric was designed to fix some of the problems found in the more popular BLEU metric, and also **\"produce good correlation with human judgement at the sentence or segment level\"**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reccommendation system\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "refs:\n",
    "* https://medium.com/qloo/popular-evaluation-metrics-in-recommender-systems-explained-324ff2fb427d#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6Ijc4M2VjMDMxYzU5ZTExZjI1N2QwZWMxNTcxNGVmNjA3Y2U2YTJhNmYiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MTEyNjUwMzEsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwMTY2NTQwMDY2NjI1NTE2MDUzNSIsImhkIjoiY2FzdGluZ3dvcmtib29rLmNvbSIsImVtYWlsIjoibGVhbmRyb0BjYXN0aW5nd29ya2Jvb2suY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJMZWFuZHJvIE9saXZlaXJhIEZlcm5hbmRlcyIsInBpY3R1cmUiOiJodHRwczovL2xoNC5nb29nbGV1c2VyY29udGVudC5jb20vLVlCSXF0OEt1cEJvL0FBQUFBQUFBQUFJL0FBQUFBQUFBQUFBL0FNWnV1Y202RmZ1VWZ2SFkxajVvWl9BdHUwWFYzWjQwOXcvczk2LWMvcGhvdG8uanBnIiwiZ2l2ZW5fbmFtZSI6IkxlYW5kcm8iLCJmYW1pbHlfbmFtZSI6Ik9saXZlaXJhIEZlcm5hbmRlcyIsImlhdCI6MTYxMTI2NTMzMSwiZXhwIjoxNjExMjY4OTMxLCJqdGkiOiJlYzY1YWFmMjUzZWQyMzAwOGZmOGE2MWI4ZWNlYTIxODEyNjFjNWRkIn0.8JDmGqz7Z-TA3cA3ZhzQ2HTUQrvMWOtPf_1wQ5TBBvYeRvfXHeZuW3OBxPFl53hp-D52D6WUi8-jsozrnVAFzDWP04sQ2YG8OtAUeJU6tIu--6CPZDNFc6DIOMlUlD3IEWpHlOjchtiy10y_KYVwbQ8WrfjwXlkMDIVChz_BqLZRkqqNg4nVZHCUhvOgNfI9BAggmWoOxHqIvLpbkCUybvEixwhJ77AusoB0LbxdcuZlIYpMynMuxyQCUa-tfTli6M3gucWqOtEtkvTXds9LtfKmdpKGu1wNSabcP0SHrhP15RpX7jopG4Yih7U9pC-MgIxG0v_UZiHTUauIK79cJg \n",
    "\n",
    "\n",
    "* https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Recall@k** is defined as the percentage of true labels captured when we recommend k labels per example. Its exact definition is:\n",
    "\n",
    "\n",
    "$\n",
    "\\text{Recall@k} = \\frac{\\text{# true label captured}}{\\text{# true label}}\n",
    "$\n",
    "\n",
    "Ex: \n",
    "\n",
    "\n",
    "* **Precision@k** is defined as the percentage of predictions we get right. Its exact definition is:\n",
    "\n",
    "$\n",
    "\\text{Precision@k} = \\frac{\\text{# true label captured}}{\\text{# prediction made}}\n",
    "$\n",
    "```text\n",
    "true =  1 1 1 0 1\n",
    "pred =  1 0 1 0 1  where 1 is relevant\n",
    "\n",
    "Precision@3 = 2/3; Precision@4 = 2/4; Precision@5 = 3/5\n",
    "\n",
    "Recall@3 = 2/3, Recall@4 = 2/3, and Recall@5 is 3/4.\n",
    "\n",
    "```\n",
    "\n",
    "* **Diversity**: Diversity measures how dissimilar recommended items are for a user. This similarity is often determined using the item’s content (e.g. movie genres) but can also be determined using how similarly items are rated.\n",
    "\n",
    "    * Work with **Cosine Distance** or **Jacacrd Similarity**\n",
    "    \n",
    "* **Coverage**: Coverage represents the percentage of things (items, users, or ratings) that the recommender system was able to recommend. Not being able to predict a particular set of users or items is usually caused by an insufficient number of ratings, and is generally known as the cold start problem\n",
    "\n",
    "$\n",
    "Coverage = \\frac{n}{N}\n",
    "$\n",
    "\n",
    "* **Serendipity**:  It is the measure of how surprising the successful or relevant recommendations are. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
