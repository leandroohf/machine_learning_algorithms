{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "**TODO:** Split notebook:\n",
    "1. backpropagation algorithm\n",
    "1. batch normalization\n",
    "1. intor to initializations on DNN \n",
    "\n",
    "1. Add discussion a bout gradient vanish on deep learning and how relu can help with that: https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/\n",
    "    * intro to grad or\n",
    "    * ds pragamatic tensorflow <== is better lHOw to detect gardient vanish or\n",
    "    * backpropagation\n",
    "    \n",
    "\n",
    "**Goals**:\n",
    "\n",
    "* Goal understand better the link with gradients and backpropagation\n",
    "* Review backpropagation\n",
    "* Implement dense layer and neural netowork\n",
    "* tets implemenattion\n",
    "* Learn batch normalization\n",
    "* IMplement Batch normalization layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import List, Set, Dict, Tuple, Optional, Union\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "\n",
    "Maybe this pdf fprm stanford is more reliable \n",
    "\n",
    "* https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf\n",
    "* https://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture04-neuralnets.pdf\n",
    "\n",
    "--------------\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "* https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n",
    "\n",
    "\n",
    "\n",
    "* Understand the math \n",
    "Watch this video explaining the math behing backpropagation from stanfor:\n",
    "* https://www.youtube.com/watch?v=isPiE-DBagM&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6 (Stanford)\n",
    "\n",
    "\n",
    "Often people confuse back propagation with gradient descent. The gradient descent algorithm minimizes (optmizes) the cost function. And the backpropagation updates all parameters in the neural network based on the chain rule fomula and the gradient returned in the gradient descent algorithm   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about initializations\n",
    "\n",
    "**TODO:**\n",
    "1. Using tensorflow and simple DNN (Peceptron)\n",
    "    * init parameters as zero\n",
    "        1. Linear\n",
    "        1. Relu\n",
    "        1. sigmoid\n",
    "        \n",
    "1. Using tensorflow and simple DNN (keep contant number of neurons to make init simple)\n",
    "    * init large parameters\n",
    "    * init small parameters\n",
    "        * Relu, Linear\n",
    "        * tanh and sigmoid\n",
    "\n",
    "Refs:\n",
    "* He paper: https://arxiv.org/pdf/1502.01852.pdf\n",
    "* with visual explanation to play: \n",
    "    https://www.deeplearning.ai/ai-notes/initialization/#I\n",
    "* https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets <== can help with the code to test all initializations scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. W = 0, b = 0: DNN not learn, Cost funtion $J = cte$\n",
    "\n",
    "1. W = cte, b = 0: Is hard to DNN to learn because all neurons has the same contrubution for the output and the grads will be the same so all neuron learn the same thing always. Neuron evolve symetrically hroughout training,ffectively preventing different neurons from learning different things.\n",
    "\n",
    "1. Case 1: A too-large initialization leads to exploding gradients\n",
    "\n",
    "Consider deeper DNN with 10 Layers all activation beeing **linear**.\n",
    "\n",
    "$\n",
    "W^{[l]} = \\begin{bmatrix}\n",
    "1.5 & 0 \\\\ \n",
    " 0& 1.5\n",
    "\\end{bmatrix} \\\\\n",
    "$\n",
    "\n",
    "for $l = {1,2,...,L}$\n",
    "\n",
    "\n",
    "$\n",
    "\\hat{y} = W^{[L]}1.5^{L-1}x \n",
    "$\n",
    "\n",
    "The activations $a^{l}$ increase exponentially:\n",
    "* In backward propagation, this leads to the exploding gradient \n",
    "* Cost funct $J$ will oscilate around its minimum !? Why?\n",
    "\n",
    "1. Case 2: A too-small initialization leads to vanishing gradients \n",
    "\n",
    "\n",
    "$\n",
    "W^{[l]} = \\begin{bmatrix}\n",
    "0.5 & 0 \\\\ \n",
    " 0& 0.5\n",
    "\\end{bmatrix} \\\\\n",
    "$\n",
    "\n",
    "for $l = {1,2,...,L}$\n",
    "\n",
    "$\n",
    "\\hat{y} = W^{[L]}0.5^{L-1}x \n",
    "$\n",
    "\n",
    "The activations functions decrease exponentially with $l$:\n",
    "\n",
    "* In backward propagation, this leads to the vanishing gradient\n",
    "* Leading to convergence of the cost before it has reached the minimum value. Train is slow, pramaeters almost do not change.\n",
    "\n",
    "In summary it is important to initialize properly the DNN:\n",
    "\n",
    "\n",
    "```text\n",
    "* din: dim of the input of the payer\n",
    "* dout: dim of output of the payer\n",
    "* d_avg = (d_in + d_out)/2 \n",
    "\n",
    "|Init  |Activation                           | sigma2 (Normal)|\n",
    "|------+-------------------------------------+----------------|\n",
    "|Xavier|Tanh, Linear(None), Logistic, softmax| 1/d_avg        |\n",
    "|He    |Relu n variants                      | 2/d_avg        |\n",
    "|leCun |Selu !?                              | 1/d_avg        |\n",
    "|------+-------------------------------------+----------------|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Implement simple regression first\n",
    "\n",
    "\n",
    "1. One layer Dense\n",
    "1. Regression\n",
    "\n",
    "Steps:\n",
    "    \n",
    "* Implement parts \n",
    "    1. activatoins\n",
    "    1. code one layer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/simple_nn.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} \\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]})\n",
    "$\n",
    "\n",
    "<img src=\"../images/forward_step.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "refs:\n",
    "* https://sudeepraja.github.io/Neural/ Matrix notation\n",
    "* https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n",
    "* https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../images/chainrule_example.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes about the big picture\n",
    "\n",
    "* We want to optmize the cost function $J(\\theta = {W,b}) = \\frac{1}{m} \\sum^m_{i=1} \\ell(\\hat{y^{(i)}} = h_{\\theta}(x^{(i)}), y^{(i)})$\n",
    "\n",
    "* In order to do that we need:\n",
    "\n",
    "    1. We need to update all the parameters: $\\{W^{[1]}, b^{[1]},...., W^{[L]}, b^{[L]}\\}$ of the NN in the right direction.\n",
    "    \n",
    "    1. To discover the right direction to update, we need to compute the gradients.: $\\frac{\\partial}{\\partial W^{[l]}}J$ and $\\frac{\\partial}{\\partial b^{[l]}}J$ for $l = \\{1,.., L\\} $\n",
    "    \n",
    "    1. To compute the gradients, we need to use the chain rule\n",
    "    \n",
    "\n",
    "The 4 equations we need to compute the gradients can be written in matrix form as:\n",
    "\n",
    "\n",
    "$\n",
    "\\delta^L = \\frac{\\partial}{\\partial \\hat{y}}J \\circ f'(z^{[L]}) \\\\\n",
    "\\delta^l = \\left ( (W^{[l+1]})^t \\delta^{[l+1]} \\right )  \\circ f'(z^{[l]}) \\\\\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\delta^l (a^{[l-1]})^t  \\\\\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\delta^l\n",
    "$\n",
    "    \n",
    "where $\\hat{y} = a^{[L]}$, $X = a^{[0]}$ and we assume all activations function are the same $f$ . Also, $\\circ$ is the element-wise multiplication. It is important in the euqation so the matrix dimensions can match.  \n",
    "\n",
    "The proof of the equations can be found: http://neuralnetworksanddeeplearning.com/chap2.html#proof_of_the_four_fundamental_equations_(optional)\n",
    "\n",
    "**TODO**: Try read and understand the proof. It will help you a lot understand the papers in the area. Or if you need to implement you personal layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "* There is a lot of details and math hot to get the equations. But there a ways to develope the equations\n",
    "\n",
    "    * Using sum and indexes notation like in: $\\frac{\\partial}{\\partial W^{[2]}_{i,j}}J = U_i f'(z^{[3]}_i)a^{[2]}_j \\equiv \\delta^{[3]}_i a^{[2]}_j$\n",
    "    \n",
    "    * Using matrix notation that simplify the equations but it is hard to understand. The equation above is tranlaste as: $\\frac{\\partial}{\\partial W^{[2]}}J = \\delta^{[3]}(a^{[2]})^t$ where $\\delta^{[3]} = U\\circ f'(z^{[3]})$ and $\\circ$ is the element-wise or Hadamard product.\n",
    "    \n",
    "* UNderstandthe big picture\n",
    "\n",
    "    * https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb  See code in train method <== Help me understand the diff with gradient descent\n",
    "    * helps to understand the equations: http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "    * wikipedia: https://en.wikipedia.org/wiki/Backpropagation\n",
    "    \n",
    "* UNderstan the implementatiio\n",
    "    * https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "    \n",
    "    \n",
    "* Understanding math with index notation\n",
    "    * https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n",
    "    \n",
    "* Undersatnding math with matrix notation\n",
    "    * https://sudeepraja.github.io/Neural/\n",
    "    * https://www.youtube.com/watch?v=isPiE-DBagM&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6 vidoe form standfort\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The chain rule and rule while deriving matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain rules\n",
    "\n",
    "$\n",
    "u = u(x(r,t), y(r,y)) \\\\\n",
    "\\frac{\\partial}{\\partial r}u = \\frac{\\partial}{\\partial x}u \\frac{\\partial}{\\partial r}x = \\frac{\\partial}{\\partial y}u \\frac{\\partial}{\\partial r}y\n",
    "$\n",
    "\n",
    "When deriving related to a matrix or vector\n",
    "\n",
    "1. The derivative has the same dimension of the the ariable been derived  **\n",
    "\n",
    "    * Scalar by scalar: $\\frac{\\partial}{\\partial x}f(x) = f'$ , so $f'$ is scalar\n",
    "    * Scalar by a vector $\\frac{\\partial}{\\partial \\mathbf{x}}f(\\mathbf{x}) = \\mathbf{f'}$ , so $\\mathbf{f'}$ is a vector of dim of $\\mathbf{x}$\n",
    "    * Matrix or vector by a matrix: $\\frac{\\partial}{\\partial \\mathbf{x}}f(\\mathbf{x}) = \\mathbf{f'}$ , so $\\mathbf{f'}$ is a vector of dim of $\\mathbf{x}$\n",
    "\n",
    "ref: https://kevinzakka.github.io/2016/09/14/batch_normalization/ But I need to double check this rules with other sources and check if it is compatible to 4 Backpropagation equations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to proof some of the equations\n",
    "\n",
    "Define gradient in matrix form as:\n",
    "\n",
    "$\n",
    "J = J(X_1,X_2,...,X_m)  \\\\\n",
    "\\frac{\\partial}{\\partial X}J = [\\frac{\\partial}{\\partial X_1}J, \\frac{\\partial}{\\partial X_2}J, ..., \\frac{\\partial}{\\partial X_m}J] \n",
    "$\n",
    "\n",
    "> PS: derivatives are row vectors\n",
    "\n",
    "\n",
    "Page 1\n",
    "<img src=\"../images/backprop_1.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> \n",
    "\n",
    "-----\n",
    "Page2\n",
    "\n",
    "<img src=\"../images/backprop_2.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The cost or objective function:  \n",
    "\n",
    "$\n",
    "J(\\theta = {W,b}) = \\frac{1}{m} \\sum^m_{i=1} \\ell(\\hat{y^{(i)}} = h_{\\theta}(x^{(i)}), y^{(i)})\n",
    "$\n",
    "\n",
    "* The loss function:\n",
    "\n",
    "$\n",
    "\\ell(\\hat{y^{(i)}}, y^{(i)})\n",
    "$\n",
    "\n",
    "* The activation function of the last layer:\n",
    "\n",
    "$\n",
    "g = h_{\\theta}(x^{(i)})\n",
    "$\n",
    "\n",
    "Reviewing chaing rule:\n",
    "\n",
    "* For function of 1 variable:\n",
    "$\n",
    "y =  f(u(x)) \\\\\n",
    "\\frac{d}{dx}y = \\frac{d}{du}y \\frac{d}{dx}u\n",
    "$\n",
    "\n",
    "* For 2 vriables \n",
    "\n",
    "$\n",
    "y =  f(u(x)) \\\\\n",
    "\\frac{d}{dx}y = \\frac{d}{du}y \\frac{d}{dx}u\n",
    "$\n",
    "\n",
    "To optmize the cost funtction $J(W^{[1]},b^{[1]},..., W^{[l]}, b^{[l]}, ...,W^{[L]}, b^{[L]} )$, where $L$ is the number of layers, we need to compute:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J \\frac{\\partial}{\\partial W^{[l]}}z^{[l]} \\\\ \n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J a^{[l-1]} \\\\\n",
    "\\frac{\\partial}{\\partial b^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J \\frac{\\partial}{\\partial b^{[l]}}z^{[l]} \\\\ \n",
    "\\frac{\\partial}{\\partial b^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J\n",
    "$\n",
    "\n",
    "\n",
    "The common term is called the local gradient:\n",
    "\n",
    "$\n",
    "\\delta^{[l]} = \\frac{\\partial}{\\partial z^{[l]}}J\n",
    "$\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "\n",
    "* Forwards pass in matrix notation:\n",
    "\n",
    "https://sudeepraja.github.io/Neural/ Matrix notation\n",
    "\n",
    "$\n",
    "J(\\theta = {W,b}) = \\frac{1}{m} \\sum^m_{i=1} \\ell(\\hat{y^{(i)}} = h_{\\theta}(x^{(i)}), y^{(i)}) \\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]})\n",
    "$\n",
    "\n",
    "* Backwards pass\n",
    "\n",
    "Last layer (mse loss)\n",
    "\n",
    "$\n",
    "\\delta^{[L]} = (a^{[L]} - y) g'^{[L]}(z^{[L]}) \\\\\n",
    "\\delta^{[L]} = (a^{[L]} - y) g'^{[L]}(W^{[L]}a^{[L-1]}) \\\\  \n",
    "$\n",
    "\n",
    "Hidden Layers\n",
    "\n",
    "$\n",
    "\\delta^{[l]} = (W^{[l]})^t \\delta^{[l+1]} g'^{[l]}(W^{[l]}a^{[l-1]} + b^{[l]})  \n",
    "$\n",
    "\n",
    "Weight updates:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J  = \\delta^{[l]} (a^{[l]})^t \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial}{\\partial W^{[l]}}J \\\\\n",
    "\\frac{\\partial}{\\partial b^{[l]}}J  = \\delta^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial}{\\partial b^{[l]}}J\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double checking the backpropagation equations\n",
    "\n",
    "https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 1\n",
    "<img src=\"../images/backprop_notes_0.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Page 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/backprop_notes_1.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "Page 3\n",
    "<img src=\"../images/backprop_notes_2.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Page 4\n",
    "<img src=\"../images/backprop_notes_3.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Page 5\n",
    "<img src=\"../images/backprop_notes_4.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP: Still need to finish this code\n",
    "class Layer():\n",
    "    \n",
    "    def _get_activation_func(self,activation_type):\n",
    "            \n",
    "        if activation_type == 'relu':\n",
    "            \n",
    "            def activation_func(z: np.array):\n",
    "    \n",
    "                a = np.maximum(0.0,z)\n",
    "        \n",
    "                return a\n",
    "            \n",
    "            def activation_derivative(z: np.array):\n",
    "                \n",
    "                return \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            def activation_func(z: np.array):\n",
    "    \n",
    "                a = 1.0/(1.0 + np.exp(-z))\n",
    "        \n",
    "                return a\n",
    "        \n",
    "            def activation_derivative_func(z: np.array):\n",
    "                \n",
    "                return activation_func*(1.0 - activation_func)\n",
    "                \n",
    "        \n",
    "        return activation_func, activation_derivative_func\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_type):\n",
    "\n",
    "        self.activation_type = activation_type\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        \n",
    "        activation , first_deriv_of_activation = self._get_activation_func(activation_type)\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.activation_derivative = first_deriv_of_activation\n",
    "            \n",
    "class DenseLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, activation_type):\n",
    "        \n",
    "        super().__init__( input_size, output_size, activation_type)\n",
    "\n",
    "    def forward_propagation(self, X_in: np.array, W: np.array, b: np.array) -> Tuple[np.array]:\n",
    "        \n",
    "        z = W.dot(X_in)\n",
    "        a = self.activation(z)\n",
    "        \n",
    "        return a, z\n",
    "    \n",
    "    def backward_propagation(self, da, W, b, z, a_prev):\n",
    "\n",
    "        dz = self.activation_derivative(z)*da  # element-wise multiplication\n",
    "        \n",
    "        dW = np.dot(dz, a_prev.T) / m\n",
    "        db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "        \n",
    "        da_prev = np.dot(W.T, dZ)\n",
    "\n",
    "        return da_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'activation_derivative_func' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b38806d826be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-5767f07c6d3c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, activation_type)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-5767f07c6d3c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, activation_type)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mactivation\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfirst_deriv_of_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_activation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-5767f07c6d3c>\u001b[0m in \u001b[0;36m_get_activation_func\u001b[0;34m(self, activation_type)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_derivative_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'activation_derivative_func' referenced before assignment"
     ]
    }
   ],
   "source": [
    "in_size = 1\n",
    "out_size = 1\n",
    "batch_size = 3\n",
    "\n",
    "l1 = DenseLayer(in_size,out_size,'relu')\n",
    "l1.activation_type\n",
    "\n",
    "l2 = DenseLayer(in_size,out_size, 'sigmoid')\n",
    "l2.activation_type\n",
    "\n",
    "\n",
    "X = np.array([[1.0, 2.0], \n",
    "              [1.0,4.0],\n",
    "             [0.0, 0.0]])\n",
    "\n",
    "print(f\"batch_size: {X.shape[0]}; in_size: {X.shape[1]}\")\n",
    "\n",
    "W = np.array([-1.0, 0.5]).reshape((1,2))\n",
    "W.shape\n",
    "print(f\"out_size: {W.shape[0]}; in_size: {W.shape[1]}\")\n",
    "\n",
    "b = np.zeros((1,))\n",
    "\n",
    "print(\"Expected Z\")\n",
    "W.dot(X[0,:])\n",
    "W.dot(X[1,:])\n",
    "W.dot(X[2,:])\n",
    "\n",
    "print(\"Layers output\")\n",
    "\n",
    "for k in range(batch_size):\n",
    "    \n",
    "    Xb = X[k,:]\n",
    "\n",
    "    l1.forward_propagation(Xb,W,b)\n",
    "    l2.forward_propagation(Xb,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "\n",
    "    def __init__(self, layers, learning_rate = 0.1, seed= 2021):\n",
    "        \n",
    "        self._seed = seed \n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.params_values = {}\n",
    "\n",
    "        self._init_layer_parameter_random()\n",
    "        \n",
    "        loss, loss_derivative = self._get_loss('cross-entropy')\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.loss_derivative = loss_derivative\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        \n",
    "\n",
    "    def _get_loss(loss_type):\n",
    "        \n",
    "        if loss_type == 'cross-entropy':\n",
    "            \n",
    "            def loss(y_hat, y):\n",
    "                \n",
    "                # J = - 1/m \\sum_1^m y log y_hat - (1 - y) log(1-y_hat)  <= cross entropy loss \n",
    "                m = y_hat.shape[1]\n",
    "                cost = -1.0 / m * (np.dot(y, np.log(y_hat).T) + np.dot(1.0 - y, np.log(1.0 - y_hat).T))\n",
    "                \n",
    "            return np.squeeze(cost)\n",
    "        \n",
    "        \n",
    "            def loss_derivative(y_hat, y):\n",
    "                \n",
    "                m = y_hat.shape[1]\n",
    "                \n",
    "                # y_hat is teh activation in the last Layer L: a^{[L]}\n",
    "                # dJ/da^[L] = dJ/dy_hat = y/y_hat - (1-y)/(1-y_hat)\n",
    "                da_prev =  -(np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat))/m\n",
    "            \n",
    "                return da_prev\n",
    "        else:\n",
    "            \n",
    "            # mse loss\n",
    "           \n",
    "            def loss(y_hat, y):\n",
    "                \n",
    "                # J = - 1/m \\sum_1^m y log y_hat - (1 - y) log(1-y_hat)  <= cross entropy loss \n",
    "                m = y_hat.shape[1]\n",
    "                cost = -0.5/m * np.dot((y - y_hat).t, (y - y_hat))\n",
    "                \n",
    "            return np.squeeze(cost)\n",
    "        \n",
    "        \n",
    "            def loss_derivative(y_hat, y):\n",
    "\n",
    "                m = y_hat.shape[1]\n",
    "                \n",
    "                # y_hat is teh activation in the last Layer L: a^{[L]}\n",
    "                # TODO: Review this equation. Maybe the constant is missing\n",
    "                # Should I include or not the term f'(z) the activation function\n",
    "                da_prev =  1.0/m * (y-y_hat)\n",
    "            \n",
    "                return da_prev\n",
    "            \n",
    "\n",
    "        return loss, loss_derivative\n",
    "        \n",
    "    # All bias are zero\n",
    "    def _init_layer_parameter_random(self):\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            \n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            # We need initialize with small values. Because high values are regions with\n",
    "            # samall (almost vanish) gradients for sigmois, tanh funcitons\n",
    "            W = np.random.randn(layer.output_size, layer.input_size) * 0.01\n",
    "            self.params_values['W' + str(layer_idx)] =  W\n",
    "            \n",
    "            b = np.random.randn(layer.output_size, 1) * 0.01\n",
    "            self.params_values['b' + str(layer_idx)] = b\n",
    "\n",
    "    def _init_layer_paremeter_xavier(self):\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def compile(self, optimizer,loss_type,metrcis):\n",
    "        \n",
    "        # TODO: Set this objects in the neral networks\n",
    "        pass\n",
    "        \n",
    "    def accuracy(self, y_hat, y):\n",
    "        \n",
    "        y_hat_ = convert_prob_into_class(y_hat)\n",
    "        \n",
    "        return (y_hat_ == y).all(axis=0).mean()\n",
    "\n",
    "        \n",
    "    def predict(self, X_in: np.array) -> np.array:\n",
    "        \n",
    "        y, _ = self.forward_propagation(X_in)\n",
    "        \n",
    "        return y\n",
    "     \n",
    "    def forward_propagation(self, X_in: np.array) -> Tuple[np.array, dict] : \n",
    "        # forward propagation\n",
    "        \n",
    "        memory = {}\n",
    "        # a^{[0]} = X_in. The first activation are the inputs\n",
    "        a_curr = X_in\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            \n",
    "            layer_idx = idx + 1\n",
    "            a_prev = A_curr\n",
    "            \n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx)]\n",
    "            \n",
    "            a_curr, z_curr = layer.forward_propagation(a_curr, W_curr, b_curr) \n",
    "            \n",
    "            # Need this for back propagation \n",
    "            memory[\"a\" + str(idx)] = a_prev  # a^{[0]} = X_in\n",
    "            memory[\"z\" + str(layer_idx)] = z_curr\n",
    "    \n",
    "        return A_curr, memory\n",
    "    \n",
    "    def backward_propagation(y_hat, y, memory):\n",
    "        \n",
    "        gradients = {}\n",
    "        m = y.shape[1]\n",
    "        y = y.reshape(y_hat.shape)\n",
    "        \n",
    "        # d/da^{[L]}J \n",
    "        da_prev = self.loss_derivative(y_hat, y)\n",
    "        \n",
    "        for idx, layer in reversed(list(enumerate(layers))):\n",
    "    \n",
    "            layer_idx = idx + 1\n",
    "            print(f\"{idx}: {layer}\")\n",
    "            \n",
    "            da_curr = da_prev\n",
    "\n",
    "            a_prev = memory[\"a\" + str(layer_idx - 1)]\n",
    "            z = memory[\"z\" + str(layer_idx)]\n",
    "            \n",
    "            W = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b = self.params_values[\"b\" + str(layer_idx)]\n",
    "            \n",
    "            da_prev, dW, db = layer.backward_propagation(da, W, b, z, a_prev)\n",
    "            \n",
    "            gradients[\"dW\" + str(layer_idx_curr)] = dW\n",
    "            gradients[\"db\" + str(layer_idx_curr)] = db\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def _update_parameters(self,grads_values):\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            \n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            self.params_values[\"W\" + str(layer_idx)] -= self.lr * grads_values[\"dW\" + str(layer_idx)]        \n",
    "            self.params_values[\"b\" + str(layer_idx)] -= self.lr * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    \n",
    "    def train(self, X, y, epochs=3):\n",
    "        \n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        \n",
    "        # Gradient descent \n",
    "        for e in range(epochs):\n",
    "         \n",
    "            y_hat, memory = forward_propagation(X_in)\n",
    "            \n",
    "            cost = self.loss(y_hat, y)\n",
    "            acc = self.accuracy(y_hat, y)\n",
    "            \n",
    "            cost_history.append(cost)\n",
    "            accuracy_history.append(acc)\n",
    "           \n",
    "            #  da_prev, dW, db = grads\n",
    "            grads = self.backward_propagation(y_hat, y, memory)\n",
    "           \n",
    "            # update self.params_values\n",
    "            self._update(grads)\n",
    "            \n",
    "        return self.params_values, cost_history, accuracy_history\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    " \n",
    "1. ~~validate forward~~ OK\n",
    "1. validate accuracy  this is easy\n",
    "1. validate loss function  easy\n",
    "1. validate layer backpropagation   this one is hard todo it\n",
    "1. validate update parameters This one is easy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 3\n",
      "1: 2\n",
      "2: 1\n",
      "\n",
      "3: 3\n",
      "2: 2\n",
      "1: 1\n"
     ]
    }
   ],
   "source": [
    "layers = [1,2,3]\n",
    "\n",
    "\n",
    "for idx, layer in enumerate(list(reversed(layers))):\n",
    "    \n",
    "    print(f\"{idx}: {layer}\")\n",
    "    \n",
    "print()\n",
    "for idx, layer in reversed(list(enumerate(layers))):\n",
    "    \n",
    "    idx = idx + 1\n",
    "    \n",
    "    print(f\"{idx}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compare or study.\n",
    "# The loss is mean square erro (mse = (y - y_hat)^2)\n",
    "def backpropagation(self,y, z_s, a_s):\n",
    "    \n",
    "    dw = []  # dC/dW\n",
    "    db = []  # dC/dB\n",
    "    \n",
    "    deltas = [None] * len(self.weights)  # delta = dC/dZ  known as error for each layer\n",
    "    \n",
    "    # insert the last layer error\n",
    "    # the derivative of mse: 2(y-y_hat)\n",
    "    deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
    "\n",
    "    # Perform BackPropagation\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "    \n",
    "        deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
    "        batch_size = y.shape[1]\n",
    "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "        \n",
    "        # return the derivitives respect to weight matrix and biases\n",
    "    return dw, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tetsing parameter init with previous layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.01488609, 0.00676011]]),\n",
       " 'b1': array([[-0.00418451]]),\n",
       " 'W2': array([[-0.00806521]]),\n",
       " 'b2': array([[0.00555876]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.49991546])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'A0': array([1., 4.]),\n",
       " 'Z1': array([0.04192653]),\n",
       " 'A1': array([0.04192653]),\n",
       " 'Z2': array([-0.00033815])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = DenseLayer(input_size=2,output_size=1,activation_type = 'relu')\n",
    "l2 = DenseLayer(input_size=1,output_size=1,activation_type = 'sigmoid')\n",
    "\n",
    "layers = [l1,l2]\n",
    "\n",
    "nn = NN(layers)\n",
    "nn.params_values\n",
    "\n",
    "y , memory = nn.predict(X[1,:])\n",
    "y \n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization  \n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* Read the paper. It explain the motivations, why there is trainable parameters and how the backpropagation is affected. There isn important conclusion regards the mistake of not including the dependency of x when doing back proagation of the bias parameter taht clears depend of the mean of the mini batch\n",
    "\n",
    "(Breaktrough in the area)\n",
    "refs:\n",
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\n",
    "    * Authors: Sergey Ioffe (same author of PLDA and works at Google) n Christian Szegedy (google)\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf Paper **TODO** Read the paper. It is simple and easy to understand/ It is a good gain experience in reading paper  \n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization tensprflow doc\n",
    "\n",
    "\n",
    "refs:\n",
    "* https://www.deeplearningbook.org/contents/optimization.html\n",
    "* https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "* https://arxiv.org/pdf/1502.03167.pdf\n",
    "* https://arxiv.org/pdf/1702.03275.pdf\n",
    "* https://www.youtube.com/watch?v=nUUqwaxLnWs\n",
    "* https://arxiv.org/pdf/1805.11604.pdf\n",
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795    \n",
    "* https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "* https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739  <== good visual explanation (reduce gradients jiggling)\n",
    "\n",
    "* Batch normalization backpropagation\n",
    "    * https://kevinzakka.github.io/2016/09/14/batch_normalization/  with the equations\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf  the paper is look like more clear\n",
    "\n",
    "\n",
    "Pros:\n",
    "* Batch Normalization is also a regularization technique. Not strong as dropout or l1 n l2\n",
    "* Train fast (you need less epochs)\n",
    "    * numerical stability. The models become less sensible to large learning rates. You can use much large learning rates and converge fast\n",
    "\n",
    "Eq from the original paper: https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "\n",
    "$\n",
    "BN(Wx) = BN((aW)x) \\\\\n",
    "\\frac{\\partial}{\\partial x} BN((aW)x) \\equiv  \\frac{\\partial}{\\partial x} BN(Wx) \\\\\n",
    "\\frac{\\partial}{\\partial (aW)} BN((aW)x) \\equiv \\frac{1}{a} \\frac{\\partial}{\\partial W} BN(Wx)\n",
    "$\n",
    "\n",
    "    * minimizes jiggling because of reduce internal covarian shift (keep learnig rate constant, it still training fast because reduce jiggling while comparing with and without BN)\n",
    "    \n",
    "* DO not need dropout. You cannot use together\n",
    "* Good for deeper networks like **CNN** and **RNN** (Normally you need a large number of layers )\n",
    "* Minimize Gradient Vanish problems: \n",
    "    * helps to keep z = WX + b between [-4,4] where the gradients is not vanished for sigmoids functions. same for relu and tanh\n",
    "    * For this to be effective, us **BatchNorm layer before the activation of the layer**\n",
    "    * https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484\n",
    "    * \"We add the BN transform immediately before the\n",
    "nonlinearity, by normalizing\" page 5, from the paper https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "Crons:\n",
    "* Useless in SGD. Only use wih Gradiwnt Descent or mini-batch. Because is the mean and std of the mini-batch and SGD has batch size = 1\n",
    "\n",
    "\n",
    "\n",
    "**It is being asked more often in the job interview**\n",
    "\n",
    "Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.\n",
    "\n",
    "**Don’t Use With Dropout:**\n",
    "\n",
    "Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input are the values of x over a batch: $B = {x_1, x_2,..., x_i,..., x_m}$\n",
    "    * where $m$ is the batch size\n",
    "* Output: $y_i = BN_{\\gamma,\\beta}(x_i)$\n",
    "* Learning parameters: $\\gamma$ and $\\beta$\n",
    "* Normalization:\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i \\\\\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\\\\n",
    "z_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} \\\\\n",
    "y_i = BN_{\\gamma,\\beta}(x_i) \\equiv \\gamma z_i + \\beta  \\text{ <= scale and shift} \n",
    "$\n",
    "\n",
    "\n",
    "Matrix formulation\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m}xˆt \\mathbb{1} \\\\\n",
    "v = \\frac{1}{m} (x-\\mu_B)ˆt(x-\\mu_B) \\\\\n",
    "z = (x-\\mu_B)\\sqrt{(v + \\epsilon)^{-1}} \\\\\n",
    "y = \\gamma z + \\beta \n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gradients equatiosn:\n",
    "    \n",
    "* https://www.adityaagrawal.net/blog/deep_learning/bprop_batch_norm\n",
    "* https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html  <== not sure the inverse is right need to test\n",
    "* https://chrisyeh96.github.io/2017/08/28/deriving-batchnorm-backprop.html <== complica demais mas da para usar para conferir, **MAS USA AS MESMAS EQ que EU na forma matricial**\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* papers teher is the eq: https://arxiv.org/pdf/1502.03167.pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0],\n",
       "       [2, 2, 5]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [3.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.66666667, -1.        ],\n",
       "       [-1.        ,  2.        ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conparing ith numpy var and covar \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 2.        ])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0. ,  2.5],\n",
       "       [ 0. ,  0. ,  0. ],\n",
       "       [ 2.5,  0. , 12.5]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -1.5],\n",
       "       [-1.5,  3. ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inversion of v\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.5, -1. ],\n",
       "       [-1. ,  2. ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -1.11022302e-16],\n",
       "       [-2.22044605e-16,  1.00000000e+00]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2],[2,2],[0,5]])\n",
    "x.shape\n",
    "x.T\n",
    "\n",
    "_ones = np.ones((3,1))\n",
    "\n",
    "print(\"mean matrix\")\n",
    "mu = np.matmul(x.T,_ones)/3.0\n",
    "mu.shape\n",
    "mu\n",
    "\n",
    "print(\"variance matrix\")\n",
    "# v is diagonal and epsilon is diagonal as well\n",
    "# the inverse of this matrix is the inverse of the non zero elements\n",
    "v = np.matmul((x- mu.T).T,(x - mu.T))/3.0\n",
    "v\n",
    "\n",
    "\n",
    "# comparing with np variance and covariance matrix\n",
    "print(\"conparing ith numpy var and covar \")\n",
    "np.var(x, axis=0)\n",
    "np.cov(x)\n",
    "np.cov(x.T)\n",
    "\n",
    "\n",
    "print(\"Inversion of v\")\n",
    "# np.linalg.inv(v)\n",
    "v_inv = v\n",
    "v_inv[0,0] = 3.0/2.0\n",
    "v_inv\n",
    "\n",
    "v_inv = np.linalg.inv(v)\n",
    "\n",
    "np.matmul(v,v_inv)\n",
    "\n",
    "# z \n",
    "# z = np.matmul((x-mu).T,(np.sqrt()))\n",
    "# z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find an expression for the gradients of Batch normalization we need to remember that:\n",
    "    \n",
    "$\n",
    "y = BN_{\\gamma,\\beta}(x) \\equiv y(\\gamma,\\beta,\\mu_B, \\sigma^2_B(\\mu_B)) =  y(\\gamma,\\beta,\\mu_B) \\\\ \n",
    "\\sigma^2_B  \\equiv\\sigma^2_B(\\mu_B) = \\frac{1}{m} (x - \\mu_B)^t(x - \\mu_B) \\\\\n",
    "z = z(\\mu_B) \\equiv \\frac{x - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} \\\\\n",
    "$\n",
    "    \n",
    "You can write $\\mu_B$ in matrix formulation as:\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m}xˆt \\mathbb{1}_{1,d}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.87152147, 1.85580424, 2.26072622, 1.00925124, 2.84652559,\n",
       "       0.97547987, 2.91852008, 2.38156737, 2.589072  , 4.11549317])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 2.38; std: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Batch\n",
    "X = np.random.uniform(0,5.0,size=(10))\n",
    "X\n",
    "\n",
    "print(f\"shape: {X.shape}; mean: {X.mean():.2f}; std: {X.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.48732024e-01, -5.90764377e-01, -1.36497064e-01, -1.54048155e+00,\n",
       "        5.20690049e-01, -1.57836842e+00,  6.01458063e-01, -9.29752983e-04,\n",
       "        2.31862185e-01,  1.94429885e+00])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.48732024e-01, -5.90764377e-01, -1.36497064e-01, -1.54048155e+00,\n",
       "        5.20690049e-01, -1.57836842e+00,  6.01458063e-01, -9.29752983e-04,\n",
       "        2.31862185e-01,  1.94429885e+00])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 0.00; std: 1.00\n"
     ]
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "epsilon = 0.0\n",
    "\n",
    "# because we did not train the layer, we are passing the mean and the variance of the batch\n",
    "Y = tf.nn.batch_normalization(X,\n",
    "                    mean = X.mean(axis=0),        # batch mean\n",
    "                    variance = X.var(axis=0),     # batch var\n",
    "                    offset = beta,scale = gamma,  # batch beta and gamma See equations  \n",
    "                    variance_epsilon = epsilon)   # batch epsilon See equations\n",
    "\n",
    "Y.numpy()\n",
    "\n",
    "# comparing with numpy\n",
    "\n",
    "Z = (X - X.mean(axis=0))/np.sqrt(X.var(axis=0) + epsilon)\n",
    "Y = gamma * Z + beta\n",
    "Y\n",
    "\n",
    "# Expectd zero mean and unit variance\n",
    "print(f\"shape: {Y.shape}; mean: {Y.mean():.2f}; std: {Y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex: Model with Batch vs without batch noraliztion\n",
    "\n",
    "ref: https://analyticsindiamag.com/hands-on-guide-to-implement-batch-normalization-in-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(input_train, target_train), (input_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data preaparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the input sets\n",
    "input_train_shape = input_train.shape\n",
    "input_test_shape = input_test.shape \n",
    "\n",
    "input_train_shape\n",
    "input_test_shape\n",
    "\n",
    "# Normalize input data\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x151514c10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOZ0lEQVR4nO3dbYxc5XnG8euKbezamMQbB9chLjjgFAg0Jl0ZEBZQoVCCKgGqArGiyKG0ThOchNaVoLQqtKKVWyVElFIkU1xMxUsgAeEPNAm1ECRqcFlcY2wIb8Y0NmaNWYENIX5Z3/2w42iBnWeXmTMv3vv/k1Yzc+45c24NXD5nznNmHkeEAIx/H+p0AwDag7ADSRB2IAnCDiRB2IEkJrZzY4d5ckzRtHZuEkjlV3pbe2OPR6o1FXbb50m6QdIESf8WEctLz5+iaTrV5zSzSQAFa2NN3VrDh/G2J0i6SdLnJZ0oaZHtExt9PQCt1cxn9gWSXoiIzRGxV9Ldki6opi0AVWsm7EdJ+sWwx1try97F9hLbfbb79mlPE5sD0IyWn42PiBUR0RsRvZM0udWbA1BHM2HfJmnOsMefqC0D0IWaCfvjkubZnmv7MElflLS6mrYAVK3hobeI2G97qaQfaWjobWVEbKqsMwCVamqcPSIelPRgRb0AaCEulwWSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJpmZxRffzxPJ/4gkfm9nS7T/7F8fUrQ1OPVBc9+hjdxTrU7/uYv3V6w+rW1vX+73iujsH3y7WT713WbF+3J8/Vqx3QlNht71F0m5Jg5L2R0RvFU0BqF4Ve/bfi4idFbwOgBbiMzuQRLNhD0k/tv2E7SUjPcH2Ett9tvv2aU+TmwPQqGYP4xdGxDbbR0p6yPbPI+LR4U+IiBWSVkjSEe6JJrcHoEFN7dkjYlvtdoek+yUtqKIpANVrOOy2p9mefvC+pHMlbayqMQDVauYwfpak+20ffJ07I+KHlXQ1zkw4YV6xHpMnFeuvnPWRYv2d0+qPCfd8uDxe/JPPlMebO+k/fzm9WP/HfzmvWF978p11ay/te6e47vL+zxXrH//JofeJtOGwR8RmSZ+psBcALcTQG5AEYQeSIOxAEoQdSIKwA0nwFdcKDJ792WL9+ttuKtY/Nan+VzHHs30xWKz/zY1fKdYnvl0e/jr93qV1a9O37S+uO3lneWhuat/aYr0bsWcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ6/A5GdfKdaf+NWcYv1Tk/qrbKdSy7afVqxvfqv8U9S3Hfv9urU3D5THyWf9838X66106H2BdXTs2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCUe0b0TxCPfEqT6nbdvrFgOXnl6s7zqv/HPPEzYcXqw/+fUbP3BPB12383eK9cfPKo+jD77xZrEep9f/AeIt3yyuqrmLniw/Ae+zNtZoVwyMOJc1e3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9i4wYeZHi/XB1weK9ZfurD9WvunMlcV1F/zDN4r1I2/q3HfK8cE1Nc5ue6XtHbY3DlvWY/sh28/XbmdU2TCA6o3lMP42Se+d9f4qSWsiYp6kNbXHALrYqGGPiEclvfc48gJJq2r3V0m6sNq2AFSt0d+gmxUR22v3X5U0q94TbS+RtESSpmhqg5sD0Kymz8bH0Bm+umf5ImJFRPRGRO8kTW52cwAa1GjY+23PlqTa7Y7qWgLQCo2GfbWkxbX7iyU9UE07AFpl1M/stu+SdLakmba3SrpG0nJJ99i+TNLLki5uZZPj3eDO15taf9+uxud3//SXni7WX7t5QvkFDpTnWEf3GDXsEbGoTomrY4BDCJfLAkkQdiAJwg4kQdiBJAg7kARTNo8DJ1z5XN3apSeXB03+/eg1xfpZX7i8WJ/+vceKdXQP9uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7ONAadrk1792QnHd/1v9TrF+1XW3F+t/efFFxXr874fr1ub8/c+K66qNP3OeAXt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCKZuTG/ij04v1O675drE+d+KUhrf96duXFuvzbtlerO/fvKXhbY9XTU3ZDGB8IOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnR1GcMb9YP2L51mL9rk/+qOFtH//wHxfrv/239b/HL0mDz29ueNuHqqbG2W2vtL3D9sZhy661vc32+trf+VU2DKB6YzmMv03SeSMs/25EzK/9PVhtWwCqNmrYI+JRSQNt6AVACzVzgm6p7Q21w/wZ9Z5ke4ntPtt9+7Snic0BaEajYb9Z0rGS5kvaLuk79Z4YESsiojcieidpcoObA9CshsIeEf0RMRgRByTdImlBtW0BqFpDYbc9e9jDiyRtrPdcAN1h1HF223dJOlvSTEn9kq6pPZ4vKSRtkfTViCh/+ViMs49HE2YdWay/cslxdWtrr7yhuO6HRtkXfemlc4v1Nxe+XqyPR6Vx9lEniYiIRSMsvrXprgC0FZfLAkkQdiAJwg4kQdiBJAg7kARfcUXH3LO1PGXzVB9WrP8y9hbrf/CNK+q/9v1ri+seqvgpaQCEHciCsANJEHYgCcIOJEHYgSQIO5DEqN96Q24HFs4v1l/8QnnK5pPmb6lbG20cfTQ3DpxSrE99oK+p1x9v2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs49z7j2pWH/um+Wx7lvOWFWsnzml/J3yZuyJfcX6YwNzyy9wYNRfN0+FPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+yFg4tyji/UXL/143dq1l9xdXPcPD9/ZUE9VuLq/t1h/5IbTivUZq8q/O493G3XPbnuO7YdtP217k+1v1Zb32H7I9vO12xmtbxdAo8ZyGL9f0rKIOFHSaZIut32ipKskrYmIeZLW1B4D6FKjhj0itkfEutr93ZKekXSUpAskHbyWcpWkC1vUI4AKfKDP7LaPkXSKpLWSZkXEwYuPX5U0q846SyQtkaQpmtpwowCaM+az8bYPl/QDSVdExK7htRiaHXLEGSIjYkVE9EZE7yRNbqpZAI0bU9htT9JQ0O+IiPtqi/ttz67VZ0va0ZoWAVRh1MN425Z0q6RnIuL6YaXVkhZLWl67faAlHY4DE4/5rWL9zd+dXaxf8nc/LNb/9CP3FeuttGx7eXjsZ/9af3it57b/Ka474wBDa1Uay2f2MyR9WdJTttfXll2toZDfY/sySS9LurglHQKoxKhhj4ifShpxcndJ51TbDoBW4XJZIAnCDiRB2IEkCDuQBGEHkuArrmM0cfZv1q0NrJxWXPdrcx8p1hdN72+opyos3bawWF938/xifeb3NxbrPbsZK+8W7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+x7f7/8s8V7/2ygWL/6uAfr1s79jbcb6qkq/YPv1K2duXpZcd3j//rnxXrPG+Vx8gPFKroJe3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSCLNOPuWC8v/rj138r0t2/ZNbxxbrN/wyLnFugfr/bjvkOOve6lubV7/2uK6g8UqxhP27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQhCOi/AR7jqTbJc2SFJJWRMQNtq+V9CeSXqs99eqIqP+lb0lHuCdONRO/Aq2yNtZoVwyMeGHGWC6q2S9pWUSssz1d0hO2H6rVvhsR366qUQCtM5b52bdL2l67v9v2M5KOanVjAKr1gT6z2z5G0imSDl6DudT2Btsrbc+os84S2322+/ZpT3PdAmjYmMNu+3BJP5B0RUTsknSzpGMlzdfQnv87I60XESsiojcieidpcvMdA2jImMJue5KGgn5HRNwnSRHRHxGDEXFA0i2SFrSuTQDNGjXsti3pVknPRMT1w5bPHva0iySVp/ME0FFjORt/hqQvS3rK9vrasqslLbI9X0PDcVskfbUF/QGoyFjOxv9U0kjjdsUxdQDdhSvogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSYz6U9KVbsx+TdLLwxbNlLSzbQ18MN3aW7f2JdFbo6rs7eiI+NhIhbaG/X0bt/siordjDRR0a2/d2pdEb41qV28cxgNJEHYgiU6HfUWHt1/Srb11a18SvTWqLb119DM7gPbp9J4dQJsQdiCJjoTd9nm2n7X9gu2rOtFDPba32H7K9nrbfR3uZaXtHbY3DlvWY/sh28/XbkecY69DvV1re1vtvVtv+/wO9TbH9sO2n7a9yfa3ass7+t4V+mrL+9b2z+y2J0h6TtLnJG2V9LikRRHxdFsbqcP2Fkm9EdHxCzBsnynpLUm3R8RJtWX/JGkgIpbX/qGcERFXdklv10p6q9PTeNdmK5o9fJpxSRdK+oo6+N4V+rpYbXjfOrFnXyDphYjYHBF7Jd0t6YIO9NH1IuJRSQPvWXyBpFW1+6s09D9L29XprStExPaIWFe7v1vSwWnGO/reFfpqi06E/ShJvxj2eKu6a773kPRj20/YXtLpZkYwKyK21+6/KmlWJ5sZwajTeLfTe6YZ75r3rpHpz5vFCbr3WxgRn5X0eUmX1w5Xu1IMfQbrprHTMU3j3S4jTDP+a5187xqd/rxZnQj7Nklzhj3+RG1ZV4iIbbXbHZLuV/dNRd1/cAbd2u2ODvfza900jfdI04yrC967Tk5/3omwPy5pnu25tg+T9EVJqzvQx/vYnlY7cSLb0ySdq+6binq1pMW1+4slPdDBXt6lW6bxrjfNuDr83nV8+vOIaPufpPM1dEb+RUl/1Yke6vT1SUlP1v42dbo3SXdp6LBun4bObVwm6aOS1kh6XtJ/Serpot7+Q9JTkjZoKFizO9TbQg0dom+QtL72d36n37tCX21537hcFkiCE3RAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMT/A65XcTMQuIbWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = input_train[0,:,:]\n",
    "imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras layer input shape\n",
    "input_shape = (input_train_shape[1], input_train_shape[2], 1)\n",
    "\n",
    "input_shape\n",
    "\n",
    "# Reshape the training data to include channels\n",
    "input_train = input_train.reshape(input_train_shape[0], input_train_shape[1], input_train_shape[2], 1)\n",
    "\n",
    "input_test = input_test.reshape(input_test_shape[0], input_test_shape[1], input_test_shape[2], 1)\n",
    "\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "\n",
    "input_train.min()\n",
    "input_train.mean()\n",
    "input_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "batch_size = 250\n",
    "no_epochs = 10\n",
    "no_classes = 10\n",
    "validation_split = 0.2\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               409856    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 431,242\n",
      "Trainable params: 431,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(no_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "192/192 [==============================] - 31s 161ms/step - loss: 0.2906 - accuracy: 0.9176 - val_loss: 0.0886 - val_accuracy: 0.9734\n",
      "Epoch 2/10\n",
      "192/192 [==============================] - 32s 167ms/step - loss: 0.0681 - accuracy: 0.9789 - val_loss: 0.0611 - val_accuracy: 0.9812\n",
      "Epoch 3/10\n",
      "192/192 [==============================] - 31s 159ms/step - loss: 0.0488 - accuracy: 0.9851 - val_loss: 0.0452 - val_accuracy: 0.9869\n",
      "Epoch 4/10\n",
      "192/192 [==============================] - 30s 157ms/step - loss: 0.0356 - accuracy: 0.9895 - val_loss: 0.0425 - val_accuracy: 0.9875\n",
      "Epoch 5/10\n",
      "192/192 [==============================] - 30s 159ms/step - loss: 0.0282 - accuracy: 0.9912 - val_loss: 0.0474 - val_accuracy: 0.9869\n",
      "Epoch 6/10\n",
      "192/192 [==============================] - 28s 148ms/step - loss: 0.0231 - accuracy: 0.9929 - val_loss: 0.0386 - val_accuracy: 0.9887\n",
      "Epoch 7/10\n",
      "192/192 [==============================] - 28s 146ms/step - loss: 0.0178 - accuracy: 0.9945 - val_loss: 0.0368 - val_accuracy: 0.9891\n",
      "Epoch 8/10\n",
      "192/192 [==============================] - 28s 147ms/step - loss: 0.0154 - accuracy: 0.9952 - val_loss: 0.0364 - val_accuracy: 0.9895\n",
      "Epoch 9/10\n",
      "192/192 [==============================] - 28s 147ms/step - loss: 0.0115 - accuracy: 0.9962 - val_loss: 0.0431 - val_accuracy: 0.9878\n",
      "Epoch 10/10\n",
      "192/192 [==============================] - 27s 143ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.0412 - val_accuracy: 0.9897\n",
      "Test loss: 0.03152180835604668 / Test accuracy: 0.9901999831199646\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(f\"default lr: {tf.keras.backend.get_value(model.optimizer.lr)}\")\n",
    "\n",
    "# Fit data to model\n",
    "history = model.fit(input_train, target_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            validation_split=validation_split)\n",
    "\n",
    "# Generate generalization metric  s\n",
    "score = model.evaluate(input_test, target_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 13, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 5, 5, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               409856    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 433,034\n",
      "Trainable params: 432,138\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_batch = tf.keras.Sequential()\n",
    "\n",
    "# add layer and batch normalizations layers\n",
    "model_batch.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model_batch.add(tf.keras.layers.BatchNormalization())\n",
    "model_batch.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_batch.add(tf.keras.layers.BatchNormalization())\n",
    "model_batch.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_batch.add(tf.keras.layers.BatchNormalization())\n",
    "model_batch.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_batch.add(tf.keras.layers.BatchNormalization())\n",
    "model_batch.add(tf.keras.layers.Flatten())\n",
    "model_batch.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model_batch.add(tf.keras.layers.BatchNormalization())\n",
    "model_batch.add(tf.keras.layers.Dense(no_classes, activation='softmax'))\n",
    "\n",
    "model_batch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default lr: 0.009999999776482582\n",
      "Epoch 1/10\n",
      "192/192 [==============================] - 55s 280ms/step - loss: 0.1119 - accuracy: 0.9657 - val_loss: 5.0127 - val_accuracy: 0.1088\n",
      "Epoch 2/10\n",
      "192/192 [==============================] - 54s 283ms/step - loss: 0.0406 - accuracy: 0.9871 - val_loss: 1.0008 - val_accuracy: 0.7089\n",
      "Epoch 3/10\n",
      "192/192 [==============================] - 55s 286ms/step - loss: 0.0266 - accuracy: 0.9915 - val_loss: 0.0548 - val_accuracy: 0.9847\n",
      "Epoch 4/10\n",
      "192/192 [==============================] - 48s 250ms/step - loss: 0.0230 - accuracy: 0.9925 - val_loss: 0.1054 - val_accuracy: 0.9737\n",
      "Epoch 5/10\n",
      "192/192 [==============================] - 50s 261ms/step - loss: 0.0189 - accuracy: 0.9936 - val_loss: 0.0968 - val_accuracy: 0.9781\n",
      "Epoch 6/10\n",
      "192/192 [==============================] - 50s 260ms/step - loss: 0.0146 - accuracy: 0.9949 - val_loss: 0.0630 - val_accuracy: 0.9857\n",
      "Epoch 7/10\n",
      "192/192 [==============================] - 44s 231ms/step - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.0528 - val_accuracy: 0.9881\n",
      "Epoch 8/10\n",
      "192/192 [==============================] - 46s 238ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.1021 - val_accuracy: 0.9797\n",
      "Epoch 9/10\n",
      "192/192 [==============================] - 50s 261ms/step - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.0783 - val_accuracy: 0.9817\n",
      "Epoch 10/10\n",
      "192/192 [==============================] - 48s 252ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.0549 - val_accuracy: 0.9893\n",
      "Test loss: 0.04337843507528305 / Test accuracy: 0.9907000064849854\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_batch.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(f\"default lr: {tf.keras.backend.get_value(model_batch.optimizer.lr)}\")\n",
    "\n",
    "# Fit data to model\n",
    "history_batch = model_batch.fit(input_train, target_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            validation_split=validation_split)\n",
    "\n",
    "# Generate generalization metric  s\n",
    "score = model_batch.evaluate(input_test, target_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x152372940>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x152372c10>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x152384100>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1523842e0>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'model accuracy')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14e6883d0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.9, 1.01)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABCUUlEQVR4nO3deXxU1d348c93lmSyJyRhC0sABURkUUQFEZf6FOtWq1VbtdLnpdYFRX/Vqm1V5Hm0VmhV1NaVFpcq7mJrXbAC+oDKFhAEwiJCEpYsZM9kmTm/P+7NZJJMwoAZJsv33c5r7j13me9M8HzvPefec8UYg1JKKdWSI9oBKKWU6pw0QSillApJE4RSSqmQNEEopZQKSROEUkqpkDRBKKWUCkkThFKAiPxdRP43zHV3isgPIh2TUtGmCUIppVRImiCU6kZExBXtGFT3oQlCdRl2084dIrJeRKpE5HkR6SMi/xaRChFZLCJpQetfICIbRaRURJaIyDFBy8aLyBp7u4WAp8VnnSciOfa2y0VkTJgxnisia0WkXER2i8isFstPtfdXai+fbpfHicifROQ7ESkTkc/tstNFJC/E7/ADe3qWiLwhIi+JSDkwXUQmisgK+zP2iMgTIhITtP2xIvKxiJSIyD4R+a2I9BWRahFJD1rveBEpFBF3ON9ddT+aIFRXczFwNjAcOB/4N/BbIBPr3/MtACIyHHgFuNVe9j7wnojE2JXlO8CLQC/gdXu/2NuOB+YDvwLSgaeBRSISG0Z8VcAvgFTgXOAGEfmxvd/BdryP2zGNA3Ls7eYCJwCT7Jh+A/jD/E0uBN6wP/NlwAfcBmQApwBnATfaMSQBi4EPgP7AUcAnxpi9wBLg0qD9XgW8aoypDzMO1c1oglBdzePGmH3GmHzgM+BLY8xaY4wXeBsYb693GfAvY8zHdgU3F4jDqoBPBtzAo8aYemPMG8DKoM+4DnjaGPOlMcZnjFkA1NrbtcsYs8QY87Uxxm+MWY+VpKbai38OLDbGvGJ/brExJkdEHMB/AzONMfn2Zy43xtSG+ZusMMa8Y39mjTFmtTHmC2NMgzFmJ1aCa4zhPGCvMeZPxhivMabCGPOlvWwBcCWAiDiBn2ElUdVDaYJQXc2+oOmaEPOJ9nR/4LvGBcYYP7AbyLKX5ZvmI1V+FzQ9GPi13URTKiKlwEB7u3aJyEki8qndNFMGXI91JI+9j+0hNsvAauIKtSwcu1vEMFxE/ikie+1mpwfDiAHgXWCUiAzBOksrM8Z8dZgxqW5AE4TqrgqwKnoARESwKsd8YA+QZZc1GhQ0vRt4wBiTGvSKN8a8Esbn/gNYBAw0xqQATwGNn7MbGBZimyLA28ayKiA+6Hs4sZqngrUckvmvwGbgaGNMMlYTXHAMQ0MFbp+FvYZ1FnEVevbQ42mCUN3Va8C5InKW3cn6a6xmouXACqABuEVE3CLyE2Bi0LbPAtfbZwMiIgl253NSGJ+bBJQYY7wiMhGrWanRy8APRORSEXGJSLqIjLPPbuYDfxaR/iLiFJFT7D6PXMBjf74b+D1wsL6QJKAcqBSRkcANQcv+CfQTkVtFJFZEkkTkpKDlLwDTgQvQBNHjaYJQ3ZIxZgvWkfDjWEfo5wPnG2PqjDF1wE+wKsISrP6Kt4K2XQVcCzwBHAC22euG40ZgtohUAPdiJarG/e4CfoSVrEqwOqjH2otvB77G6gspAf4IOIwxZfY+n8M6+6kCml3VFMLtWImpAivZLQyKoQKr+eh8YC+wFTgjaPn/YXWOrzHGBDe7qR5I9IFBSqlgIvIf4B/GmOeiHYuKLk0QSqkAETkR+BirD6Ui2vGo6NImJqUUACKyAOseiVs1OSjQMwillFJt0DMIpZRSIXWbgb0yMjJMdnZ2tMNQSqkuZfXq1UXGmJb31gDdKEFkZ2ezatWqaIehlFJdioi0eTmzNjEppZQKSROEUkqpkDRBKKWUCkkThFJKqZA0QSillApJE4RSSqmQNEEopZQKSROEUkqpkDRBKKWUCkkThFJKqZA0QSillApJE4RSSqmQNEEopZQKSROEUkqpkDRBKKWUCiliCUJE5ovIfhHZ0MZyEZF5IrJNRNaLyPFBy64Wka326+pIxaiUUqptkTyD+DswrZ3l5wBH26/rgL8CiEgv4D7gJGAicJ+IpEUwTqWUUiFE7IlyxphlIpLdzioXAi8YYwzwhYikikg/4HTgY2NMCYCIfIyVaF6JVKxKRYMxBp/x0eBvaHoZ673eX9+s3Gd8GGMQEevV+D9p8d443WLegQOEZvMiAlhlDgkxb68LBOYd4iDOFRcoV91bNB85mgXsDprPs8vaKm9FRK7DOvtg0KBBkYlSfW/GGOr99dT56pq/++uo99W3uczn9+E3fgD8xo/BYIwJTLdc1mzemKb18bda5scPBvz4A/ts63OCK/GWFXdjhR78qvfXhywP3qZxP12RU5ykxKZYr5iUpunYFFJjU1uVNZbHu+I1sXQxXfqZ1MaYZ4BnACZMmGCiHE6XYYzB6/NSXltOeV05FXUVlNdZ0+W15VTWV1Lnq6PB30Cdv65V5R2o1O0KvnGdBn9Dq8q/zl/X6SvClkfZDhzNjqhd4sLlaOMlLtwOd2De4/IEyoPXC14n1P4Cy9v6LHEhIoHE15i8rP+HLgtOdkDb83ZZIKlimiXY4Hef8VFZV0lpbSlltWWU1ZWxr3ofuQdyKa0tpaahps3f2eVwtU4eMXZSCZFQGtfVM5boiWaCyAcGBs0PsMvysZqZgsuXHLGoughjDNUN1YFKvrFyD0yHmK+oqwiU1fvr292/IMQ4Y4hxxOB2unE73MQ4Y5q9ux1uPC4PSY6kkMtalsU4Y3A5XE37DV6/5Wc4YnA6nDjE0VRx29PBzSoOsbrRgptAWs43rh/cbNKyWUV1jDpfHeV15ZR6SymrK6O0tpTy2nLKaq3psroyK7HUlrGncg+bajdRXlfebmJxO9yBpJEck0xKbApJMUkIYiVEW2MSA5reg8oaE2eby+35tpY3Jt/g5UDg4MLpcDb7d+UQB05pKmssb7PM3s/Bylr+23aKk4y4DH4w+AeH/XdrSzQTxCJghoi8itUhXWaM2SMiHwIPBnVM/xdwd7SCjDS/8VNWW0ZRTREl3hLKasuaVfAtj+6DK3uf8bW5X0FIikkiOSaZ5NhkkmOS6RPfp9l8YNrdvCwhJgG3w30EfwXVXcQ4Y8iIyyAjLuOQtqv11QYSRyCp2AmmsbzxjCWvMo/KukrA+ncONEv0LcuC+1Ial7VaHmo/bSwP3gcQOLNqbJYMNE36fYEzucamTZ85tLJwjckY07UShIi8gnUmkCEieVhXJrkBjDFPAe8DPwK2AdXAL+1lJSLyP8BKe1ezGzusuwpjDBX1FRTXFFNUU0RxTTHFXmu6cb7xvcRbQoMJ3QTjEhfJscmBij4lNoWBSQMDlXnLBNA4nRSTRKI7MXA0rVRnF+uMpXd8b3rH9452KJ1Gs+ZEO2kE97fV1DdQVlNLWU0dDnFGJIZIXsX0s4MsN8BNbSybD8yPRFzfR3V9devKPmi6pKYkMF3nr2u1vUtc9IrrRbonnYy4DEb2Gkl6nDWdHpdOuic9kAiSY5K17VWpbsoYg7feT7m3nvKaevu9wXr3NrQuq7HKK+z3cm89dQ3+wP7GD0rl7RtDXsvzvXTpTuqO4PP72F+9v1VlH+qov7qhutX2gpDmSQucVg9OHtxU4duVf4bHmk+JTdGjeqW6AWMMVXU+KoIq8YoQFXrLir4iqKze134TUozTQXKcm+Q4F8keNylxbgamxVllHqs8yeMm2eOib7InIt+zxyeIEm8J//Xmf7UqT4lNCRzpj84YHZhurPwbp1NjU3E5evzPqFSXYYyhtsHfrMKu8DbYr/pAeVNl3lhuvTeu5z9IF4HH7bArcjdJHhdp8TEMTk8g2eNqVsk3rtNYnuSxyjzuyDQbHYoeX7OledKYdcqsZpV/L08vYpwx0Q5NKdVCY+VeHXT0Hlx5B1figYq/tmm9xoq/zudv93NEICnWOkJvrLCzUj0keZJI8rgCZUntVPKxruhX8N9Xj08QLoeLi4dfHO0wlOryGitvb72Pmnof1XU+aup8zea99VZZdZ1VFpiv9+GtC9quvvm6jfuoqfdhwri4JyHGGai8kzxu0hNjGJKRYFfudqVvV+iNZcl2eZLHRUKMC4dD+/96fIJQSjXx+w2lNfUUVdZSVFFLYWUtRZV1gfniqjqqahtCVvo19b6DNru05BCIczuJi3ERF+Owpt1O4mKcpCfEEJfmJM7dcpmLOLfDTgDuZkf0yR43iR4XTq3cO4QmCKW6OZ/fcKC6jsKKWquir6ylqMKq9AMJwF5WUlVHQ4ha3u0U0hNiSU+MISHWRVpCDP2DKvPGd4/bSXxM6/I4d9CyoPIYp96s2JlpglCqC2rw+SmpqmtVwRcFHfFbCaGOkqrakEf2MU4HGYkxZCTF0jfFw+isZDKTYslIbHplJsWQkRhLSpxbK/IeSBOEUp1ITZ2P/RVe9pXXsq/cy75yL/srrOmmM4A6DlTXhWyLj3U5rMo9KZYBaXGMH5TarMJvTAgZibEke1xa6at2aYJQ6giobfCxv7y2ReVfy/5yL/sqmqbLva3vqo9xOeidFEtmUizZ6QlMyO5lHd0nxgSSQWPlnxirlb7qOJoglPoe6n1+CiuCKvwKb2B6X7k3kBQOVLceHNHtFHoneeidHMtRmYlMHpZO72QPfZI99E6KpU+yhz7J2ryjokcTRA/i93ppKCy0XvsLm6ZbvPwVFTgSEnAkJeFISsSZmIQjKQlnYmKLskScSUk4EpNwJtnL7GmJ69rDhDT4/BRX1bWo7O3piqaKv6iy9ZAqToeQmRhLn+RYBqXHc+KQNPok2RV/cmwgAaTFx+illKpT0wTRxRlj8FdUNK/k26j8/ZWVrXfgcuHKyMCVmYk7K4u4ceNwJiXiq6rCX1GJv6ICX2Ul9bt3462ssMoqKznoxehOZ1BCaUouzqREHAlB0+0kGkdCxz9gxuc3FFfVttvcs7/cautv2bHrEMhIjKV3ciz9UjyMHZhKn+SmI/3edhLolRDT5mWW/tpafGVl+Pfsw1tejq+sDF9ZOf7yssC0r6wMnz3vb5yvqECcTitxx8dbr8bp9t4T2l4uHk+XTuIq8jRBdFLG78dXUtK6ot9fSENRUbMyU1vbanvxeHBlZuLKzCR2+HASJk8OzLsyM3H1tt6dqamI49DGhzJ+P/7qait5VFTgr7QTiZ08/JX2dEUFvsakUlFBfUEBtXbC8VdUgL/9u1kBJC4Oh8eDxHlweBqnW5TFeSDWQ50rhipxUYGLCr+TUr+DEp+DogYHhXWwrxb21UG1uKl1uql1xlDrctMgTjKSGiv4WEb3T6F3Umyz5p6+KR7SE2JwOR0Ynw9feTn+sjJ85eX4yvbh21aOr7wMf1kZRYFKvhxfWWlTJV9ejvF62/mygiM5GWdyMs6UFJzJycRkZVllSckYv8/63auqAu++ygrq9+21560yGsJ8QJPDETqxtJNsnKmpJJ5xBo7Y2DD/taiuTBPEEWSMwV9Vja+okIbiYhoKi6zKvrgIX1GRNd9Y8RcXg6/18x4cycmBSj5u/PjmlX5Q5e9ISIjY0aE4HDgTE3EmJuLu1++w9mGMwVRXB5JF60RjJRLj9eKtrKa6vApvZTV11dU0VNbgLyzHeGuRWi/O+lrc9XXE+upw+30kA8m08ZzaUJzOZonHEedBPE1JyNTXU11WToVdyfsrKtr/feLjm1fy2YOtSj4l1SpPtcodySnWOinWuo6kpENO1qH46+rwV1VZv2/Qe8vk0tZ7/f59mKpqfNVVmKpq/NXNB6n0jB7NgHmP4e7f/3vH2pV5v/mG8g8+tGZEQKx3EYc9b5WJw57HLnPYz6doLHPYzwsPbONovj+Ho2nblvvz14G3HFdyPIkXXtnh31FMOPetdwETJkwwq1atispn+71eGoqK26/4i4tpKCrC1IR4cpbDgTO9F66MTFyZGfZ760rflZGBwxOZURs7A7/fsGVfBSu2F7N8ezFb9pWzr7y22bDGjVLi3IHmnczGDt2kWPokuOkdC5kxhl5OP+76OvxeL/6aGozXi7/Gi/HW4K/x4ve2UVbjxe/12stqEJfLqsBTgir5xkq9seJPaUoIEtO9xvEyfr+dzMupWbWSPbP+B3G5yHrgdySMGwW+evDV2u911quhrmk6sLyuaZ2DLW+2TmNZ0GcYAwmZkNQHEvva78HTfSE+3ap8O/r3MIbSV19l34N/wPh81mcYY50RR6k+9fRzM+TT9Ye1rYisNsZMCLVMzyDaYOrraSgpsSv7QnwtK/7G6aKi0G37gDMtDVdGOs6MDOLGjcOVnm4ngAycGda7KyPDauZxdv2BvQ6VMYZvi6pYvr2YFduLWbGjmJIqq9M3Oz2e4wel0TfFQ5+kps7dxunOMNLlEeH3QVUhVOyFuqrWlaivRUXbUBvG8uB9HLzyFl8t4qvDgfXEr9jTnOR93otdM+6g99hyeo2o4rBPVp2x4IwBp9t6d8XY841l9vKYBHCmNS03xvpd9m+C7Uugtqz1vh0uSOjdIom0fLdfzvCeoOivrmbPrFmUL3qPhClT6P/wH3GlpbVazxjTPGkY+/lwfj801EPlXijfiynLt/625Xus94q9mPI9ULHP+v0JyjlGrKSY2BsS+mASelvfL7EPkp59OL/+QfX4BOErK6Por0+1OuL3lZaGXN+RlGRV7OnpxB4zkoSMTLuiT29e8ffqhbj1sZ0t5R2oDiSE5duL2Fdu9Z/0S/Fw+ohMJg3L4JRh6WSlxkU50ggzBmoOQMUe6xWoIILf90DlPjAH76sJyRkLrtimyjfwHlwWAzGJLdZr+QqqvB1uYl2xZP/YsOfZj9i/cgvexCn0u+kyHIlJbeyjcfsWn+FwcfiZpYW6auu3qtxn/X4t38t2Q95KqC4KvX18ejtJxHqvLfSSf/ud1G7bTsYtN5Nx/fWtmwTrqqB8D1JRAOVBr4o9SON01f7Wf1NnLCT3g6T+0PtEazo5C5Ls9+R+h5TIOkqPb2LyVVSw9bSpgaN5q5JPt6czAxV/Y+WvnXOHZn+5lxU7ilm+rZjlO4rYXWI1sWUkxnDy0HQmDctg0rB0Bqd3/BVLUVNbYR8VFoSu9Bvnfa0vkSWuFyT3h6S+9quf9Z7YF2KTmle6rSr1oCNuh7PjKt82GGMofuZZCh99lNjhwxnwxOPEDBwY0c/83nz1ULnfOoKv2BfiPejlb+rsL9/tYc+XqYhT6P/DOBKP6W8lD1ecneALoKIAvCHOZDwpVsWf3L8pCSQHvZL6Q3yviP+92tJeE1OPTxDGmO5TMR0Jfh/U11hNFQ01UO+FBvtVX0NFZQVb84vYvqeI7/YWU1ZZiYc6UtwNDE11MjjZSVaikOJuQBpqA9s17cNr7beh1jrKcsVa/xG6PeCyX+64oGmPtdwV21TujmuxXYvlwds17jecI7N6b4vKPlTlvxfqQjQ5xiTZlUNQpZ/UL+jV1zpCdHe9PqbKzz4j//Y7AMiaO5fEKadGOaIO4PdDTQnmQB77H3+akneXEDe0N1lXjMHtLmtKKg211t8ucLTfouJP7mc1j3VimiDUwRkDe3Jg49uw60uor7L+8QdX2PU14G99R3DYHK52KvkQFTvSlIgaPz+QUIITlD0d6og8XOJsO4HUVVmVf82B1tsFmgZaVv79g+b7WEf/3Vjd7t3kzbiZ2txcMm+9lfTrru3yB171e/eSf9v/o2btWtKuuoo+d9ze7S5AAO2kVm0xBvaut5LCxrfhwE6rEh9wonVEFKIir5dY8ir8bCv1saWonu2lPmr8bnzOWAb36cWIAb05dlBvjs7KxB0b37zSd0b4n5vfZ5+JtJdMWiSb4LOWFmdDgfeE3jB4UoiKvy/EpUWtaaAziRk4kOxX/sGee+6l8JFH8G7YQL8//AFnYuc+em5L1YoV5P/6dvxeL1l//hPJP/pRtEOKCk0QPY0xsG9DU1Io2WEdPQ+dClNuh5HnWu2httoGHzm7SgMdy2t3HaDeZ3A5hHEDU5k0Jp2Th6Vz/KC06F9Z5HBap/Od/JS+u3LEx9N/7hw8x41m/5y51F52GQMef5zYoUOiHVrYjN9P8TPPUDjvcWKGDGHwvMeIHTYs2mFFjTYx9QTGwP5vmpJC8TbrZpwhp8GxF8HI8yEhHbCGolifZyWEL3YUs3JnCd56PyJwXFYKpwxN55Rh6ZyY3YuEWD2+UKFVffEl+bfdhqmvp//DfyTpzDOjHdJB+UpLyb/zTqqWLiP5vPPod/8sHAnd/2BD+yB6qv2bmpJCUa6VFLJPtZLCMRdAQkZg1c17y3l7bT7vri1gb7k1HMSIPkmcMiydScPSOWlIOinxetmuCl99QQF5N9+Cd+NGMm68kYwZN3XIneKRUPP1BvJnzqS+sJA+d99F2s9+1uX7UMKlfRA9SWFuU1Io3ASIlRRO+pWVFBJ7B1bdV+7l3Zx83l5bwKY95TgdwtThmdx1zkhOPTqDjES9pFcdPnf//gx++SX23j+bor/8Be/GjfSf8zDO5ORohxZgjKF04Wvse+ABnJkZZL/8EnFjxkQ7rE5DzyC6g6JtTUlh/0ZArE7VxjOFpD6BVatqG/hgw17eycnn/7YV4TcwdkAKF43P4ryx/TUpqA7XODTF3gcexJ3Vn4FPPEHs0UdHOyz8NTXsnTWLsncXtXtXdHenTUzdUfF2Oym8A/u+tsoGndKUFJKbBtFr8Pn5fFsRb6/N56ON+6ip9zEgLY6Lxmfx4/FZDMtMjM53UD1K9Zo15M2cib+qmv4PPkDytGlRi6X222/Jv2Umtdu2kTHjJjJuuKHTNn9FmjYxdRclO6yEsPFt6/JUgIEnwbSHrKSQ0jR+qTGGjQXlvLUmn0XrCiiqrCXZ4+Ki47O4aHwWEwan9Zg2VtU5xB9/PEPeeJP8mTPJv/U2vNdsIPO22474OGTlH3zInt/9DnG7GfjssySeOvmIfn5Xogmiszuwsykp7MmxygacCD98EEZdCCkDmq2eX1rDO2vzeWdtPlv3V+J2CmeO7M1F47M4Y2RvYl09ZJA71Sm5+/Rm8AsL2PvggxQ/9zzebzbR/09zj0jTjqmvZ//cP1GyYAGesWMY8Oijhz1cfU+hTUydUemupqRQsMYqyzrBaj4adSGkDmq2erm3nn9/vYe31uTz5bclAEwYnMZFx2dx7nH9SI3vfnd/qq6v9M032Tvrfly9ezPg8Xl4Ro2K2GfV79tn3RW9Zg1pV15Jn9/c0S3vij4c2gfRFVSXwLpXYMNbkG9/j37jrKRw7I8hLbvZ6nUNfpblFvL22nw+3rSPugY/QzISrH6FcVkMSo8/0t9AqUNWs349ebfMxHfgAP3+ZzYpF1zQ4Z9R9cUX1l3RNTXWZ5x7bod/RlcWtT4IEZkGPAY4geeMMQ+1WD4YmA9kAiXAlcaYPHvZw8C5gAP4GJhpuks2C1byLXzxV1j7ItRXQ98xcNZ9VlLoNbTZqsYYcnaX8vbafN5bV8CB6np6JcTwsxMHctHxAxg7IEX7FVSXEjdmDEPefIP8W2+j4Dd3UrNhA33uuKNDhsq37op+lsJ586y7ol9Y0KPvij4cEUsQIuIEngTOBvKAlSKyyBjzTdBqc4EXjDELRORM4A/AVSIyCZgMNF6Q/DkwFVgSqXiPuLxVsPxx2LTIGuriuJ/CKTdB39GtVv2uuIp31hbwTk4+3xZVEetycPaoPlw0PovThmfidvbMqy9U9+BKT2fQ/OfZP3cuJQteoPabTWQ9+giujIyDb9wGX1kZBXfeReWSJSSfey79Zt/fI+6K7miRPIOYCGwzxuwAEJFXgQuB4AQxCvh/9vSnwDv2tAE8QAzWk1ndwL4Ixnpk+P2Q+4GVGHYth9gUmDwTJv6q2WWpAKXVdfxz/R7eXpvP6u8OIAInD0nnhtOHMW10X5I9elez6j7E7abP3XfjGT2aPffcy7cXX8KAx+cd1k1rNRs2WndF799Pn3t+T9rPf65n1ocpkgkiC9gdNJ8HnNRinXXAT7CaoS4CkkQk3RizQkQ+BfZgJYgnjDGbWn6AiFwHXAcwaNCglos7j/oaWPcqrHjCGgcpZZB1aer4K5sNA13b4OPTzft5a00+n27ZT73PMLxPIndOG8mF4/rTv7s/ZU31eCnnn0/sUUeRN+NmvrviSvredy+pl1wS1rbGGEpfe926Kzo9neyXXiRu7NgIR9y9Rfsy19uBJ0RkOrAMyAd8InIUcAzQeA3nxyIyxRjzWfDGxphngGfA6qQ+YlGHq6oYVj4HXz1jPeqw3zi4ZD4cc2Groa93FVdz0V/+j+KqOjKTYrn6lGwuOj6LUf2S9ehH9SieY44h+43XKfj17ez5/T3UfL2BPr/7LY52rjqy7oq+n7J33yXh1FPpP+fhHnlXdEeLZILIB4KfPzjALgswxhRgnUEgIonAxcaYUhG5FvjCGFNpL/s3cArQLEF0WkXb4IsnIecf1jMFhk+DSTfD4MltPjvgvfUFFFfVMX/6BKYO743ToUlB9VyutDQGPvsMhY8+RvGzz1K7eTNZ8+bh7tO71bp1O3eSd8tMarduJWPGDDJuuP6I33zXXUWyd3MlcLSIDBGRGOByYFHwCiKSISKNMdyNdUUTwC5gqoi4RMSN1UHdqompUzEGdn0Br14BT0yAtS/BmEvhpq/g5wutAfPaORNYmlvI6KxkzhzZR5ODUoA4nfT+9f8j69FH8W7dyrcXX0z16tXN1in/6CO+vfgSGvbvZ+Azz5A54yZNDh0oYgnCGNMAzAA+xKrcXzPGbBSR2SLSeLHz6cAWEckF+gAP2OVvANuBr7H6KdYZY96LVKzfi98H37wLz58N838I3/0fnHY73LoBLngcMkccdBcV3nrWfHeA047OPAIBK9W1JE/7IUMWvoozIYHvrp5OycsvY+rr2ffHh8m/ZSYxRw1jyFtvdo9nYXcyeqPc4aqrspqQVjxhDYeRlg2nzIBxPz/kJ5p9uHEvv3pxNa9edzInD02PSLhKdXW+8nIKfnMnlUuW4Orbl4a9e0m74gr63PkbvSv6e9DB+jpS5X6r03nlc9ZD7LMmwNmzYeR51iMvD8PS3EISY10cP0g71ZRqizM5mQF/eZKiJ//CgYUL6f+nuXpXdIRpgghX4RbrbGHdQvDVWc9unnSzNZrq97jKyBjDstxCThmWToxLb3hTqj3icJB58wwyb54R7VB6BE0Q7THG6lNY/rh1g5vLA+OvgJNvgoyjOuQjdhRVkXeghuun6hAASqnORRNEKL4G2PSulRgK1kJ8Bpx+N5x4TbPnOHeEZbmFAEwdrh3USqnORRNEsNoK6/LUFX+Bsl2QfhSc9yiMvRzckbmLeWluIUMzEhjYS0dfVUp1LpogAMr3wFdPw6r54C2DQZPgnD9aN7hF8DGE3nofX+wo5vITO/EwIUqpHksTRPF2ePIkMD7rsZ2TboYBIa/46nCrdh7AW+/X5iWlVKekCaLXUDjz99aT2noNOaIfvTR3PzFOBycN7XVEP1cppcKhCUIETr01Kh+9LLeIiUN6ER+jfwalVOejF95HyZ6yGrbsq+C04R17VZRSSnUUTRBR8lluEQBTh7cenVIppToDTRBRsjS3kL7JHob3SYx2KEopFZImiCho8Pn5fFsRU47O0IcBKaU6LU0QUbAur4yymnqmjtDLW5VSnZcmiChYlluIQ+DUo7SDWinVeWmCiIKluYWMHZhKaryOYa+U6rw0QRxhpdV1rM8r1afHKaU6PU0QR9jn24rwG7T/QSnV6WmCOMKWbikkJc7N2AGp0Q5FKaXapQniCDLGsGxrIacenYHToZe3KqU6N00QR9CWfRXsK69lqvY/KKW6AE0QR1Dj0+NO0+G9lVJdgCaII2hpbiEj+iTRN8UT7VCUUuqgwkoQIvKWiJwrIppQDlN1XQMrvz2gVy8ppbqMcCv8vwA/B7aKyEMiMiKCMXVLX+wops7n1/sflFJdRlgJwhiz2BhzBXA8sBNYLCLLReSXIuKOZIDdxbLcIjxuBxOy06IdilJKhSXsJiMRSQemA9cAa4HHsBLGxxGJrJtZmlvIKUPT8bid0Q5FKaXCEtazLkXkbWAE8CJwvjFmj71ooYisilRw3cXukmq+LariF6cMjnYoSikVtnAfhjzPGPNpqAXGmAkdGE+3tNS+vHWqXt6qlOpCwm1iGiUiqY0zIpImIjdGJqTuZ2luIQPS4hiSkRDtUJRSKmzhJohrjTGljTPGmAPAtQfbSESmicgWEdkmIneFWD5YRD4RkfUiskREBgQtGyQiH4nIJhH5RkSyw4y1U6lr8LNiezFTh2fq0+OUUl1KuAnCKUG1m4g4gXYfZmCv8yRwDjAK+JmIjGqx2lzgBWPMGGA28IegZS8Ac4wxxwATgf1hxtqprNl1gMraBr17WinV5YSbID7A6pA+S0TOAl6xy9ozEdhmjNlhjKkDXgUubLHOKOA/9vSnjcvtROIyxnwMYIypNMZUhxlrp7IstxCXQ5g0LD3aoSil1CEJN0HciVWB32C/PgF+c5BtsoDdQfN5dlmwdcBP7OmLgCT7ctrhQKl9B/daEZljn5E0IyLXicgqEVlVWFgY5lc5spbmFnL84DSSPHq7iFKqawn3Rjm/MeavxphL7NfTxhhfB3z+7cBUEVkLTAXyAR/W1VVT7OUnAkOx7sFoGdczxpgJxpgJmZmdrwmnsKKWjQXlevWSUqpLCvc+iKOx+gdGAYGR5owxQ9vZLB8YGDQ/wC4LMMYUYJ9BiEgicLExplRE8oAcY8wOe9k7wMnA8+HE21l8tlUvb1VKdV3hNjH9Dfgr0ACcgdWB/NJBtlkJHC0iQ0QkBrgcWBS8gohkBA0AeDcwP2jbVBFprFnPBL4JM9ZOY1luIRmJMYzqlxztUJRS6pCFmyDijDGfAGKM+c4YMws4t70NjDENwAzgQ2AT8JoxZqOIzBaRC+zVTge2iEgu0Ad4wN7Wh9W89ImIfA0I8OwhfbMo8/sNy7YWMeXoTBz69DilVBcU7p3UtfaR/lYRmYHVVJR4sI2MMe8D77couzdo+g3gjTa2/RgYE2Z8nc7GgnJKquo4bXhGtENRSqnDEu4ZxEwgHrgFOAG4Erg6UkF1B0tzrds2pujw3kqpLuqgZxD25aWXGWNuByqBX0Y8qm5gWW4Ro7OSyUiMjXYoSil1WA56BmH3B5x6BGLpNsq99azZdUCvXlJKdWnh9kGsFZFFwOtAVWOhMeatiETVxS3fVkyD3+jT45RSXVq4CcIDFGNdbtrIAJogQli2tZDEWBfHD9anxymluq6wEoQxRvsdwmSMYemWQiYNS8ftDPuBfUop1emEeyf137DOGJoxxvx3h0fUxe0oqiK/tIYbzxgW7VCUUup7CbeJ6Z9B0x6sgfUKOj6crm/pFmt4De1/UEp1deE2Mb0ZPC8irwCfRySiLm7Z1kKGZiYwsFd8tENRSqnv5XAbyY8GendkIN2Bt97HFzuK9exBKdUthNsHUUHzPoi9WM+IUEFW7izBW+/X+x+UUt1CuE1MSZEOpDtYuqWQGJeDk4b2inYoSin1vYXVxCQiF4lIStB8qoj8OGJRdVHLthYyMbsX8THh9v0rpVTnFW4fxH3GmLLGGWNMKXBfRCLqovaU1ZC7r1Kbl5RS3Ua4CSLUenqYHGRZrn15qyYIpVQ3EW6CWCUifxaRYfbrz8DqSAbW1SzLLaJvsofhfQ76mAyllOoSwk0QNwN1wELgVcAL3BSpoLqaBp+fz7YWctrwDET06XFKqe4h3KuYqoC7IhxLl7Uur4xybwNTh+utIUqp7iPcq5g+FpHUoPk0EfkwYlF1MUtzC3EInHqUPl5UKdV9hNvElGFfuQSAMeYAeid1wLLcQsYNTCUl3h3tUJRSqsOEmyD8IjKocUZEsgkxumtPdKCqjnV5pXr1klKq2wn3UtXfAZ+LyFJAgCnAdRGLqgv5fFsRxqD3Pyilup1wO6k/EJEJWElhLfAOUBPBuLqMpbmFpMa7GTMgNdqhKKVUhwp3sL5rgJnAACAHOBlYQfNHkPY4xhg+21rI5KMycDr08lalVPcSbh/ETOBE4DtjzBnAeKA0UkF1FVv2VbCvvFabl5RS3VK4CcJrjPECiEisMWYzMCJyYXUN+vQ4pVR3Fm4ndZ59H8Q7wMcicgD4LlJBdRXLthYysm8SfVM80Q5FKaU6XLid1BfZk7NE5FMgBfggYlF1AdV1Daz89gDTJ2dHOxSllIqIQx6R1RizNBKBdDVf7CimzqdPj1NKdV+H+0zqHm/plkLi3E4mZKdFOxSllIoITRCHadnWIk4Zlk6syxntUJRSKiIimiBEZJqIbBGRbSLSajRYERksIp+IyHoRWSIiA1osTxaRPBF5IpJxHqpdxdV8W1TFaUfr4HxKqe4rYglCRJzAk8A5wCjgZyIyqsVqc4EXjDFjgNnAH1os/x9gWaRiPFxLt1qXt04doeMVKqW6r0ieQUwEthljdhhj6rAeNHRhi3VGAf+xpz8NXi4iJwB9gI8iGONhWbqlkIG94shOj492KEopFTGRTBBZwO6g+Ty7LNg64Cf29EVAkoiki4gD+BNwe3sfICLXicgqEVlVWFjYQWG3r67Bz4rtRUwdnqlPj1NKdWvR7qS+HZgqImuBqUA+4ANuBN43xuS1t7Ex5hljzARjzITMzCNzuemaXQeoqvPp3dNKqW7vkO+DOAT5wMCg+QF2WYAxpgD7DEJEEoGLjTGlInIKMEVEbgQSgRgRqTTGRP2xp0tzC3E5hFOGpUc7FKWUiqhIJoiVwNEiMgQrMVwO/Dx4BRHJAEqMMX7gbmA+gDHmiqB1pgMTOkNyAOvpcScMTiPJo0+PU0p1bxFrYjLGNAAzgA+BTcBrxpiNIjJbRC6wVzsd2CIiuVgd0g9EKp6OUFhRy8aCcn16nFKqR4jkGQTGmPeB91uU3Rs0/QbwxkH28Xfg7xEI75B91nh5qyYIpVQPEO1O6i5laW4hGYkxjOqXHO1QlFIq4jRBhMnvN3y2tYjTjs7EoU+PU0r1AJogwrShoIySqjrtf1BK9RiaIMK0LLcQEZii4y8ppXoITRBhWppbyOj+KaQnxkY7FKWUOiIiehVTd1HurWfNrlJumDos2qEo1eHq6+vJy8vD6/VGOxQVQR6PhwEDBuB2h38PlyaIMCzfVozPb7T/QXVLeXl5JCUlkZ2dreOLdVPGGIqLi8nLy2PIkCFhb6dNTGFYmltIUqyL8YNSox2KUh3O6/WSnp6uyaEbExHS09MP+SxRE8RBGGNYllvIpKPScTv151LdkyaH7u9w/sZa4x3E9sIq8ktrtHlJKdXjaII4iGW51vAaOry3Up3T9OnTeeONdkfsaWbnzp384x//OOh62dnZFBUVhb3f0tJS/vKXv4S9frAf/ehHlJaWHta2kaQJ4iCW5hYyNDOBgb306XFKdQfhJohD1V6CaGhoaHfb999/n9TU1A6P6fvSq5ja4a338eW3xfxs4qBoh6LUEXH/exv5pqC8Q/c5qn8y951/bJvLd+7cyTnnnMOpp57K8uXLycrK4t133yUuLo6cnByuv/56qqurGTZsGPPnzyctLa3VPhYvXsxDDz1EeXk5f/7znznvvPPYuXMnV111FVVVVQA88cQTTJo0ibvuuotNmzYxbtw4rr76am655RbuvPNOPvjgAxwOB9deey0333wzAI8//jjvvfce9fX1vP7664wcObLN73HXXXexfft2xo0bx9lnn825557LPffcQ1paGps3byY3N5cf//jH7N69G6/Xy8yZM7nuuusA62xl1apVVFZWtvlbRIOeQbTjq29L8Nb7tf9BqQjbunUrN910Exs3biQ1NZU333wTgF/84hf88Y9/ZP369Rx33HHcf//9IbffuXMnX331Ff/617+4/vrr8Xq99O7dm48//pg1a9awcOFCbrnlFgAeeughpkyZQk5ODrfddhvPPPMMO3fuJCcnh/Xr13PFFYHH0ZCRkcGaNWu44YYbmDt3LgCrVq3immuuaRXDQw89xLBhw8jJyWHOnDkArFmzhscee4zc3FwA5s+fz+rVq1m1ahXz5s2juLg47N8iGvQMoh3LcguJcTk4eYg+PU71DO0d6UfSkCFDGDduHAAnnHACO3fupKysjNLSUqZOnQrA1VdfzU9/+tOQ21966aU4HA6OPvpohg4dyubNmxkyZAgzZswgJycHp9MZqKRbWrx4Mddffz0ul1Ud9urVK7DsJz/5SSCmt956C4AJEybw3HPPhfW9Jk6c2Oy+g3nz5vH2228DsHv3brZu3Up6evP6JdRvES2aINqxNLeQk4b0Ii7GGe1QlOrWYmObhrBxOp3U1NQc0vYtL+EUER555BH69OnDunXr8Pv9eDyew47L6XQetB8hlISEhMD0kiVLWLx4MStWrCA+Pp7TTz895H0J3/e36EjaxNSGgtIatu6v1IcDKRUlKSkppKWl8dlnnwHw4osvBs4mWnr99dfx+/1s376dHTt2MGLECMrKyujXrx8Oh4MXX3wRn88HQFJSEhUVFYFtzz77bJ5++ulAAigpKTmseFvut6WysjLS0tKIj49n8+bNfPHFF4f1OUeSJog2BC5v1QShVNQsWLCAO+64gzFjxpCTk8O9994bcr1BgwYxceJEzjnnHJ566ik8Hg833ngjCxYsYOzYsWzevDlwND9mzBicTidjx47lkUce4ZprrmHQoEGMGTOGsWPHHvQKp7b6INLT05k8eTKjR4/mjjvuaLV82rRpNDQ0cMwxx3DXXXdx8sknH8YvcmSJMSbaMXSICRMmmFWrVnXY/m58eTVrd5Wy/K4z9S5T1a1t2rSJY445JtphqCMg1N9aRFYbYyaEWl/PIEJo8Pn53H56nCYHpVRPpQkihHV5pZR7G5g6QpuXlFI9lyaIEJbmFuEQmDxMnx6nlOq5NEGEsDS3kHEDU0mJD//BGkop1d1ogmjhQFUd6/NKmTq8d7RDUUqpqNIE0cJn24owBk4brs1LSqmeTRNEC8tyC0mNdzNmQGq0Q1FKhaE7DPcN8Oijj1JdXX3Y20eCJoggjU+PO/WoDJwOvbxVqe4oGsN9h6MzJggdiynI5r0V7K+o1eE1VM/177tg79cdu8++x8E5D7W5uLsO9z1nzhzmzJnDa6+9Rm1tLRdddBH3338/VVVVXHrppeTl5eHz+bjnnnvYt28fBQUFnHHGGWRkZPDpp59+zx+9Y+gZRJClOryGUlHRHYf7/uijj9i6dStfffUVOTk5rF69mmXLlvHBBx/Qv39/1q1bx4YNG5g2bRq33HIL/fv359NPP+00yQH0DKKZZbmFjOybRJ/kQx/1UaluoZ0j/UjqjsN9f/TRR3z00UeMHz8egMrKSrZu3cqUKVP49a9/zZ133sl5553HlClTwviFoiOiCUJEpgGPAU7gOWPMQy2WDwbmA5lACXClMSZPRMYBfwWSAR/wgDFmYSRjraptYNXOA/xycnYkP0YpFUJ3HO7bGMPdd9/Nr371q1bL1qxZw/vvv8/vf/97zjrrrDYHIYy2iDUxiYgTeBI4BxgF/ExERrVYbS7wgjFmDDAb+INdXg38whhzLDANeFREUiMVK8AXO4qp8/m1/0GpTqKrD/f9wx/+kPnz51NZWQlAfn4++/fvp6CggPj4eK688kruuOMO1qxZE3L7ziCSZxATgW3GmB0AIvIqcCHwTdA6o4D/Z09/CrwDYIwJnAsaYwpEZD/WWUZppIJdlltInNvJCdmtO8CUUtGxYMGCQCf10KFD+dvf/hZyvcbhvsvLy5sN933xxRfzwgsvMG3atJDDfU+fPp2bb76Z3NxcxowZg9vt5tprr2XGjBltxrRq1SqeeuqpVs1MwcN9n3POOcyZM4dNmzZxyimnAJCYmMhLL73Etm3buOOOO3A4HLjdbv76178CcN111zFt2rRAX0RnELHhvkXkEmCaMeYae/4q4CRjzIygdf4BfGmMeUxEfgK8CWQYY4qD1pkILACONcb4W3zGdcB1AIMGDTrhu+++O+x4T5/zKcMyE3l++omHvQ+luiId7rvn6GrDfd8OTBWRtcBUIB+rzwEAEekHvAj8smVyADDGPGOMmWCMmZCZefhNQ98VV7GzuFqvXlJKqSCRbGLKBwYGzQ+wywKMMQXATwBEJBG42BhTas8nA/8CfmeMieiz+fTpcUop1VokzyBWAkeLyBARiQEuBxYFryAiGSLSGMPdWFc0Ya//NlYHdvj30B+mpblFDOoVT3Z6fKQ/SimluoyIJQhjTAMwA/gQ2AS8ZozZKCKzReQCe7XTgS0ikgv0AR6wyy8FTgOmi0iO/RoXiTjrGvys2F7EacMz9OlxSikVJKL3QRhj3gfeb1F2b9D0G0CrMwRjzEvAS5GMrVFJVR3jBqVy1sg+R+LjlFKqy+jxd1L3TfHw8jUnRzsMpZTqdKJ9FZNSqofbuXMno0ePPqRt/v73v1NQUHDQddq7n6GzWLJkCcuXLz/k7VatWhUYXypSNEEopbqccBLEkXCow2+E0l6CaG//EyZMYN68ed/789vT45uYlFJN/vjVH9lcsrlD9zmy10junHhnu+s0NDRwxRVXsGbNGo499lheeOEF4uPjmT17Nu+99x41NTVMmjSJp59+mjfffJNVq1ZxxRVXEBcXx4oVK9iwYQMzZ86kqqqK2NhYPvnkEwAKCgqYNm0a27dv56KLLuLhhx9u9dnZ2dlcffXVrYb1Likp4b//+7/ZsWMH8fHxPPPMM4wZM4ZZs2YFhvQYNGgQI0aM4Ntvv2XHjh3s2rWLRx55hC+++IJ///vfZGVl8d577+F2h36+/c6dO3nqqadwOp289NJLPP744zz//PN4PB7Wrl3L5MmTufzyy5k5cyZer5e4uDj+9re/MWLECJYsWcLcuXP55z//yaxZs9i1a1cghltvvbVDzi70DEIpFXVbtmzhxhtvZNOmTSQnJwcevDNjxgxWrlzJhg0bqKmp4Z///CeXXHIJEyZM4OWXXw6M1HrZZZfx2GOPsW7dOhYvXkxcXBwAOTk5LFy4kK+//pqFCxeye/fukJ8faljv++67j/Hjx7N+/XoefPBBfvGLXwTW/+abb1i8eDGvvPIKANu3b+c///kPixYt4sorr+SMM87g66+/Ji4ujn/9618A3HvvvSxa1OxKf7Kzs7n++uu57bbbyMnJCYzsmpeXx/Lly/nzn//MyJEj+eyzz1i7di2zZ8/mt7/9bcjvsHnzZj788EO++uor7r//furr6w/3zxGgZxBKqYCDHelHysCBA5k8eTIAV155JfPmzeP222/n008/5eGHH6a6upqSkhKOPfZYzj///GbbbtmyhX79+nHiidYwOcnJyYFlZ511FikpKQCMGjWK7777joEDB9JSqGG9P//888BzKc4880yKi4spLy8H4IILLggkIYBzzjkHt9vNcccdh8/nY9q0aQAcd9xx7Ny5E4DZs2eH/Xv89Kc/xel0AlBWVsbVV1/N1q1bEZE2K/5zzz2X2NhYYmNj6d27N/v27WPAgAFhf2YoegahlIq6UMN1e71ebrzxRt544w2+/vprrr32Wrxe7yHtt+Uw4m216R/qsN6NA/+13L5xAL7G7+NwOA6rnyJ4//fccw9nnHEGGzZs4L333mvzNwj3ux4KTRBKqajbtWsXK1asAOAf//gHp556aqAizMjIoLKykjfeaLplKnho7BEjRrBnzx5WrlwJQEVFRYdUjlOmTOHll18GrI7kjIyMZmcnHeVgw3yXlZWRlZUFWJ3zR5ImCKVU1I0YMYInn3ySY445hgMHDnDDDTeQmprKtddey+jRo/nhD38YaEICmD59Otdffz3jxo3D5/OxcOFCbr75ZsaOHcvZZ599yGcaocyaNYvVq1czZswY7rrrLhYsWPC99heqDwLg/PPP5+2332bcuHGBZ18E+81vfsPdd9/N+PHjOyTxHYqIDfd9pE2YMMGsWrUq2mEo1eXocN89R1cb7lsppVQnpQlCKaVUSJoglFJKhaQJQimlVEiaIJRSSoWkCUIppVRImiCUUlHVE4f7fuedd/jmm28Oa9ucnBzef//9g6/YATRBKKW6nK4+3HdXSRA6WJ9SKmDvgw9Su6ljh/uOPWYkfdsYgbRRTxrue/ny5SxatIilS5fyv//7v4EBAW+66SYKCwuJj4/n2WefZeTIkbz++uvcf//9OJ1OUlJSWLx4Mffeey81NTV8/vnn3H333Vx22WUd+NdqTs8glFJR15OG+540aRIXXHABc+bMIScnh2HDhnHdddfx+OOPs3r1aubOncuNN94IWCPAfvjhh6xbt45FixYRExPD7Nmzueyyy8jJyYlocgA9g1BKBTnYkX6k9OThvisrK1m+fDk//elPA2W1tbUATJ48menTp3PppZcGYjySNEEopaKuveG+V61axcCBA5k1a1a3HO7b7/eTmppKTk5Oq2VPPfUUX375Jf/617844YQTWL169UFj60jaxKSUirqeNtx3cPzJyckMGTKE119/HQBjDOvWrQOspquTTjqJ2bNnk5mZye7duw86PHhH0gShlIq6njbc9+WXX86cOXMYP34827dv5+WXX+b5559n7NixHHvssbz77rsA3HHHHRx33HGMHj2aSZMmMXbsWM444wy++eYbxo0bx8KFC7/392yPDvetVA+nw333HDrct1JKqQ6hCUIppVRImiCUUnSXpmbVtsP5G2uCUKqH83g8FBcXa5LoxowxFBcX4/F4Dmk7vQ9CqR5uwIAB5OXlUVhYGO1QVAR5PB4GDBhwSNtoglCqh3O73QwZMiTaYahOKKJNTCIyTUS2iMg2EbkrxPLBIvKJiKwXkSUiMiBo2dUistV+XR3JOJVSSrUWsQQhIk7gSeAcYBTwMxEZ1WK1ucALxpgxwGzgD/a2vYD7gJOAicB9IpIWqViVUkq1FskziInANmPMDmNMHfAqcGGLdUYB/7GnPw1a/kPgY2NMiTHmAPAxMC2CsSqllGohkn0QWUDw2Lp5WGcEwdYBPwEeAy4CkkQkvY1ts1p+gIhcB1xnz1aKyJbvEW8GUPQ9tu9O9LdoTn+P5vT3aNIdfovBbS2Idif17cATIjIdWAbkA75wNzbGPAM80xGBiMiqtm4372n0t2hOf4/m9Pdo0t1/i0gmiHwgeOD1AXZZgDGmAOsMAhFJBC42xpSKSD5weottl0QwVqWUUi1Esg9iJXC0iAwRkRjgcmBR8AoikiEijTHcDcy3pz8E/ktE0uzO6f+yy5RSSh0hEUsQxpgGYAZWxb4JeM0Ys1FEZovIBfZqpwNbRCQX6AM8YG9bAvwPVpJZCcy2yyKpQ5qqugn9LZrT36M5/T2adOvfotsM962UUqpj6VhMSimlQtIEoZRSKqQenyAONhxITyIiA0XkUxH5RkQ2isjMaMcUbSLiFJG1IvLPaMcSbSKSKiJviMhmEdkkIqdEO6ZoEpHb7P9ONojIKyJyaEOldgE9OkGEORxIT9IA/NoYMwo4Gbiph/8eADOxLrJQ1g2tHxhjRgJj6cG/i4hkAbcAE4wxowEn1pWa3UqPThCENxxIj2GM2WOMWWNPV2BVAK3uYO8p7MEjzwWei3Ys0SYiKcBpwPMAxpg6Y0xpVIOKPhcQJyIuIB4oiHI8Ha6nJ4iwhvToiUQkGxgPfBnlUKLpUeA3gD/KcXQGQ4BC4G92k9tzIpIQ7aCixRiTjzXY6C5gD1BmjPkoulF1vJ6eIFQI9l3tbwK3GmPKox1PNIjIecB+Y8zqaMfSSbiA44G/GmPGA1VAj+2zs2/gvRArcfYHEkTkyuhG1fF6eoI46HAgPY2IuLGSw8vGmLeiHU8UTQYuEJGdWE2PZ4rIS9ENKarygDxjTOMZ5RtYCaOn+gHwrTGm0BhTD7wFTIpyTB2upyeIgw4H0pOIiGC1MW8yxvw52vFEkzHmbmPMAGNMNta/i/8YY7rdEWK4jDF7gd0iMsIuOgv4JoohRdsu4GQRibf/uzmLbthpH+3RXKPKGNMgIo3DgTiB+caYjVEOK5omA1cBX4tIjl32W2PM+9ELSXUiNwMv2wdTO4BfRjmeqDHGfCkibwBrsK7+W0s3HHZDh9pQSikVUk9vYlJKKdUGTRBKKaVC0gShlFIqJE0QSimlQtIEoZRSKiRNEEp1AiJyuo4YqzobTRBKKaVC0gSh1CEQkStF5CsRyRGRp+3nRVSKyCP2swE+EZFMe91xIvKFiKwXkbft8XsQkaNEZLGIrBORNSIyzN59YtDzFl6279BVKmo0QSgVJhE5BrgMmGyMGQf4gCuABGCVMeZYYClwn73JC8CdxpgxwNdB5S8DTxpjxmKN37PHLh8P3Ir1bJKhWHe2KxU1PXqoDaUO0VnACcBK++A+DtiPNRz4Qnudl4C37OcnpBpjltrlC4DXRSQJyDLGvA1gjPEC2Pv7yhiTZ8/nANnA5xH/Vkq1QROEUuETYIEx5u5mhSL3tFjvcMevqQ2a9qH/faoo0yYmpcL3CXCJiPQGEJFeIjIY67+jS+x1fg58bowpAw6IyBS7/Cpgqf2kvjwR+bG9j1gRiT+SX0KpcOkRilJhMsZ8IyK/Bz4SEQdQD9yE9fCcifay/Vj9FABXA0/ZCSB49NOrgKdFZLa9j58ewa+hVNh0NFelvicRqTTGJEY7DqU6mjYxKaWUCknPIJRSSoWkZxBKKaVC0gShlFIqJE0QSimlQtIEoZRSKiRNEEoppUL6/5PRebTmoUpHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show dont tell me\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history_batch.history['accuracy'])\n",
    "plt.plot(history_batch.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['no batch: train', 'no batch: test','batch norm: train', 'batch norm:test'], loc='lower right')\n",
    "plt.ylim(0.9,1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Famous Neural Network archictecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50: Deep Residual Learning for Image Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* SOTA: 1st place on the ILSVRC 2015 (object localization/detection and image/scene classification 1k classes)\n",
    "* https://github.com/keras-team/keras/blob/v2.6.0/keras/applications/resnet.py#L440-L459\n",
    "* paper: https://arxiv.org/abs/1512.03385\n",
    "* archictecture:\n",
    "    * Conv2d -> Conv2d -> MaxPooling2d: 12 Conv2 Layers\n",
    "        \n",
    "```python \n",
    "# Block 4\n",
    "  x = layers.Conv2D(\n",
    "      512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "  x = layers.Conv2D(\n",
    "      512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "  x = layers.Conv2D(\n",
    "      512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG\n",
    "\n",
    "* https://github.com/keras-team/keras/blob/v2.6.0/keras/applications/vgg16.py#L43-L224\n",
    "*  ILSVRC 2015 \n",
    "* archictecture:\n",
    "    * Conv2d -> BatchNormalization -> Activation(\"relu\"): \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://github.com/google-research/bert\n",
    "    \n",
    "* Droput\n",
    "* Dense layers \n",
    "* layer_nomalization But the code is hard to understand\n",
    "* SMooth Relu: https://arxiv.org/pdf/1606.08415.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xvectors\n",
    "\n",
    "* Layers: 8\n",
    "* #param: 6M\n",
    "* Arch\n",
    "    * TDNN and mean and std pooling\n",
    "    * activations Relu and affine \n",
    "    * Softmax in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet \n",
    "\n",
    "refs:\n",
    "* https://medium.com/swlh/alexnet-with-tensorflow-46f366559ce8\n",
    "* https://en.wikipedia.org/wiki/AlexNet\n",
    "* Paper 2012 Imagenet competition (BEfore BAtchNormalization paper)\n",
    "\n",
    "Utilizes Dropout and does not use BatchNormalization ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.datasets as datasets\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()\n",
    "x_train = tf.pad(x_train, [[0, 0], [2,2], [2,2]])/255\n",
    "x_test = tf.pad(x_test, [[0, 0], [2,2], [2,2]])/255\n",
    "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
    "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
    "x_train = tf.repeat(x_train, 3, axis=3)\n",
    "x_test = tf.repeat(x_test, 3, axis=3)\n",
    "x_val = x_train[-2000:,:,:,:]\n",
    "y_val = y_train[-2000:]\n",
    "x_train = x_train[:-2000,:,:,:]\n",
    "y_train = y_train[:-2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resizing (Resizing)          (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 56, 56, 96)        34944     \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 56, 56, 96)        0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 56, 56, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 7, 7, 256)         614656    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 1, 1, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1, 1, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 1, 1, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1, 1, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 1, 1, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 21,622,154\n",
      "Trainable params: 21,622,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Convolutions: 5 layers\n",
    "model.add(tf.keras.layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(96, 11, strides=4, padding='same'))\n",
    "model.add(tf.keras.layers.Lambda(tf.nn.local_response_normalization))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(3, strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(256, 5, strides=4, padding='same'))\n",
    "model.add(tf.keras.layers.Lambda(tf.nn.local_response_normalization))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(3, strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(384, 3, strides=4, padding='same'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Conv2D(384, 3, strides=4, padding='same'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Conv2D(256, 3, strides=4, padding='same'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Dense with Dropout\n",
    "model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.keras.layers.Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "301.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
