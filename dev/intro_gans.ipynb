{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANS\n",
    "\n",
    "\n",
    "I want to understand GANS so I can use to to data augmentation on tabular data or spot bad predictions.\n",
    "\n",
    "**GOAL:** \n",
    "    \n",
    "1. Understand GANS big picture \n",
    "1. How to initialze the DNNs\n",
    "1. Why to use binary cross entropy loss\n",
    "1. Simple explanation\n",
    "1. Have tensorflow code\n",
    "\n",
    "refs:\n",
    "\n",
    "* https://debuggercafe.com/introduction-to-generative-adversarial-networks-gans/ <== looks good\n",
    "    * https://debuggercafe.com/vanilla-gan-pytorch/ <== short version\n",
    "* https://towardsdatascience.com/gans-generative-adversarial-networks-an-advanced-solution-for-data-generation-2ac9756a8a99\n",
    "* https://towardsdatascience.com/fundamentals-of-generative-adversarial-networks-b7ca8c34f0bc#:~:text=What%20are%20GANs%2C%20and%20what,photos%20of%20more%20handwritten%20digits.\n",
    "* https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29\n",
    "* https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "* https://arxiv.org/pdf/1406.2661.pdf <== paper\n",
    "*  https://mafda.medium.com/gans-generative-adversarial-network-with-mnist-part-db8b9c061de0\n",
    "* https://www.safwan.xyz/Vanilla-GAN/ <= with the algo in pseud code\n",
    "* https://www.kaggle.com/code/zhoulingyan0228/generating-mnist-gan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NOTE: list of todos\n",
    "* DONE: ~~understand the sparse categorical loss equation. ~~\n",
    "     1. ~~Implement it using numpy.Softmax, categorical and sparse categorical cross entropy~~\n",
    "     1. ~~Run some examples~~ ok\n",
    "     1. ~~show the lead to same reulst and the diff is the label format~~\n",
    "     1. ~~prove categorcal cross entropy = softmax + cross entropy loss~~\n",
    "     1. see: https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\n",
    "             https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "             https://gombru.github.io/2018/05/23/cross_entropy_loss/  <== looks good \n",
    "             https://stackoverflow.com/questions/59787897/how-does-tensorflow-sparsecategoricalcrossentropy-work\n",
    "\n",
    "* DONE: ~~move the sparse categorical loss learnings to another nb and refactor the code~~\n",
    "\n",
    "* DONE: ~~Inspecting the discriminator predictions for fake and real image to understand why 11 instead of 10 output~~\n",
    "\n",
    "* DONE: ~~In training, change np.full(10) -> np.full(11) np.full(0) to see if the dnn still learns~~\n",
    "\n",
    "* TODO: Fix the performance issue after refactor\n",
    "    * RUN old nb in a copy to preserve the results\n",
    "    * debug differences\n",
    "\n",
    "* TODO: Test the code by using batchnormalization your way (that yiou believe is right) and also remove dropout.\n",
    "    * change batchnormalization postion. keep it between output of the layer and activations \n",
    "    * remove dropput with batchnormalization. Maybe use droppou in a different layer is okay\n",
    "    \n",
    "* TODO: try improve a little bit\n",
    "\n",
    "* TODO: Break the code \n",
    "     1. generator is much simple than discriminator\n",
    "     1. discriminator is simple\n",
    "     1. both are simple\n",
    "     1. always train both discriminator and generator paremters in the same iteration\n",
    "     1. change learning rate so one DNN learn faster than the other\n",
    "     \n",
    "* TODO: FIX the image and add the Discriminator as multiclass classifier and also add the label as GANS input and not only the latent space\n",
    "\n",
    "\n",
    "* TODO: Try call back to load the best model.  (<== this is a nice to have features in all DNN)\n",
    "    * for load in inspect generator output load the best instead the last\n",
    "    * Once you done add this ds_pragamntic_programming. It is a good idea to have this to be used in your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds-vm-leandro2\n",
      "2018-06-08-lhof-running_gmm_em_code_from_mcdickenson.ipynb\n",
      "2018-06-18-lhof-simulated_data.ipynb\n",
      "2021-10-31-lhof-study_learning_rate_finder.ipynb\n",
      "2023-02-10-lhof-study_GANS-Copy1.ipynb\n",
      "2023-02-10-lhof-study_GANS.ipynb\n",
      "checkpoints\n",
      "generator_00000.h5\n",
      "generator_00015.h5\n",
      "generator_00020.h5\n",
      "generator_00030.h5\n",
      "generator_00040.h5\n",
      "generator_00045.h5\n",
      "generator_00060.h5\n",
      "generator_00075.h5\n",
      "generator_00080.h5\n",
      "generator_00090.h5\n",
      "generator_00100.h5\n",
      "generator_00105.h5\n",
      "generator_00120.h5\n",
      "generator_00135.h5\n",
      "generator_00140.h5\n",
      "generator_00150.h5\n",
      "generator_00160.h5\n",
      "generator_00180.h5\n",
      "generator_00200.h5\n",
      "intro_KNN.ipynb\n",
      "intro_batch_normalization.ipynb\n",
      "intro_gans.ipynb\n",
      "intro_gradient_algos.ipynb\n",
      "intro_kmeans.ipynb\n",
      "intro_main_losses.ipynb\n",
      "intro_pca.ipynb\n",
      "intro_plda.ipynb\n",
      "intro_reinforcement_learning.ipynb\n",
      "intro_to_prediction_interval.ipynb\n",
      "machine_learning_metrics.ipynb\n"
     ]
    }
   ],
   "source": [
    "!hostname \n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Generative Adversarial Network (GAN) is a deep learning technique used for generating new data similar to existing data. It consists of two neural networks:\n",
    "\n",
    "1. Generator: The generator goal is to learn how to generate sysnthectic data. We say the generator learns the data distribution\n",
    "1. Discriminator: The discriminator goals is to distinguish between real data and the synsthetic data\n",
    "\n",
    "\n",
    "The NN are trained in a way that the Generator try to fools the Discriminator to label the data produced by the Generator as real. The Discriminator tries to correctly label the Generator output as fake (synthetic data). At the begining of training both NN commit a lot of error since they are still learning. But up to certain point, the quality of the output of the Generator increase and also the Discriminator becomes better and hard to be fooled.\n",
    "\n",
    "GANs has many applications in the industry:\n",
    "1. Image, Video and audio generations: application on social media, advertisemenst and entertainment industry. Widely used in social media\n",
    "1. Data augmentation: Help training other Machine learnings models. Many task that utilizes GANs for increase the the data size of the minority classes acheive SOTA performance. Ex:\n",
    "    * CIFAR-10: paper \"Improved Training of Wasserstein GANs,\" Arjovsky et al. (2017) and \"BigGAN: Large Scale GAN Training for High Fidelity Natural Image Synthesis,\" Brock et al. (2018) \n",
    "    * Medical imaging: \"Adversarial deep structured nets for mass segmentation from mammograms\" by Li et al. (2019), Synthesizing medical images for augmented reality training data using generative adversarial networks\" by Yang et al. (2018)  and \"Improving medical image segmentation via adversarial training and perceptual similarity metrics\" by Zhang et al. (2018)\n",
    "1. Anomaly detection: Achieve SOTA performance \n",
    "    * Credit Card Fraud Detection\n",
    "    * paper \"Generating Synthetic Tabular Data using Generative Adversarial Networks,\" Wang et al. (2019) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure below is an example of GANS archictectur for generating handwritten digits using the public mnist digit database \n",
    "\n",
    "* It is reccomended that the input noisy size matches images sizes\n",
    "    * Smaller: the generator may not have enough information to create a high-quality image\n",
    "    * Larger: he generator may have too much information, leading to overfitting and producing similar images repeatedly\n",
    "    \n",
    "\n",
    "* Generator and Discrminator DNN size\n",
    "    * relative size and complexity of the generator and discriminator can affect the performance and stability of the GAN.\n",
    "    * discriminator is much larger and more complex than the generator, it may be able to easily identify and reject the generated samples. This result in the generator failing to learn and producing low-quality samples\n",
    "    * if the generator is much larger and more complex than the discriminator, it may be able to generate realistic-looking samples that can fool the discriminator, but at the cost of requiring more training time and computational resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/gans.png\" alt=\"GANS\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, GANs alternate between updating the discriminator to better distinguish real from generated images, and updating the generator to produce more realistic images that fool the discriminator.\n",
    "The discriminator and generators have different losses (Because they are performing different tasks). The classifier nature of the discriminator impose loss related to binary classification or multiclass classifications while the generator  has a special loss that takes the discriminator predictions into account. Usually, discriminator loss decreases rapidly compared to the generator and then generator catch ups. The discriminator loss increases (not mandatory) at the end keep stable.\n",
    "\n",
    "* GANS loss:\n",
    "\n",
    "$\n",
    "L_{GAN}(G,D) = E_{x \\sim p_{data}(x)}[\\log D(x)] + E_{z \\sim p_z(z)}[\\log(1-D(G(z))]\n",
    "$\n",
    "\n",
    "\n",
    "where:\n",
    "* $G$ is the generator and $D$ is the discriminator\n",
    "* $G(z)$ is the generator output (fake image)\n",
    "* $x$ is real data (real image) drawn from the true data distribution $p_{data}$ ($x \\sim p_{data}$)\n",
    "* $z$ is random noise data drawn from a **prior** distribution $p_z$ ($z \\sim p_z$)\n",
    "* $E$ is the expectation operaor (\"The mean\")\n",
    "\n",
    "Bellow is a discussion of the equation above. It is the **GANS loss big picture equation**:\n",
    "\n",
    "$\n",
    "L(G, D) = E_x[log(D(x))] + E_z[log(1 - D(G(z)))]\n",
    "$\n",
    "\n",
    "where:\n",
    "* $G$ is the generator and $D$ is the discriminator \n",
    "* $x$ is the true images \n",
    "* $z$ is noise data for generating fake images\n",
    "* $E$ is the expectation operator (\"The mean\")\n",
    "\n",
    "The first term $ E_x[log(D(x))]$ is related to the probability of the discriminator (log-likelihood of the discriminator) detect an image is true given the image is real. Training find the parameters of the discriminator that maximize that term. The discriminator is encouraged to maximize the probablity of detected real images. \n",
    "\n",
    "The second term is related to the ability of the generator to foo the discriminator (log-likelihood of the discriminator incorrectly classifying a fake image as true). **Maximize** that ability of the generator ecncourage the generator creates more realistic images to foo the discriminator. This is the same of **minimize** the probability of be detected by the discriminator (the probability that the discriminator correctly classifies generated samples as fake).\n",
    "\n",
    "Training is performed in  2 stages\n",
    "\n",
    "1. All layers are trainable. This mean both discriminator and generator parmeters will be changing in order to maximize the loss function above. \n",
    "\n",
    "1. Only genrators layers are trainable. This mean only generators parameters will be modified by back propagation and the gradient descent in order to maximize the loss above and the first term will remain constant. Note that it is equivalent of minimize $D(G(z)$. **The generators learn twice as fast than the discriminator** for each iteration.\n",
    "\n",
    "\n",
    "The choice of prior distribution can have a significant impact on the quality and diversity of the generated samples. Using a non-uniform prior distribution, such as a normal distribution ($z \\sim N(0,1)$), is that it can help the generator to focus on specific regions of the data distribution where the most important features are located. For example, if the data distribution has a specific shape or structure, the prior distribution can be designed to reflect this structure and guide the generator to generate more realistic samples.Most common common priori distribution is the uniform $z \\sim U(0,1)$, but different authors use: Normal, Bernouli, Multivariate Normal Distribution and Laplacian Distribution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agor vai\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import Image\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "print(\"agor vai\")\n",
    "\n",
    "# import nbformat\n",
    "# from nbformat import v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 00:23:57.872736: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 00:23:58.008992: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-16 00:23:58.015021: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-16 00:23:58.015040: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-16 00:23:58.045019: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-16 00:23:58.861329: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 00:23:58.861401: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 00:23:58.861409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate, Dense, Reshape, Dropout, Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU, BatchNormalization, UpSampling2D, Conv2D,Activation\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (60000, 28, 28, 1) (60000,)\n",
      "Test (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print('Train', x_train.shape, y_train.shape)\n",
    "print('Test', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f344ebfb880>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbh0lEQVR4nO3df2xV9f3H8dcF6QWxvayU9vYKxQIqUwQzJl2DMIQKdI6BsASdf6AhGrC4KRM3nPxyW+qYEafrZImGYhRxbAMmWzBQbcm2FgPKiNE1lBSpoy3Yyb1QpBD6+f5BvN9daYFzubfv/ng+kk/Se8553/Pm47GvnntOT33OOScAADpYL+sGAAA9EwEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE1dZN/BVra2tOnLkiFJTU+Xz+azbAQB45JzTiRMnFAqF1KtX++c5nS6Ajhw5oiFDhli3AQC4QnV1dRo8eHC76zvdR3CpqanWLQAAEuBS38+TFkAlJSW67rrr1LdvX+Xl5em99967rDo+dgOA7uFS38+TEkBvvvmmFi9erBUrVuj999/XmDFjNG3aNB09ejQZuwMAdEUuCcaNG+eKioqir8+dO+dCoZArLi6+ZG04HHaSGAwGg9HFRzgcvuj3+4SfAZ05c0Z79+5VQUFBdFmvXr1UUFCgysrKC7ZvaWlRJBKJGQCA7i/hAfTZZ5/p3LlzysrKilmelZWlhoaGC7YvLi5WIBCIDu6AA4CewfwuuKVLlyocDkdHXV2ddUsAgA6Q8N8DysjIUO/evdXY2BizvLGxUcFg8ILt/X6//H5/otsAAHRyCT8DSklJ0dixY1VWVhZd1traqrKyMuXn5yd6dwCALiopT0JYvHix5s2bp29+85saN26cnn/+eTU3N+uBBx5Ixu4AAF1QUgJo7ty5OnbsmJYvX66Ghgbdeuut2r59+wU3JgAAei6fc85ZN/G/IpGIAoGAdRsAgCsUDoeVlpbW7nrzu+AAAD0TAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABNXWTcAoOfq37+/55ry8nLPNaFQyHONJI0fP95zzaFDh+LaV0/EGRAAwAQBBAAwkfAAWrlypXw+X8wYOXJkoncDAOjiknIN6Oabb9bOnTv/fydXcakJABArKclw1VVXKRgMJuOtAQDdRFKuAR04cEChUEjDhg3Tfffdp8OHD7e7bUtLiyKRSMwAAHR/CQ+gvLw8lZaWavv27XrppZdUW1urCRMm6MSJE21uX1xcrEAgEB1DhgxJdEsAgE7I55xzydzB8ePHNXToUD333HOaP3/+BetbWlrU0tISfR2JRAghoIfg94C6t3A4rLS0tHbXJ/3ugAEDBuiGG25QTU1Nm+v9fr/8fn+y2wAAdDJJ/z2gkydP6uDBg8rOzk72rgAAXUjCA+jxxx9XRUWFDh06pH/+85+6++671bt3b917772J3hUAoAtL+Edwn376qe699141NTVp0KBBuv3221VVVaVBgwYlelcAgC4s4QG0cePGRL8lAMV3Ib0jf/D7/PPPPdfccccdnmvGjh3ruaa6utpzjSQ1NTXFVYfLw7PgAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmEj6H6QDLIwaNSquuh/+8Ieea4YOHRrXvry64YYbPNfk5OQkoZO2PfPMM55rbrrpJs81Pp/Pc81//vMfzzWSlJKSElcdLg9nQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEzwNG93S5MmT46qbP39+gjtJnJaWFs81r732Wlz7imf+fvrTn8a1L6+cc55rSktL49pXU1NTXHW4PJwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMHDSNHprVy50nPNkiVLEt9IO9avX++55tixY55rnn322Q7ZjyTdeuutnmvefvttzzUZGRmea+L5N/3xj3/0XIPk4wwIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACR5Gik6vf//+nmv69esX174++eQTzzU/+9nPPNfU19d7ronHiBEj4qp78sknPdcMGjTIc01zc7PnmngeTnv69GnPNUg+zoAAACYIIACACc8BtGvXLs2YMUOhUEg+n09btmyJWe+c0/Lly5Wdna1+/fqpoKBABw4cSFS/AIBuwnMANTc3a8yYMSopKWlz/erVq/XCCy9o7dq12r17t/r3769p06bxGSwAIIbnmxAKCwtVWFjY5jrnnJ5//nk99dRTmjlzpiTp1VdfVVZWlrZs2aJ77rnnyroFAHQbCb0GVFtbq4aGBhUUFESXBQIB5eXlqbKyss2alpYWRSKRmAEA6P4SGkANDQ2SpKysrJjlWVlZ0XVfVVxcrEAgEB1DhgxJZEsAgE7K/C64pUuXKhwOR0ddXZ11SwCADpDQAAoGg5KkxsbGmOWNjY3RdV/l9/uVlpYWMwAA3V9CAyg3N1fBYFBlZWXRZZFIRLt371Z+fn4idwUA6OI83wV38uRJ1dTURF/X1tZq3759Sk9PV05Ojh599FH94he/0PXXX6/c3FwtW7ZMoVBIs2bNSmTfAIAuznMA7dmzR3fccUf09eLFiyVJ8+bNU2lpqZ544gk1NzfroYce0vHjx3X77bdr+/bt6tu3b+K6BgB0eT7nnLNu4n9FIhEFAgHrNtCJ5OXlea55+eWX49rXTTfd5Lnmtdde81zz8MMPe66J5/+LtWvXeq6RpLvuustzzeeff+655pe//KXnmjVr1niugY1wOHzR6/rmd8EBAHomAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJz3+OAeho+/bt81xTVVUV177ieRr25MmTPdfceeednmvieQp0Tk6O55p4rVq1ynPNiy++mIRO0FVwBgQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEDyNFp9fS0uK5JhKJJKGTtoVCIc81f/rTnzzX+Hw+zzXOOc81kvTKK694rtmyZUtc+0LPxRkQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEzyMFN3SJ598Yt1Cp/C3v/0trrpnn33Wc01dXV1c+0LPxRkQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEzyMFJ1e7969PddMmDAhrn35fL646jrCX//6V881M2bMSEInQGJwBgQAMEEAAQBMeA6gXbt2acaMGQqFQvL5fNqyZUvM+vvvv18+ny9mTJ8+PVH9AgC6Cc8B1NzcrDFjxqikpKTdbaZPn676+vroeOONN66oSQBA9+P5JoTCwkIVFhZedBu/369gMBh3UwCA7i8p14DKy8uVmZmpG2+8UQsXLlRTU1O727a0tCgSicQMAED3l/AAmj59ul599VWVlZXpV7/6lSoqKlRYWKhz5861uX1xcbECgUB0DBkyJNEtAQA6oYT/HtA999wT/fqWW27R6NGjNXz4cJWXl2vKlCkXbL906VItXrw4+joSiRBCANADJP027GHDhikjI0M1NTVtrvf7/UpLS4sZAIDuL+kB9Omnn6qpqUnZ2dnJ3hUAoAvx/BHcyZMnY85mamtrtW/fPqWnpys9PV2rVq3SnDlzFAwGdfDgQT3xxBMaMWKEpk2bltDGAQBdm+cA2rNnj+64447o6y+v38ybN08vvfSS9u/fr/Xr1+v48eMKhUKaOnWqfv7zn8vv9yeuawBAl+c5gCZNmiTnXLvr33777StqCPiqjRs3eq6ZPXt2XPu62LFtrTP3BsSDZ8EBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwk/E9yo+cIhUKeax544AHPNXPmzPFcE++To99//33PNf/6178818QzD5mZmZ5rgM6MMyAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmeBgp4jZlyhTPNU8//XQSOrnQU089FVfdb3/7W881s2bN8lwTz8NIP/roI881QGfGGRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATPIwUmjRpUlx1L7zwQmIbacf3vvc9zzU7d+6Ma1/BYNBzzfLly+Pal1eHDh3qkP0AHYUzIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ4GCl05513xlUXCAQ811RUVHiu2bZtm+eaPn36eK6RpO9+97uea+KZB5/P57nm2LFjnmuAzowzIACACQIIAGDCUwAVFxfrtttuU2pqqjIzMzVr1ixVV1fHbHP69GkVFRVp4MCBuuaaazRnzhw1NjYmtGkAQNfnKYAqKipUVFSkqqoq7dixQ2fPntXUqVPV3Nwc3eaxxx7TW2+9pU2bNqmiokJHjhzR7NmzE944AKBr83QTwvbt22Nel5aWKjMzU3v37tXEiRMVDof1yiuvaMOGDZo8ebIkad26dfr617+uqqoqfetb30pc5wCALu2KrgGFw2FJUnp6uiRp7969Onv2rAoKCqLbjBw5Ujk5OaqsrGzzPVpaWhSJRGIGAKD7izuAWltb9eijj2r8+PEaNWqUJKmhoUEpKSkaMGBAzLZZWVlqaGho832Ki4sVCASiY8iQIfG2BADoQuIOoKKiIn344YfauHHjFTWwdOlShcPh6Kirq7ui9wMAdA1x/SLqokWLtG3bNu3atUuDBw+OLg8Ggzpz5oyOHz8ecxbU2NioYDDY5nv5/X75/f542gAAdGGezoCcc1q0aJE2b96sd955R7m5uTHrx44dqz59+qisrCy6rLq6WocPH1Z+fn5iOgYAdAuezoCKioq0YcMGbd26VampqdHrOoFAQP369VMgEND8+fO1ePFipaenKy0tTY888ojy8/O5Aw4AEMNTAL300kuSpEmTJsUsX7dune6//35J0po1a9SrVy/NmTNHLS0tmjZtmn73u98lpFkAQPfhKYCcc5fcpm/fviopKVFJSUncTaFjtba2xlV3OcdDImriebDorFmzPNdI0m9+8xvPNZ9//rnnmpdfftlzzZc/AALdBc+CAwCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYiOsvoqJ7yczM7LB9HTt2zHPNjh07PNdMmDDBc028HnjgAc81b731VhI6AboWzoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCY4GGk0Mcff9xh+/r+97/vucbn83mu+e9//+u5RpJKSko81+zcuTOufQE9HWdAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATPAwUmj9+vVx1aWkpHiuWbZsmeeaPXv2eK75y1/+4rlGktasWRNXHQDvOAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgwuecc9ZN/K9IJKJAIGDdBgDgCoXDYaWlpbW7njMgAIAJAggAYMJTABUXF+u2225TamqqMjMzNWvWLFVXV8dsM2nSJPl8vpixYMGChDYNAOj6PAVQRUWFioqKVFVVpR07dujs2bOaOnWqmpubY7Z78MEHVV9fHx2rV69OaNMAgK7P019E3b59e8zr0tJSZWZmau/evZo4cWJ0+dVXX61gMJiYDgEA3dIVXQMKh8OSpPT09Jjlr7/+ujIyMjRq1CgtXbpUp06davc9WlpaFIlEYgYAoAdwcTp37py766673Pjx42OW//73v3fbt293+/fvd6+99pq79tpr3d13393u+6xYscJJYjAYDEY3G+Fw+KI5EncALViwwA0dOtTV1dVddLuysjInydXU1LS5/vTp0y4cDkdHXV2d+aQxGAwG48rHpQLI0zWgLy1atEjbtm3Trl27NHjw4Itum5eXJ0mqqanR8OHDL1jv9/vl9/vjaQMA0IV5CiDnnB555BFt3rxZ5eXlys3NvWTNvn37JEnZ2dlxNQgA6J48BVBRUZE2bNigrVu3KjU1VQ0NDZKkQCCgfv366eDBg9qwYYO+853vaODAgdq/f78ee+wxTZw4UaNHj07KPwAA0EV5ue6jdj7nW7dunXPOucOHD7uJEye69PR05/f73YgRI9ySJUsu+Tng/wqHw+afWzIYDAbjyselvvfzMFIAQFLwMFIAQKdEAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDR6QLIOWfdAgAgAS71/bzTBdCJEyesWwAAJMClvp/7XCc75WhtbdWRI0eUmpoqn88Xsy4SiWjIkCGqq6tTWlqaUYf2mIfzmIfzmIfzmIfzOsM8OOd04sQJhUIh9erV/nnOVR3Y02Xp1auXBg8efNFt0tLSevQB9iXm4Tzm4Tzm4Tzm4TzreQgEApfcptN9BAcA6BkIIACAiS4VQH6/XytWrJDf77duxRTzcB7zcB7zcB7zcF5XmodOdxMCAKBn6FJnQACA7oMAAgCYIIAAACYIIACAiS4TQCUlJbruuuvUt29f5eXl6b333rNuqcOtXLlSPp8vZowcOdK6raTbtWuXZsyYoVAoJJ/Ppy1btsSsd85p+fLlys7OVr9+/VRQUKADBw7YNJtEl5qH+++//4LjY/r06TbNJklxcbFuu+02paamKjMzU7NmzVJ1dXXMNqdPn1ZRUZEGDhyoa665RnPmzFFjY6NRx8lxOfMwadKkC46HBQsWGHXcti4RQG+++aYWL16sFStW6P3339eYMWM0bdo0HT161Lq1DnfzzTervr4+Ov7+979bt5R0zc3NGjNmjEpKStpcv3r1ar3wwgtau3atdu/erf79+2vatGk6ffp0B3eaXJeaB0maPn16zPHxxhtvdGCHyVdRUaGioiJVVVVpx44dOnv2rKZOnarm5uboNo899pjeeustbdq0SRUVFTpy5Ihmz55t2HXiXc48SNKDDz4YczysXr3aqON2uC5g3LhxrqioKPr63LlzLhQKueLiYsOuOt6KFSvcmDFjrNswJclt3rw5+rq1tdUFg0H361//Orrs+PHjzu/3uzfeeMOgw47x1Xlwzrl58+a5mTNnmvRj5ejRo06Sq6iocM6d/2/fp08ft2nTpug2H3/8sZPkKisrrdpMuq/Og3POffvb33Y/+tGP7Jq6DJ3+DOjMmTPau3evCgoKost69eqlgoICVVZWGnZm48CBAwqFQho2bJjuu+8+HT582LolU7W1tWpoaIg5PgKBgPLy8nrk8VFeXq7MzEzdeOONWrhwoZqamqxbSqpwOCxJSk9PlyTt3btXZ8+ejTkeRo4cqZycnG59PHx1Hr70+uuvKyMjQ6NGjdLSpUt16tQpi/ba1ekeRvpVn332mc6dO6esrKyY5VlZWfr3v/9t1JWNvLw8lZaW6sYbb1R9fb1WrVqlCRMm6MMPP1Rqaqp1eyYaGhokqc3j48t1PcX06dM1e/Zs5ebm6uDBg3ryySdVWFioyspK9e7d27q9hGttbdWjjz6q8ePHa9SoUZLOHw8pKSkaMGBAzLbd+Xhoax4k6Qc/+IGGDh2qUCik/fv36yc/+Ymqq6v15z//2bDbWJ0+gPD/CgsLo1+PHj1aeXl5Gjp0qP7whz9o/vz5hp2hM7jnnnuiX99yyy0aPXq0hg8frvLyck2ZMsWws+QoKirShx9+2COug15Me/Pw0EMPRb++5ZZblJ2drSlTpujgwYMaPnx4R7fZpk7/EVxGRoZ69+59wV0sjY2NCgaDRl11DgMGDNANN9ygmpoa61bMfHkMcHxcaNiwYcrIyOiWx8eiRYu0bds2vfvuuzF/viUYDOrMmTM6fvx4zPbd9Xhobx7akpeXJ0md6njo9AGUkpKisWPHqqysLLqstbVVZWVlys/PN+zM3smTJ3Xw4EFlZ2dbt2ImNzdXwWAw5viIRCLavXt3jz8+Pv30UzU1NXWr48M5p0WLFmnz5s165513lJubG7N+7Nix6tOnT8zxUF1drcOHD3er4+FS89CWffv2SVLnOh6s74K4HBs3bnR+v9+Vlpa6jz76yD300ENuwIABrqGhwbq1DvXjH//YlZeXu9raWvePf/zDFRQUuIyMDHf06FHr1pLqxIkT7oMPPnAffPCBk+See+4598EHH7hPPvnEOefcM8884wYMGOC2bt3q9u/f72bOnOlyc3PdF198Ydx5Yl1sHk6cOOEef/xxV1lZ6Wpra93OnTvdN77xDXf99de706dPW7eeMAsXLnSBQMCVl5e7+vr66Dh16lR0mwULFricnBz3zjvvuD179rj8/HyXn59v2HXiXWoeampq3NNPP+327Nnjamtr3datW92wYcPcxIkTjTuP1SUCyDnnXnzxRZeTk+NSUlLcuHHjXFVVlXVLHW7u3LkuOzvbpaSkuGuvvdbNnTvX1dTUWLeVdO+++66TdMGYN2+ec+78rdjLli1zWVlZzu/3uylTprjq6mrbppPgYvNw6tQpN3XqVDdo0CDXp08fN3ToUPfggw92ux/S2vr3S3Lr1q2LbvPFF1+4hx9+2H3ta19zV199tbv77rtdfX29XdNJcKl5OHz4sJs4caJLT093fr/fjRgxwi1ZssSFw2Hbxr+CP8cAADDR6a8BAQC6JwIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACb+D9hspvhEloFAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot raw pixel data\n",
    "#pyplot.imshow(trainX[i], cmap='gray')\n",
    "k = 17\n",
    "y_train[k]\n",
    "plt.imshow(x_train[k], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 16)        160       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 14, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 32)          4640      \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 4, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11)                5643      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103,755\n",
      "Trainable params: 103,275\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 00:24:00.668643: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-16 00:24:00.668677: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-16 00:24:00.668696: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ds-vm-leandro2): /proc/driver/nvidia/version does not exist\n",
      "2023-03-16 00:24:00.668909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def discriminator_model(in_shape=(28,28,)):\n",
    "\n",
    "    img_input = Input(shape=in_shape)\n",
    "    x = Reshape((28,28,1))(img_input)\n",
    "    \n",
    "    x = Conv2D(16, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(32, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # NOTE: Why outputs is 11 and not 10\n",
    "    x = Dense(11, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=img_input, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = discriminator_model()\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape_2\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [6272], output_shape = [7, 7, 256]\n\nCall arguments received by layer \"reshape_2\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 6272), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     41\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m \u001b[38;5;66;03m# z size\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m generator\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mgenerator_model\u001b[0;34m(latent_dim)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m7\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m7\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m128\u001b[39m)(x)\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m LeakyReLU()(x)\n\u001b[0;32m---> 14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m UpSampling2D()(x)\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m128\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n",
      "File \u001b[0;32m~/.venvs/gans-env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.venvs/gans-env/lib/python3.9/site-packages/keras/layers/reshaping/reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[38;5;241m=\u001b[39m original \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original \u001b[38;5;241m!=\u001b[39m known:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_2\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [6272], output_shape = [7, 7, 256]\n\nCall arguments received by layer \"reshape_2\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 6272), dtype=float32)"
     ]
    }
   ],
   "source": [
    "def generator_model(latent_dim = 100):\n",
    "    \n",
    "         \n",
    "    latent_input = Input(shape=(latent_dim,), name='latent_input')\n",
    "    \n",
    "    label_input = Input(shape=(1,), name='label_input') # 0...9\n",
    "    x = Embedding(input_dim=10, output_dim=10)(label_input)\n",
    "    x = Reshape((10,))(x)\n",
    "    \n",
    "    x = concatenate([x, latent_input], axis=-1)\n",
    "\n",
    "    x = Dense(7*7*128)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Reshape((7,7,128))(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(32, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(1, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    x = Reshape((28,28,))(x)\n",
    "    \n",
    "    model = Model(inputs=[latent_input, label_input], outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "latent_dim = 28*28 # z size\n",
    "generator = generator_model(latent_dim)\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_input = Input(shape=(latent_dim,), name='latent_input')\n",
    "label_input = Input(shape=(1,), name='label_input')\n",
    "\n",
    "x = generator([latent_input, label_input])\n",
    "x = discriminator(x)\n",
    "\n",
    "gans = Model(inputs=[latent_input, label_input], outputs=x)\n",
    "\n",
    "discriminator.trainable = False\n",
    "gans.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = True\n",
    "discriminator.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], \n",
    "                      optimizer=tf.keras.optimizers.Adam(0.0005))\n",
    "\n",
    "discriminator.trainable = False\n",
    "gans.compile(loss='sparse_categorical_crossentropy', metrics=['acc'],\n",
    "            optimizer=tf.keras.optimizers.Adam(0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and learning curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint path and format for saving the generator's weights\n",
    "checkpoint_path = 'checkpoints/best_generator.h5'\n",
    "\n",
    "# Define the callback to save the best generator's weights\n",
    "class SaveBestGenerator(Callback):\n",
    "    \n",
    "    def __init__(self, generator):\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.best_loss = np.inf\n",
    "    \n",
    "    def on_epoch_end(self, epoch, current_loss = None):\n",
    "        \n",
    "        #current_loss = gloss[0] #logs.get('loss')\n",
    "        if current_loss < self.best_loss:\n",
    "\n",
    "            self.best_loss = current_loss\n",
    "            self.generator.save_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def random_mnist_set(X, y, size=10):\n",
    "    \n",
    "    length = len(X)\n",
    "    indices = np.random.choice(length, size)\n",
    "    \n",
    "    return X[indices], y[indices]\n",
    "\n",
    "def random_generated_set(generator, size=10, latent_dim=100):\n",
    "    \n",
    "    # z\n",
    "    latents = np.random.normal(0, 1, (size, latent_dim))\n",
    "    \n",
    "    return generator.predict_on_batch({'latent_input':latents,\n",
    "                      'label_input':np.random.randint(10, size=size)})\n",
    "\n",
    "X_i, y_i = random_mnist_set(x_train, y_train, 1)\n",
    "\n",
    "X_i.shape\n",
    "y_i.shape\n",
    "\n",
    "y_i\n",
    "plt.imshow(X_i[0,:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gans_train(X, y, discriminator, gans, save_interval,epochs, batch_size, latent_dim ):\n",
    "    \n",
    "    history = {'dloss':[], 'gloss':[]}\n",
    "    save_best_generator = SaveBestGenerator(generator)\n",
    "    for iteration in range(epochs):\n",
    "\n",
    "        # train discriminator\n",
    "        discriminator.trainable = True\n",
    "\n",
    "        X_sample, y_sample = random_mnist_set(X, y, batch_size)\n",
    "        dloss_real = discriminator.train_on_batch(X_sample, y_sample)\n",
    "\n",
    "        # NOTE: GEt random fake image created by the generator\n",
    "        fake_image = random_generated_set(generator, size=batch_size, latent_dim=latent_dim)\n",
    "        # NOTE: digit 10 means fake  one_hot_digit.shape = (1,11). If the last elem is the largets it means it is fake\n",
    "        fake_label = np.full(batch_size, 10)\n",
    "        dloss_fake = discriminator.train_on_batch(fake_image, fake_label)\n",
    "\n",
    "        # train generator\n",
    "        # NOTE: Freeze discriminator parameters\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        gan_labels = np.random.randint(10, size=batch_size)\n",
    "        gloss = gans.train_on_batch({'latent_input':np.random.normal(0, 1, (batch_size, latent_dim)),\n",
    "                            'label_input':gan_labels}, \n",
    "                           gan_labels)\n",
    "\n",
    "        history['dloss'].append((dloss_real[0]+dloss_fake[0])/2)\n",
    "        history['gloss'].append(gloss[0])\n",
    "\n",
    "        if iteration % save_interval == 0:\n",
    "\n",
    "            print(f'iteration: {iteration}')\n",
    "            print(f'gan: loss = {gloss[0]:.3f}. acc = { gloss[1]:.3f}')\n",
    "            print(f'discriminator: loss_real = {dloss_real[0]:.3f}, acc_real = {dloss_real[1]:.3f}')\n",
    "            print(f'               loss_fake = {dloss_fake[0]:.3f}, acc_fake = {dloss_fake[1]:.3f}')\n",
    "            generator.save_weights('checkpoints/generator_{0:05d}.h5'.format(iteration))\n",
    "            \n",
    "        # Save the best generator's weights\n",
    "        save_best_generator.on_epoch_end(iteration, gloss[0])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "save_interval = 15*4\n",
    "epochs = save_interval*10 + 1\n",
    "batch_size = 20\n",
    "\n",
    "history = gans_train(x_train, y_train,discriminator, gans, save_interval, epochs, batch_size, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['dloss'], 'r')\n",
    "plt.plot(history['gloss'], 'b')\n",
    "plt.legend(['discriminator', 'gans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "import glob\n",
    "\n",
    "checkpoints = sorted(glob.glob('checkpoints/generator_*.h5'))\n",
    "\n",
    "loaded_generator = generator_model(latent_dim)\n",
    "\n",
    "plt.figure(figsize=(10,2*len(checkpoints)))\n",
    "for i,cp in enumerate(checkpoints):\n",
    "    \n",
    "    loaded_generator.load_weights(cp)\n",
    "    \n",
    "    z = np.random.normal(0, 1, (10, latent_dim))\n",
    "    digits = np.arange(10)\n",
    "    \n",
    "    generated = loaded_generator.predict_on_batch({'latent_input':z,\n",
    "                      'label_input':digits})\n",
    "    \n",
    "    for j, g in enumerate(generated):\n",
    "        _ = plt.subplot(len(checkpoints),10,10*i+j+1)\n",
    "        _ = plt.imshow(g, cmap='gray')\n",
    "        _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_generator = generator_model(latent_dim)\n",
    "best_generator.load_weights('checkpoints/best_generator.h5')\n",
    "\n",
    "digits = np.arange(10) \n",
    "z = np.random.normal(0, 1, (10, latent_dim))\n",
    "\n",
    "generated = best_generator.predict_on_batch({'latent_input':z,\n",
    "                      'label_input':digits})\n",
    "\n",
    "i = 0\n",
    "for j, g in enumerate(generated):\n",
    "    _ = plt.subplot(1,10,10*i+j+1)\n",
    "    _ = plt.imshow(g, cmap='gray')\n",
    "    _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In memory generator\n",
    "digits = np.arange(10) \n",
    "z = np.random.normal(0, 1, (10, latent_dim))\n",
    "\n",
    "generated = generator.predict_on_batch({'latent_input':z,\n",
    "                      'label_input':digits})\n",
    "i = 0\n",
    "for j, g in enumerate(generated):\n",
    "    _ = plt.subplot(1,10,10*i+j+1)\n",
    "    _ = plt.imshow(g, cmap='gray')\n",
    "    _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting gans outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image = random_generated_set(generator, size=1, latent_dim=latent_dim)\n",
    "fake_image.shape\n",
    "\n",
    "one_hot_digit = discriminator.predict(fake_image, verbose=0)\n",
    "one_hot_digit.shape\n",
    "\n",
    "digit_pred = np.argmax(one_hot_digit)\n",
    "digit_prob = np.max(one_hot_digit)\n",
    "\n",
    "print(f'predict digit: {digit_pred}; prob: {digit_prob:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_image, digit = random_mnist_set(x_train, y_train, 1)\n",
    "digit_image.shape\n",
    "\n",
    "one_hot_digit = discriminator.predict(digit_image, verbose=0)\n",
    "one_hot_digit.shape\n",
    "\n",
    "digit_pred = np.argmax(one_hot_digit)\n",
    "digit_prob = np.max(one_hot_digit)\n",
    "\n",
    "print(f'digit: {digit[0]}; predict digit: {digit_pred}; prob: {digit_prob:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting generator for especifc digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: best generator loaded\n",
    "\n",
    "best_generator = generator_model()\n",
    "best_generator.load_weights('checkpoints/best_generator.h5')\n",
    "\n",
    "digit = 3\n",
    "n_samples = 5\n",
    "\n",
    "generated = best_generator.predict_on_batch({'latent_input':np.random.normal(0, 1, (n_samples, latent_dim)),\n",
    "                  'label_input': np.full(n_samples, digit)})\n",
    "\n",
    "for i, g in enumerate(generated):\n",
    "        \n",
    "    _ = plt.subplot(1,n_samples,i+1)\n",
    "    _ = plt.imshow(g, cmap='gray')\n",
    "    _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gans-env",
   "language": "python",
   "name": "gans-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "179.222px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "92274f54c36a370591365d84493a50389ebed307f294330ec188d930f0bcf294"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
