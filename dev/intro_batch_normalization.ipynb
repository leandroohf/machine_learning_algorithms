{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "**Goals**:\n",
    "\n",
    "* Goal understand better the link with gradients and backproapagation\n",
    "* Review backpropagation\n",
    "* IMplement dense layer and neural netowork\n",
    "* tets implemenattion\n",
    "* Learn batch normalization\n",
    "* IMplement Batch normalization layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import List, Set, Dict, Tuple, Optional, Union\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "\n",
    "Maybe this pdf fprm stanford is more reliable \n",
    "\n",
    "* https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf\n",
    "* https://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture04-neuralnets.pdf\n",
    "\n",
    "--------------\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "* https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n",
    "\n",
    "\n",
    "\n",
    "* Understand the math \n",
    "Watch this video explaining the math behing backpropagation from stanfor:\n",
    "* https://www.youtube.com/watch?v=isPiE-DBagM&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6 (Stanford)\n",
    "\n",
    "\n",
    "Often people confuse backward propagation with gradient descent. The gradient descent algorithm minimizes (optmizes) the cost function. And the backpropagation updates all parameters in the neural network based on the chain rule fomula and the gradient returned in the gradient descent algorithm   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement simple regression first\n",
    "\n",
    "\n",
    "1. One layer Dense\n",
    "1. Regression\n",
    "\n",
    "Steps:\n",
    "    \n",
    "* Implement parts \n",
    "    1. activatoins\n",
    "    1. code one layer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/simple_nn.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} \\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]})\n",
    "$\n",
    "\n",
    "<img src=\"../images/forward_step.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "refs:\n",
    "* https://sudeepraja.github.io/Neural/ Matrix notation\n",
    "* https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n",
    "\n",
    "#### Some notes about the big picture\n",
    "\n",
    "* We want to optmize the cost function $J(\\theta = {W,b}) = \\frac{1}{m} \\sum^m_{i=1} \\ell(\\hat{y^{(i)}} = h_{\\theta}(x^{(i)}), y^{(i)})$\n",
    "\n",
    "* In order to do that we need:\n",
    "\n",
    "    1. We need to update all the parameters: $\\{W^{[1]}, b^{[1]},...., W^{[L]}, b^{[L]}\\}$ of the NN in the right direction.\n",
    "    \n",
    "    1. To discover the right direction to update, we need to compute the gradients.: $\\frac{\\partial}{\\partial W^{[l]}}J$ and $\\frac{\\partial}{\\partial b^{[l]}}J$ for $l = \\{1,.., L\\} $\n",
    "    \n",
    "    1. To compute the gradients, we need to use the chain rule\n",
    "    \n",
    "\n",
    "The 4 equations we need to compute the gradients can be written in matrix form as:\n",
    "\n",
    "\n",
    "$\n",
    "\\delta^L = \\frac{\\partial}{\\partial \\hat{y}}J \\circ f'(z^{[L]}) \\\\\n",
    "\\delta^l = \\left ( (W^{[l+1]})^t \\delta^{[l+1]} \\right )  \\circ f'(z^{[l]}) \\\\\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\delta^l (a^{[l-1]})^t  \\\\\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\delta^l\n",
    "$\n",
    "    \n",
    "where $\\hat{y} = a^{[L]}$, $X = a^{[0]}$ and we assume all activations function are the same $f$ . Also, $\\circ$ is the element-wise multiplication. It is important in the euqation so the matrix dimensions can match.  \n",
    "\n",
    "The proof of the equations can be found: http://neuralnetworksanddeeplearning.com/chap2.html#proof_of_the_four_fundamental_equations_(optional)\n",
    "\n",
    "**TODO**: Try read and understand the proof. It will help you a lot understand the papers in the area. Or if you need to implement you personal layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "* There is a lot of details and math hot to get the equations. But there a ways to develope the equations\n",
    "\n",
    "    * Using sum and indexes notation like in: $\\frac{\\partial}{\\partial W^{[2]}_{i,j}}J = U_i f'(z^{[3]}_i)a^{[2]}_j \\equiv \\delta^{[3]}_i a^{[2]}_j$\n",
    "    \n",
    "    * Using matrix notation that simplify the equations but it is hard to understand. The equation above is tranlaste as: $\\frac{\\partial}{\\partial W^{[2]}}J = \\delta^{[3]}(a^{[2]})^t$ where $\\delta^{[3]} = U\\circ f'(z^{[3]})$ and $\\circ$ is the element-wise or Hadamard product.\n",
    "    \n",
    "* UNderstandthe big picture\n",
    "\n",
    "    * https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb  See code in train method <== Help me understand the diff with gradient descent\n",
    "    * helps to understand the equations: http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "    * wikipedia: https://en.wikipedia.org/wiki/Backpropagation\n",
    "    \n",
    "* UNderstan the implementatiio\n",
    "    * https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "    \n",
    "    \n",
    "* Understanding math with index notation\n",
    "    * https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n",
    "    \n",
    "* Undersatnding math with matrix notation\n",
    "    * https://sudeepraja.github.io/Neural/\n",
    "    * https://www.youtube.com/watch?v=isPiE-DBagM&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6 vidoe form standfort\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The chain rule and rule while deriving matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain rules\n",
    "\n",
    "$\n",
    "u = u(x(r,t), y(r,y)) \\\\\n",
    "\\frac{\\partial}{\\partial r}u = \\frac{\\partial}{\\partial x}u \\frac{\\partial}{\\partial r}x = \\frac{\\partial}{\\partial y}u \\frac{\\partial}{\\partial r}y\n",
    "$\n",
    "\n",
    "When deriving related to a matrix or vector\n",
    "\n",
    "1. The derivative has the same dimension of the the ariable been derived  **\n",
    "\n",
    "    * Scalar by scalar: $\\frac{\\partial}{\\partial x}f(x) = f'$ , so $f'$ is scalar\n",
    "    * Scalar by a vector $\\frac{\\partial}{\\partial \\mathbf{x}}f(\\mathbf{x}) = \\mathbf{f'}$ , so $\\mathbf{f'}$ is a vector of dim of $\\mathbf{x}$\n",
    "    * Matrix or vector by a matrix: $\\frac{\\partial}{\\partial \\mathbf{x}}f(\\mathbf{x}) = \\mathbf{f'}$ , so $\\mathbf{f'}$ is a vector of dim of $\\mathbf{x}$\n",
    "\n",
    "ref: https://kevinzakka.github.io/2016/09/14/batch_normalization/ But I need to double check this rules with other sources and check if it is compatible to 4 Backpropagation equations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to proof some of the equations\n",
    "\n",
    "Define gradient in matrix form as:\n",
    "\n",
    "$\n",
    "J = J(X_1,X_2,...,X_m)  \\\\\n",
    "\\frac{\\partial}{\\partial X}J = [\\frac{\\partial}{\\partial X_1}J, \\frac{\\partial}{\\partial X_2}J, ..., \\frac{\\partial}{\\partial X_m}J] \n",
    "$\n",
    "\n",
    "> PS: derivatives are row vectors\n",
    "\n",
    "\n",
    "Page 1\n",
    "<img src=\"../images/backprop_1.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> \n",
    "\n",
    "-----\n",
    "Page2\n",
    "\n",
    "<img src=\"../images/backprop_2.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The cost or objective function:  \n",
    "\n",
    "$\n",
    "J(\\theta = {W,b}) = \\frac{1}{m} \\sum^m_{i=1} \\ell(\\hat{y^{(i)}} = h_{\\theta}(x^{(i)}), y^{(i)})\n",
    "$\n",
    "\n",
    "* The loss function:\n",
    "\n",
    "$\n",
    "\\ell(\\hat{y^{(i)}}, y^{(i)})\n",
    "$\n",
    "\n",
    "* The activation function of the last layer:\n",
    "\n",
    "$\n",
    "g = h_{\\theta}(x^{(i)})\n",
    "$\n",
    "\n",
    "Reviewing chaing rule:\n",
    "\n",
    "* For function of 1 variable:\n",
    "$\n",
    "y =  f(u(x)) \\\\\n",
    "\\frac{d}{dx}y = \\frac{d}{du}y \\frac{d}{dx}u\n",
    "$\n",
    "\n",
    "* For 2 vriables \n",
    "\n",
    "$\n",
    "y =  f(u(x)) \\\\\n",
    "\\frac{d}{dx}y = \\frac{d}{du}y \\frac{d}{dx}u\n",
    "$\n",
    "\n",
    "To optmize the cost funtction $J(W^{[1]},b^{[1]},..., W^{[l]}, b^{[l]}, ...,W^{[L]}, b^{[L]} )$, where $L$ is the number of layers, we need to compute:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J \\frac{\\partial}{\\partial W^{[l]}}z^{[l]} \\\\ \n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J a^{[l-1]} \\\\\n",
    "\\frac{\\partial}{\\partial b^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J \\frac{\\partial}{\\partial b^{[l]}}z^{[l]} \\\\ \n",
    "\\frac{\\partial}{\\partial b^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J\n",
    "$\n",
    "\n",
    "\n",
    "The common term is called the local gradient:\n",
    "\n",
    "$\n",
    "\\delta^{[l]} = \\frac{\\partial}{\\partial z^{[l]}}J\n",
    "$\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "\n",
    "* Forwards pass in matrix notation:\n",
    "\n",
    "https://sudeepraja.github.io/Neural/ Matrix notation\n",
    "\n",
    "$\n",
    "J(\\theta = {W,b}) = \\frac{1}{m} \\sum^m_{i=1} \\ell(\\hat{y^{(i)}} = h_{\\theta}(x^{(i)}), y^{(i)}) \\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]})\n",
    "$\n",
    "\n",
    "* Backwards pass\n",
    "\n",
    "Last layer (mse loss)\n",
    "\n",
    "$\n",
    "\\delta^{[L]} = (a^{[L]} - y) g'^{[L]}(z^{[L]}) \\\\\n",
    "\\delta^{[L]} = (a^{[L]} - y) g'^{[L]}(W^{[L]}a^{[L-1]}) \\\\  \n",
    "$\n",
    "\n",
    "Hidden Layers\n",
    "\n",
    "$\n",
    "\\delta^{[l]} = (W^{[l]})^t \\delta^{[l+1]} g'^{[l]}(W^{[l]}a^{[l-1]} + b^{[l]})  \n",
    "$\n",
    "\n",
    "Weight updates:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J  = \\delta^{[l]} (a^{[l]})^t \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial}{\\partial W^{[l]}}J \\\\\n",
    "\\frac{\\partial}{\\partial b^{[l]}}J  = \\delta^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial}{\\partial b^{[l]}}J\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double checking the backpropagation equations\n",
    "\n",
    "https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 1\n",
    "<img src=\"../images/backprop_notes_0.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Page 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/backprop_notes_1.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "Page 3\n",
    "<img src=\"../images/backprop_notes_2.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Page 4\n",
    "<img src=\"../images/backprop_notes_3.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Page 5\n",
    "<img src=\"../images/backprop_notes_4.jpg\" style=\"float:left\" width=\"600\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def _get_activation_func(self,activation_type):\n",
    "            \n",
    "        if activation_type == 'relu':\n",
    "            \n",
    "            def activation_func(z: np.array):\n",
    "    \n",
    "                a = np.maximum(0.0,z)\n",
    "        \n",
    "                return a\n",
    "            \n",
    "            def activation_derivative(z: no.array):\n",
    "                \n",
    "                \n",
    "                return \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            def activation_func(z: np.array):\n",
    "    \n",
    "                a = 1.0/(1.0 + np.exp(-z))\n",
    "        \n",
    "                return a\n",
    "        \n",
    "            def activation_derivative_func(z: no.array):\n",
    "                \n",
    "                return activation_func*(1.0 - activation_func)\n",
    "                \n",
    "        \n",
    "        return activation_func, activation_derivative_func\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_type):\n",
    "\n",
    "        self.activation_type = activation_type\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        \n",
    "        activation , first_deriv_of_activation = self._get_activation_func(activation_type)\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.activation_derivative = first_deriv_of_activation\n",
    "            \n",
    "class DenseLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, activation_type):\n",
    "        \n",
    "        super().__init__( input_size, output_size, activation_type)\n",
    "\n",
    "    def forward_propagation(self, X_in: np.array, W: np.array, b: np.array) -> Tuple[np.array]:\n",
    "        \n",
    "        z = W.dot(X_in)\n",
    "        a = self.activation(z)\n",
    "        \n",
    "        return a, z\n",
    "    \n",
    "    def backward_propagation(self, da, W, b, z, a_prev):\n",
    "\n",
    "        dz = self.activation_derivative(z)*da  # element-wise multiplication\n",
    "        \n",
    "        dW = np.dot(dz, a_prev.T) / m\n",
    "        db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "        \n",
    "        da_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return da_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'sigmoid'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 3; in_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_size: 1; in_size: 2\n",
      "Expected Z\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.]), array([0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.5]), array([0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([1.]), array([1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.73105858]), array([1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.]), array([0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.5]), array([0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_size = 1\n",
    "out_size = 1\n",
    "batch_size = 3\n",
    "\n",
    "l1 = DenseLayer(in_size,out_size,'relu')\n",
    "l1.activation_type\n",
    "\n",
    "l2 = DenseLayer(in_size,out_size, 'sigmoid')\n",
    "l2.activation_type\n",
    "\n",
    "\n",
    "X = np.array([[1.0, 2.0], \n",
    "              [1.0,4.0],\n",
    "             [0.0, 0.0]])\n",
    "\n",
    "print(f\"batch_size: {X.shape[0]}; in_size: {X.shape[1]}\")\n",
    "\n",
    "W = np.array([-1.0, 0.5]).reshape((1,2))\n",
    "W.shape\n",
    "print(f\"out_size: {W.shape[0]}; in_size: {W.shape[1]}\")\n",
    "\n",
    "\n",
    "b = np.zeros((1,))\n",
    "\n",
    "print(\"Expected Z\")\n",
    "W.dot(X[0,:])\n",
    "W.dot(X[1,:])\n",
    "W.dot(X[2,:])\n",
    "\n",
    "print(\"Layers output\")\n",
    "\n",
    "for k in range(batch_size):\n",
    "    \n",
    "    Xb = X[k,:]\n",
    "\n",
    "    l1.forward_propagation(Xb,W,b)\n",
    "    l2.forward_propagation(Xb,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "\n",
    "    def __init__(self, layers, learning_rate = 0.1, seed= 2021):\n",
    "        \n",
    "        self._seed = seed \n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.params_values = {}\n",
    "\n",
    "        self._init_layer_parameter_random()\n",
    "        \n",
    "        loss, loss_derivative = self._get_loss('cross-entropy')\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.loss_derivative = loss_derivative\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        \n",
    "\n",
    "    def _get_loss(loss_type):\n",
    "        \n",
    "        if loss_type == 'cross-entropy':\n",
    "            \n",
    "            def loss(y_hat, y):\n",
    "                \n",
    "                # J = - 1/m \\sum_1^m y log y_hat - (1 - y) log(1-y_hat)  <= cross entropy loss \n",
    "                m = y_hat.shape[1]\n",
    "                cost = -1.0 / m * (np.dot(y, np.log(y_hat).T) + np.dot(1.0 - y, np.log(1.0 - y_hat).T))\n",
    "                \n",
    "            return np.squeeze(cost)\n",
    "        \n",
    "        \n",
    "            def loss_derivative(y_hat, y):\n",
    "                \n",
    "                m = y_hat.shape[1]\n",
    "                \n",
    "                # y_hat is teh activation in the last Layer L: a^{[L]}\n",
    "                # dJ/da^[L] = dJ/dy_hat = y/y_hat - (1-y)/(1-y_hat)\n",
    "                da_prev =  -(np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat))/m\n",
    "            \n",
    "                return da_prev\n",
    "        else:\n",
    "            \n",
    "            # mse loss\n",
    "           \n",
    "            def loss(y_hat, y):\n",
    "                \n",
    "                # J = - 1/m \\sum_1^m y log y_hat - (1 - y) log(1-y_hat)  <= cross entropy loss \n",
    "                m = y_hat.shape[1]\n",
    "                cost = -0.5/m * np.dot((y - y_hat).t, (y - y_hat))\n",
    "                \n",
    "            return np.squeeze(cost)\n",
    "        \n",
    "        \n",
    "            def loss_derivative(y_hat, y):\n",
    "\n",
    "                m = y_hat.shape[1]\n",
    "                \n",
    "                # y_hat is teh activation in the last Layer L: a^{[L]}\n",
    "                # TODO: Review this equation. Maybe the constant is missing\n",
    "                # Should I include or not the term f'(z) the activation function\n",
    "                da_prev =  1.0/m * (y-y_hat)\n",
    "            \n",
    "                return da_prev\n",
    "            \n",
    "\n",
    "        return loss, loss_derivative\n",
    "        \n",
    "    # All bias are zero\n",
    "    def _init_layer_parameter_random(self):\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            \n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            # We need initialize with small values. Because high values are regions with\n",
    "            # samall (almost vanish) gradients for sigmois, tanh funcitons\n",
    "            W = np.random.randn(layer.output_size, layer.input_size) * 0.01\n",
    "            self.params_values['W' + str(layer_idx)] =  W\n",
    "            \n",
    "            b = np.random.randn(layer.output_size, 1) * 0.01\n",
    "            self.params_values['b' + str(layer_idx)] = b\n",
    "\n",
    "    def _init_layer_paremeter_xavier(self):\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def compile(self, optimizer,loss_type,metrcis):\n",
    "        \n",
    "        # TODO: Set this objects in the neral networks\n",
    "        pass\n",
    "        \n",
    "    def accuracy(self, y_hat, y):\n",
    "        \n",
    "        y_hat_ = convert_prob_into_class(y_hat)\n",
    "        \n",
    "        return (y_hat_ == y).all(axis=0).mean()\n",
    "\n",
    "        \n",
    "    def predict(self, X_in: np.array) -> np.array:\n",
    "        \n",
    "        y, _ = self.forward_propagation(X_in)\n",
    "        \n",
    "        return y\n",
    "     \n",
    "    def forward_propagation(self, X_in: np.array) -> Tuple[np.array, dict] : \n",
    "        # forward propagation\n",
    "        \n",
    "        memory = {}\n",
    "        # a^{[0]} = X_in. The first activation are the inputs\n",
    "        a_curr = X_in\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            \n",
    "            layer_idx = idx + 1\n",
    "            a_prev = A_curr\n",
    "            \n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx)]\n",
    "            \n",
    "            a_curr, z_curr = layer.forward_propagation(a_curr, W_curr, b_curr) \n",
    "            \n",
    "            # Need this for back propagation \n",
    "            memory[\"a\" + str(idx)] = a_prev  # a^{[0]} = X_in\n",
    "            memory[\"z\" + str(layer_idx)] = z_curr\n",
    "    \n",
    "        return A_curr, memory\n",
    "    \n",
    "    def backward_propagation(y_hat, y, memory):\n",
    "        \n",
    "        gradients = {}\n",
    "        m = y.shape[1]\n",
    "        y = y.reshape(y_hat.shape)\n",
    "        \n",
    "        # d/da^{[L]}J \n",
    "        da_prev = self.loss_derivative(y_hat, y)\n",
    "        \n",
    "        for idx, layer in reversed(list(enumerate(layers))):\n",
    "    \n",
    "            layer_idx = idx + 1\n",
    "            print(f\"{idx}: {layer}\")\n",
    "            \n",
    "            da_curr = da_prev\n",
    "\n",
    "            a_prev = memory[\"a\" + str(layer_idx - 1)]\n",
    "            z = memory[\"z\" + str(layer_idx)]\n",
    "            \n",
    "            W = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b = self.params_values[\"b\" + str(layer_idx)]\n",
    "            \n",
    "            da_prev, dW, db = layer.backward_propagation(da, W, b, z, a_prev)\n",
    "            \n",
    "            gradients[\"dW\" + str(layer_idx_curr)] = dW\n",
    "            gradients[\"db\" + str(layer_idx_curr)] = db\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def _update_parameters(self,grads_values):\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            \n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            self.params_values[\"W\" + str(layer_idx)] -= self.lr * grads_values[\"dW\" + str(layer_idx)]        \n",
    "            self.params_values[\"b\" + str(layer_idx)] -= self.lr * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    \n",
    "    def train(self, X, y, epochs=3):\n",
    "        \n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        \n",
    "        # Gradient descent \n",
    "        for e in range(epochs):\n",
    "         \n",
    "            y_hat, memory = forward_propagation(X_in)\n",
    "            \n",
    "            cost = self.loss(y_hat, y)\n",
    "            acc = self.accuracy(y_hat, y)\n",
    "            \n",
    "            cost_history.append(cost)\n",
    "            accuracy_history.append(acc)\n",
    "           \n",
    "            #  da_prev, dW, db = grads\n",
    "            grads = self.backward_propagation(y_hat, y, memory)\n",
    "           \n",
    "            # update self.params_values\n",
    "            self._update(grads)\n",
    "            \n",
    "        return self.params_values, cost_history, accuracy_history\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    " \n",
    "1. ~~validate forward~~ OK\n",
    "1. validate accuracy  this is easy\n",
    "1. validate loss function  easy\n",
    "1. validate layer backpropagation   this one is hard todo it\n",
    "1. validate update parameters This one is easy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 3\n",
      "1: 2\n",
      "2: 1\n",
      "\n",
      "3: 3\n",
      "2: 2\n",
      "1: 1\n"
     ]
    }
   ],
   "source": [
    "layers = [1,2,3]\n",
    "\n",
    "\n",
    "for idx, layer in enumerate(list(reversed(layers))):\n",
    "    \n",
    "    print(f\"{idx}: {layer}\")\n",
    "    \n",
    "print()\n",
    "for idx, layer in reversed(list(enumerate(layers))):\n",
    "    \n",
    "    idx = idx + 1\n",
    "    \n",
    "    print(f\"{idx}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compare or study.\n",
    "# The loss is mean square erro (mse = (y - y_hat)^2)\n",
    "def backpropagation(self,y, z_s, a_s):\n",
    "    \n",
    "    dw = []  # dC/dW\n",
    "    db = []  # dC/dB\n",
    "    \n",
    "    deltas = [None] * len(self.weights)  # delta = dC/dZ  known as error for each layer\n",
    "    \n",
    "    # insert the last layer error\n",
    "    # the derivative of mse: 2(y-y_hat)\n",
    "    deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
    "\n",
    "    # Perform BackPropagation\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "    \n",
    "        deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
    "        batch_size = y.shape[1]\n",
    "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "        \n",
    "        # return the derivitives respect to weight matrix and biases\n",
    "    return dw, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tetsing parameter init with previous layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.01488609, 0.00676011]]),\n",
       " 'b1': array([[-0.00418451]]),\n",
       " 'W2': array([[-0.00806521]]),\n",
       " 'b2': array([[0.00555876]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.49991546])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'A0': array([1., 4.]),\n",
       " 'Z1': array([0.04192653]),\n",
       " 'A1': array([0.04192653]),\n",
       " 'Z2': array([-0.00033815])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = DenseLayer(input_size=2,output_size=1,activation_type = 'relu')\n",
    "l2 = DenseLayer(input_size=1,output_size=1,activation_type = 'sigmoid')\n",
    "\n",
    "layers = [l1,l2]\n",
    "\n",
    "nn = NN(layers)\n",
    "nn.params_values\n",
    "\n",
    "y , memory = nn.predict(X[1,:])\n",
    "y \n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization  \n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* Read the paper. It explain the motivations, why there is trainable parameters and how the backpropagation is affected. There isn important conclusion regards the mistake of not including the dependency of x when doing back proagation of the bias parameter taht clears depend of the mean of the mini batch\n",
    "\n",
    "(Breaktrough in the area)\n",
    "refs:\n",
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\n",
    "    * Authors: Sergey Ioffe (same author of PLDA and works at Google) n Christian Szegedy (google)\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf Paper **TODO** Read the paper. It is simple and easy to understand/ It is a good gain experience in reading paper  \n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization tensprflow doc\n",
    "\n",
    "\n",
    "refs:\n",
    "* https://www.deeplearningbook.org/contents/optimization.html\n",
    "* https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "* https://arxiv.org/pdf/1502.03167.pdf\n",
    "* https://arxiv.org/pdf/1702.03275.pdf\n",
    "* https://www.youtube.com/watch?v=nUUqwaxLnWs\n",
    "* https://arxiv.org/pdf/1805.11604.pdf\n",
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795    \n",
    "* https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "* Batch normalization backpropagation\n",
    "    * https://kevinzakka.github.io/2016/09/14/batch_normalization/  with the equations\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf  the paper is look like more clear\n",
    "\n",
    "**It is being asked more often in the job interview**\n",
    "\n",
    "Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.\n",
    "\n",
    "**Donâ€™t Use With Dropout:**\n",
    "\n",
    "Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input are the values of x over a batch: $B = {x_1, x_2,..., x_i,..., x_m}$\n",
    "    * where $m$ is the batch size\n",
    "* Output: $y_i = BN_{\\gamma,\\beta}(x_i)$\n",
    "* Learning parameters: $\\gamma$ and $\\beta$\n",
    "* Normalization:\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i \\\\\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\\\\n",
    "z_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} \\\\\n",
    "y_i = BN_{\\gamma,\\beta}(x_i) \\equiv \\gamma z_i + \\beta\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.87152147, 1.85580424, 2.26072622, 1.00925124, 2.84652559,\n",
       "       0.97547987, 2.91852008, 2.38156737, 2.589072  , 4.11549317])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 2.38; std: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Batch\n",
    "X = np.random.uniform(0,5.0,size=(10))\n",
    "X\n",
    "\n",
    "print(f\"shape: {X.shape}; mean: {X.mean():.2f}; std: {X.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.48732024e-01, -5.90764377e-01, -1.36497064e-01, -1.54048155e+00,\n",
       "        5.20690049e-01, -1.57836842e+00,  6.01458063e-01, -9.29752983e-04,\n",
       "        2.31862185e-01,  1.94429885e+00])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.48732024e-01, -5.90764377e-01, -1.36497064e-01, -1.54048155e+00,\n",
       "        5.20690049e-01, -1.57836842e+00,  6.01458063e-01, -9.29752983e-04,\n",
       "        2.31862185e-01,  1.94429885e+00])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 0.00; std: 1.00\n"
     ]
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "epsilon = 0.0\n",
    "\n",
    "# because we did not train the layer, we are passing the mean and the variance of the batch\n",
    "Y = tf.nn.batch_normalization(X,\n",
    "                    mean = X.mean(axis=0),        # batch mean\n",
    "                    variance = X.var(axis=0),     # batch var\n",
    "                    offset = beta,scale = gamma,  # batch beta and gamma See equations  \n",
    "                    variance_epsilon = epsilon)   # batch epsilon See equations\n",
    "\n",
    "Y.numpy()\n",
    "\n",
    "# comparing with numpy\n",
    "\n",
    "Z = (X - X.mean(axis=0))/np.sqrt(X.var(axis=0) + epsilon)\n",
    "Y = gamma * Z + beta\n",
    "Y\n",
    "\n",
    "# Expectd zero mean and unit variance\n",
    "print(f\"shape: {Y.shape}; mean: {Y.mean():.2f}; std: {Y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: [ 5.48732024e-01 -5.90764377e-01 -1.36497064e-01 -1.54048155e+00\n  5.20690049e-01 -1.57836842e+00  6.01458063e-01 -9.29752983e-04\n  2.31862185e-01  1.94429885e+00]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-18002abdb984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m       raise TypeError('The added layer must be '\n\u001b[0m\u001b[1;32m    183\u001b[0m                       \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                       'Found: ' + str(layer))\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: [ 5.48732024e-01 -5.90764377e-01 -1.36497064e-01 -1.54048155e+00\n  5.20690049e-01 -1.57836842e+00  6.01458063e-01 -9.29752983e-04\n  2.31862185e-01  1.94429885e+00]"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "301.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
