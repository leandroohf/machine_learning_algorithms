{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "**Goals**:\n",
    "\n",
    "* Goal understand better the link with gradients and backproapagation\n",
    "* Review backpropagation\n",
    "* IMplement dense layer and neural netowork\n",
    "* tets implemenattion\n",
    "* Learn batch normalization\n",
    "* IMplement Batch normalization layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import List, Set, Dict, Tuple, Optional, Union\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "* https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement simple regression first\n",
    "\n",
    "\n",
    "1. One layer Dense\n",
    "1. Regression\n",
    "\n",
    "Steps:\n",
    "    \n",
    "* Implement parts \n",
    "    1. activatoins\n",
    "    1. code one layer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/simple_nn.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} \\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]})\n",
    "$\n",
    "\n",
    "<img src=\"../images/forward_step.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "refs:\n",
    "* https://sudeepraja.github.io/Neural/ Matrix notation\n",
    "* https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often people confuse backward propagation with gradient descent. The gradient descent algorithm minimizes (optmizes) the cost function. And the backpropagation updates all parameters in the neural network based on the chain rule fomula and the gradient returned in the gradient descent algorithm   \n",
    "\n",
    "\n",
    "* The cost or objective function:  \n",
    "\n",
    "$\n",
    "J(\\theta = {W,b}) = \\frac{1}{m} \\sum^m_{i=1} \\ell(\\hat{y^{(i)}} = h_{\\theta}(x^{(i)}), y^{(i)})\n",
    "$\n",
    "\n",
    "* The loss function:\n",
    "\n",
    "$\n",
    "\\ell(\\hat{y^{(i)}}, y^{(i)})\n",
    "$\n",
    "\n",
    "* The activation function of the last layer:\n",
    "\n",
    "$\n",
    "g = h_{\\theta}(x^{(i)})\n",
    "$\n",
    "\n",
    "Reviewing chaing rule:\n",
    "\n",
    "* For function of 1 variable:\n",
    "$\n",
    "y =  f(u(x)) \\\\\n",
    "\\frac{d}{dx}y = \\frac{d}{du}y \\frac{d}{dx}u\n",
    "$\n",
    "\n",
    "* For 2 vriables \n",
    "\n",
    "$\n",
    "y =  f(u(x)) \\\\\n",
    "\\frac{d}{dx}y = \\frac{d}{du}y \\frac{d}{dx}u\n",
    "$\n",
    "\n",
    "To optmize the cost funtction $J(W^{[1]},b^{[1]},..., W^{[l]}, b^{[l]}, ...,W^{[L]}, b^{[L]} )$, where $L$ is the number of layers, we need to compute:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J \\frac{\\partial}{\\partial W^{[l]}}z^{[l]} \\\\ \n",
    "\\frac{\\partial}{\\partial W^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J a^{[l-1]} \\\\\n",
    "\\frac{\\partial}{\\partial b^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J \\frac{\\partial}{\\partial b^{[l]}}z^{[l]} \\\\ \n",
    "\\frac{\\partial}{\\partial b^{[l]}}J = \\frac{\\partial}{\\partial z^{[l]}}J\n",
    "$\n",
    "\n",
    "\n",
    "The common term is called the local gradient:\n",
    "\n",
    "$\n",
    "\\delta^{[l]} = \\frac{\\partial}{\\partial z^{[l]}}J\n",
    "$\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "\n",
    "* Forwards pass in mtrix notation:\n",
    "\n",
    "https://sudeepraja.github.io/Neural/ Matrix notation\n",
    "\n",
    "$\n",
    "J(\\theta = {W,b}) = \\frac{1}{m} \\sum^m_{i=1} \\ell(\\hat{y^{(i)}} = h_{\\theta}(x^{(i)}), y^{(i)}) \\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]})\n",
    "$\n",
    "\n",
    "* Backwards pass\n",
    "\n",
    "Last layer\n",
    "$\n",
    "\\delta^{[L]} = (a^{[L]} - y) g'^{[L]}(W^{[L]}a^{[L-1]}) \\\\  \n",
    "$\n",
    "\n",
    "Hidden Layers\n",
    "\n",
    "$\n",
    "\\delta^{[l]} = (W^{[l]})^t \\delta^{[l+1]} g'^{[l]}(W^{[l]}a^{[l-1]} + b^{[l]})  \n",
    "$\n",
    "\n",
    "Weight updates:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial W^{[l]}}J  = \\delta^{[l]} (a^{[l]})^t \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial}{\\partial W^{[l]}}J \\\\\n",
    "\\frac{\\partial}{\\partial b^{[l]}}J  = \\delta^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial}{\\partial b^{[l]}}J\n",
    "$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def _get_activation_func(self,activation_type):\n",
    "            \n",
    "        if activation_type == 'relu':\n",
    "            \n",
    "            def activation_func(z: np.array):\n",
    "    \n",
    "                a = np.maximum(0.0,z)\n",
    "        \n",
    "                return a\n",
    "        else:\n",
    "            \n",
    "            def activation_func(z: np.array):\n",
    "    \n",
    "                a = 1.0/(1.0 + np.exp(-z))\n",
    "        \n",
    "                return a\n",
    "        \n",
    "        return activation_func\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_type):\n",
    "\n",
    "        self.activation_type = activation_type\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.activation = self._get_activation_func(activation_type)\n",
    "            \n",
    "class DenseLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, activation_type):\n",
    "        \n",
    "        super().__init__( input_size, output_size, activation_type)\n",
    "\n",
    "    def forward_propagation(self, X_in: np.array, W: np.array, b: np.array) -> Tuple[np.array]:\n",
    "        \n",
    "        z = W.dot(X_in)\n",
    "        a = self.activation(z)\n",
    "        \n",
    "        return a, z\n",
    "    \n",
    "    def  backward_propagation(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'sigmoid'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 3; in_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_size: 1; in_size: 2\n",
      "Expected Z\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.]), array([0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.5]), array([0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([1.]), array([1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.73105858]), array([1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.]), array([0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.5]), array([0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_size = 1\n",
    "out_size = 1\n",
    "batch_size = 3\n",
    "\n",
    "l1 = DenseLayer(in_size,out_size,'relu')\n",
    "l1.activation_type\n",
    "\n",
    "l2 = DenseLayer(in_size,out_size, 'sigmoid')\n",
    "l2.activation_type\n",
    "\n",
    "\n",
    "X = np.array([[1.0, 2.0], \n",
    "              [1.0,4.0],\n",
    "             [0.0, 0.0]])\n",
    "\n",
    "print(f\"batch_size: {X.shape[0]}; in_size: {X.shape[1]}\")\n",
    "\n",
    "W = np.array([-1.0, 0.5]).reshape((1,2))\n",
    "W.shape\n",
    "print(f\"out_size: {W.shape[0]}; in_size: {W.shape[1]}\")\n",
    "\n",
    "\n",
    "b = np.zeros((1,))\n",
    "\n",
    "print(\"Expected Z\")\n",
    "W.dot(X[0,:])\n",
    "W.dot(X[1,:])\n",
    "W.dot(X[2,:])\n",
    "\n",
    "print(\"Layers output\")\n",
    "\n",
    "for k in range(batch_size):\n",
    "    \n",
    "    Xb = X[k,:]\n",
    "\n",
    "    l1.forward_propagation(Xb,W,b)\n",
    "    l2.forward_propagation(Xb,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "\n",
    "    def __init__(self, layers, seed= 2021):\n",
    "        \n",
    "        self._seed = seed \n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.params_values = {}\n",
    "\n",
    "        self._init_layer_parameter_random()\n",
    "                        \n",
    "    # All bias are zero\n",
    "    def _init_layer_parameter_random(self):\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            \n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            # We need initialize with small values. Because high values are regions with\n",
    "            # samall (almost vanish) gradients for sigmois, tanh funcitons\n",
    "            W = np.random.randn(layer.output_size, layer.input_size) * 0.01\n",
    "            self.params_values['W' + str(layer_idx)] =  W\n",
    "            \n",
    "            b = np.random.randn(layer.output_size, 1) * 0.01\n",
    "            self.params_values['b' + str(layer_idx)] = b\n",
    "\n",
    "    def _init_layer_paremeter_xavier(self):\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def predict(self, X_in: np.array) -> np.array:\n",
    "        \n",
    "        y, _ = self.forward_propagation(X_in)\n",
    "        \n",
    "        return y\n",
    "     \n",
    "    def forward_propagation(self, X_in: np.array) -> Tuple[np.array, dict] : \n",
    "        # forward propagation\n",
    "        \n",
    "        memory = {}\n",
    "        A_curr = X_in\n",
    "        \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            \n",
    "            layer_idx = idx + 1\n",
    "            A_prev = A_curr\n",
    "            \n",
    "            W_curr = self.params_values[\"W\" + str(layer_idx)]\n",
    "            b_curr = self.params_values[\"b\" + str(layer_idx)]\n",
    "            \n",
    "            A_curr, Z_curr = layer.forward_propagation(A_curr, W_curr, b_curr) \n",
    "            \n",
    "            # Need this for back propagation \n",
    "            memory[\"A\" + str(idx)] = A_prev  # A0 = X_in\n",
    "            memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "    \n",
    "        return A_curr, memory\n",
    "    \n",
    "    def train(self, X, Y, epochs=3):\n",
    "        \n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            \n",
    "            y_hat, memory = forward_propagation(X_in)\n",
    "            \n",
    "            cost = self.loss(y_hat, y)\n",
    "            acc = self.accuracy(y_hat, y)\n",
    "            \n",
    "            cost_history.append(cost)\n",
    "            accuracy_history.append(acc)\n",
    "            \n",
    "\n",
    "            grads = self.backward_propagation(y_hat, y, memory)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tetsing parameter init with previous layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.01488609, 0.00676011]]),\n",
       " 'b1': array([[-0.00418451]]),\n",
       " 'W2': array([[-0.00806521]]),\n",
       " 'b2': array([[0.00555876]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.49991546])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'A0': array([1., 4.]),\n",
       " 'Z1': array([0.04192653]),\n",
       " 'A1': array([0.04192653]),\n",
       " 'Z2': array([-0.00033815])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = DenseLayer(input_size=2,output_size=1,activation_type = 'relu')\n",
    "l2 = DenseLayer(input_size=1,output_size=1,activation_type = 'sigmoid')\n",
    "\n",
    "layers = [l1,l2]\n",
    "\n",
    "nn = NN(layers)\n",
    "nn.params_values\n",
    "\n",
    "y , memory = nn.predict(X[1,:])\n",
    "y \n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization  \n",
    "\n",
    "(Breaktrough in the area)\n",
    "refs:\n",
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\n",
    "    * Authors: Sergey Ioffe (same author of PLDA and works at Google) n Christian Szegedy (google)\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf Paper **TODO** Read the paper. It is simple and easy to understand/ It is a good gain experience in reading paper  \n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization tensprflow doc\n",
    "\n",
    "\n",
    "refs:\n",
    "* https://www.deeplearningbook.org/contents/optimization.html\n",
    "* https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "* https://arxiv.org/pdf/1502.03167.pdf\n",
    "* https://arxiv.org/pdf/1702.03275.pdf\n",
    "* https://www.youtube.com/watch?v=nUUqwaxLnWs\n",
    "* https://arxiv.org/pdf/1805.11604.pdf\n",
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795    \n",
    "* https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "**It is being asked more often in the job interview**\n",
    "\n",
    "Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.\n",
    "\n",
    "\n",
    "Donâ€™t Use With Dropout:\n",
    "\n",
    "Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input are the values of x over a batch: $B = {x_1, x_2,..., x_i,..., x_m}$\n",
    "    * where $m$ is the batch size\n",
    "* Output: $y_i = BN_{\\gamma,\\beta}(x_i)$\n",
    "* Learning parameters: $\\gamma$ and $\\beta$\n",
    "* Normalization:\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i \\\\\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\\\\n",
    "z_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} \\\\\n",
    "y_i = BN_{\\gamma,\\beta}(x_i) \\equiv \\gamma z_i + \\beta\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.31080257, 3.92155066, 0.48447198, 0.29285643, 4.81197995,\n",
       "       3.08278722, 0.43314981, 2.80636181, 3.08262354, 4.81921511])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 2.70; std: 1.64\n"
     ]
    }
   ],
   "source": [
    "# Batch\n",
    "X = np.random.uniform(0,5.0,size=(10))\n",
    "X\n",
    "\n",
    "print(f\"shape: {X.shape}; mean: {X.mean():.2f}; std: {X.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36919292,  0.74114184, -1.35205787, -1.46875279,  1.28341815,\n",
       "        0.23033032, -1.38331335,  0.06198574,  0.23023064,  1.2878244 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.36919292,  0.74114184, -1.35205787, -1.46875279,  1.28341815,\n",
       "        0.23033032, -1.38331335,  0.06198574,  0.23023064,  1.2878244 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: -0.00; std: 1.00\n"
     ]
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "epsilon = 0.0\n",
    "\n",
    "# because we did not train the layer, we are passing the mean and the variance of the batch\n",
    "Y = tf.nn.batch_normalization(X,\n",
    "                    mean = X.mean(axis=0),        # batch mean\n",
    "                    variance = X.var(axis=0),     # batch var\n",
    "                    offset = beta,scale = gamma,  # batch beta and gamma See equations  \n",
    "                    variance_epsilon = epsilon)   # batch epsilon See equations\n",
    "\n",
    "Y.numpy()\n",
    "\n",
    "# comparing with numpy\n",
    "\n",
    "Z = (X - X.mean(axis=0))/np.sqrt(X.var(axis=0) + epsilon)\n",
    "Y = gamma * Z + beta\n",
    "Y\n",
    "\n",
    "# Expectd zero mean and unit variance\n",
    "print(f\"shape: {Y.shape}; mean: {Y.mean():.2f}; std: {Y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
