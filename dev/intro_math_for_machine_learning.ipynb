{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d84d3c85",
   "metadata": {},
   "source": [
    "# Matrix derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714aa97a",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "* MOtivations\n",
    "    * show matrix notation is concise\n",
    "    * Ex: $\\sum_i x_i = 1^T X$ or\n",
    "    * $Y = WX$ <= linear problem or Cost \n",
    "    * $J(\\theta) = \\frac{1}{2n} \\left ( X \\theta - y \\right )^t\\left ( X \\theta - y \\right )$   <== function applications\n",
    "\n",
    "* Proof \n",
    "    * d/dX X = I (identity)\n",
    "    * d/dx X^t X and d/dX^t X^tX\n",
    "    * d/dX X^T A X\n",
    "    * d/dX (X^{-1})\n",
    "    * d/dX AX = A^T\n",
    "    * Rememeber:\n",
    "        * (AB)^T = (B^TA) => (W^TX) = X^TW\n",
    "        * (AB) != BA  <= \n",
    "    * A symetric => (X^T A X)^T = (X^T A X)  (might be handy)\n",
    "    \n",
    "    \n",
    "* Verify that the Limit definition is equivalent to d/dX derive over all variables (**Jacobian definition**)\n",
    "    * for at least on simple example that bith deifinition leads to same resuls\n",
    "    \n",
    "* Mention because of the Limit definition the chain rule applies and to derivative\n",
    "    * review chain rule because of the matrix dimension needs to pay attention\n",
    "    * verify or proof: d/dX F(G(X)) = d/dX G(X) d/dU F(U)  (double check this and learn more about this)\n",
    "\n",
    "* Jacobian and Gradient relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652a32e",
   "metadata": {},
   "source": [
    "   Refs:\n",
    "   * https://github.com/dair-ai/Mathematics-for-ML\n",
    "     1. https://arxiv.org/pdf/1802.01528.pdf <== this looks good\n",
    "        * The gradient of neuron activation  <= **start to proof some rules I found it on deep learning and loss equations**\n",
    "        * https://en.wikipedia.org/wiki/Matrix_calculus  has all the same definitions\n",
    "        * Limit definition\n",
    "          * https://mathinsight.org/differentiability_multivariable_definition\n",
    "          * https://mathinsight.org/differentiability_multivariable_subtleties\n",
    "          * https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative \n",
    "     2. http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf has derivative cheat sheet at the end that is handy\n",
    "     3. https://www.youtube.com/playlist?list=PLiiljHvN6z1_o1ztXTKWPrShrMrBLo5P3\n",
    "     4. https://zulko.wordpress.com/2012/04/15/symbolic-matrix-differentiation-with-sympy/ <= symbolic math in python\n",
    "     4. Proofs\n",
    "        1. https://math.stackexchange.com/questions/312077/differentiate-fx-xtax\n",
    "           1. good d/dx (X^T A X) he utilizes different methods and inclusive\n",
    "              f(x+h) <== that was cool\n",
    "           2. f(x+h) = (x + h)^t A (x+h) = X^TAX + X^TAH + H^TAX + H^TAH\n",
    "              1. = f(X) + X^TAX +  (A^TH)^TX + H^TAH  (using A is symetric A^T = A)\n",
    "              2. = f(X) + X^TAX +  (AH)^TX + H^TAH\n",
    "              3. = f(X) + X^TAX +  X^TAH + H^TAH  (Using H^T AH -> O)\n",
    "              4. f(X+H) - f(X) = 2 X^TA H   H^{-1}\n",
    "              5. d/dx F(X)= 2X^TA  c.q.d\n",
    "           3. Using A = I\n",
    "              1. d/dx F(X) = d(X^T X)/dx = 2X^T\n",
    "           4. Repeat by using deriving the elements of the matrix (use A as\n",
    "              3x3 matrix for simplification)\n",
    "        1. remember:\n",
    "           1. (AX)^T = (X^TA^T)\n",
    "              1. let A = W^T => (W^TX) = (X^TW)\n",
    "              2. also if A is symetric (A^T = A) => (AX)^T = (X^TA)\n",
    "           2. (X^TAX)^T = (BX)^T = (X^TB^T) = (X^T(X^TA)^T) = (X^T(A^TX)) = (X^T A^T X)\n",
    "              1. If A is symetric => (X^TAX)^T = (X^T A X) => (X^T A X) is also\n",
    "                 symetric\n",
    "           3. d/dX means derive of all x_i s\n",
    "              1. (df(X)/dX)^T = \\Delta f(x_1,x_2) is the transposed gradient\n",
    "                 of the function\n",
    "                 https://en.wikipedia.org/wiki/Matrix_calculus\n",
    "        1. Proof d(X^{-1})/dX = X^{-1}\n",
    "        1. d/dX (XY) = (d/dX X) Y + X (d/dX Y)\n",
    "        1. d/dX X^T = (d/dX X)^T\n",
    "        1. Verify for 1 and simple examples that Fr√©chet defintion of derivative is\n",
    "           equivalent to d/dX of the Jacobian (derive all x_is ) In the\n",
    "           wikipedia has a samll discussion about that in a section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07236a7",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e94d4",
   "metadata": {},
   "source": [
    "Matrix and matrix derivatives are an essential concept in machine learning and optimization problems. Often in machine learning,  we have to compute the derivative of a function with respect to a matrix variable. This notation and rules of matrix calculus can be challenging to grasp initially, but they offer a concise and efficient way to represent mathematical operations on matrices. See the examples bellow:\n",
    "\n",
    "* Simple matrix operations\n",
    "\n",
    "    * $\\sum_i x_i = 1^T X$\n",
    "    * $c_{ij}=\\sum_{k=1}^na_{ik}b_{kj} \\implies C = AB$ (Matrix multiplication)\n",
    "\n",
    "And the more complex examples\n",
    "\n",
    "* Gradient of a vector with respect to a matrix:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{F}(\\mathbf{x}) =  \\begin{bmatrix}\n",
    "f_1(\\mathbf{x}) \\\\\n",
    "f_2(\\mathbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "f_m(\\mathbf{x})\n",
    "\\end{bmatrix} \\\\ \n",
    "\\mathbf{J}(\\mathbf{x}) = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\begin{bmatrix}\n",
    "\\nabla f_1 \\\\\n",
    "\\nabla f_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\nabla f_m\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The notation of matrix calculus offers a concise and compact way of writing complex mathematical operations. Instead of writing out lengthy equations involving summations and nested brackets, we can use matrix calculus to represent these operations more efficiently. Furthermore, matrix calculus provides a framework for optimizing complex models, such as deep neural networks, which involve thousands or even millions of parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355da49",
   "metadata": {},
   "source": [
    "Here is a chatgpt cheat sheet for some common matrix derivatives:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf{A}}(\\mathbf{A}+\\mathbf{B}) = \\mathbf{I}$  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf{A}}(\\mathbf{AB}) = \\mathbf{B}^T$  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf{A}}(\\mathbf{A}^T) = \\mathbf{I}$  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf{A}}(\\mathbf{A}^{-1}) = -\\mathbf{A}^{-1}\\frac{\\partial \\mathbf{A}}{\\partial \\mathbf{A}}\\mathbf{A}^{-1}$  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf{A}}(\\text{tr}(\\mathbf{AB})) = \\mathbf{B}^T$  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf{A}}(\\text{tr}(\\mathbf{A}^T\\mathbf{BA})) = \\mathbf{B}(\\mathbf{A}+\\mathbf{A}^T)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49b921",
   "metadata": {},
   "source": [
    "**example of aplication: Cost function for regressor (MSE)**\n",
    "\n",
    "\n",
    "J(theta) = 1/2n (X Theta - y)^T (X Theta - y)\n",
    "\n",
    "A = (X Theta - y)\n",
    "\n",
    "d/dTheta J(X) = d/dTheta A d/dA A^T A\n",
    "\n",
    "= 1/2n X^T 2A = 1/n X^T (X Theta - y)\n",
    "\n",
    "======\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c709fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
