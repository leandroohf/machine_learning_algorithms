{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning: Batchnormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is a technique used in deep learning to improve the training of neural networks. The motivation behind batch normalization is to address the problem of **internal covariate shift**, which refers to the phenomenon where the distribution of the inputs to a layer of a neural network changes as the parameters of the previous layers are updated during training. By normalizing the inputs of the layer, batch normalization reduces the amount of internal covariate shift and improves the stability of the network. This can result in faster training, better performance, and more robust models.\n",
    "\n",
    "Batch normalization works by normalizing the activations input of each layer to have **zero mean and unit variance**, and then applying a scale and shift parameter to the normalized values.\n",
    "`\n",
    "Batch normalization has become a standard technique in deep learning and is used in many state-of-the-art models.\n",
    "\n",
    "\n",
    "Motivations:\n",
    "\n",
    "\n",
    "* Reduces the amount of internal covariate shift and improves the stability of the network. This leads to better performance, and more robust models.\n",
    "\n",
    "* Allows to use large mearning rates. This results in result in faster training\n",
    "\n",
    "<img src=\"images/bn-v0.png\" style=\"transform: scale(0.85);\" alt=\"bn-comparison\">\n",
    "\n",
    "    * Legends:\n",
    "        * BN-Baseline: learning rate reference.\n",
    "        * BN-x5: Initial learning rate of 0.0075 (5 times Inceptionâ€™s learning rate).\n",
    "        * BN-x30: Initial learning rate 0.045 (30 times that of Inception).\n",
    "        * BN-x5-Sigmoid: Uses Sigmoid activation function (non-linearity) instead of ReLU.\n",
    "        \n",
    "        \n",
    "     In the graph above you can see that using Batch normalization it is possible to train the DNN inspection with large learning rate and reach the same performance in less epochs. \n",
    "\n",
    "* Provides some level of regularization\n",
    "\n",
    "The forward equations belows is the batchnormalztion layer:\n",
    "\n",
    "$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^m x_i \\\\\n",
    "\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2  \\\\\n",
    "\\hat{z}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\n",
    "y_i = \\gamma \\hat{z}_i + \\beta\n",
    "$\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are the scaling and shift parameters determine during training.\n",
    "\n",
    "Backward equations: \n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{z}i} = \\frac{\\partial L}{\\partial y_i} \\cdot \\gamma \\\\\n",
    "\\frac{\\partial L}{\\partial \\sigma^2} = \\sum_{i=1}^m \\frac{\\partial L}{\\partial \\hat{z}i} \\cdot (x_i - \\mu) \\cdot \\frac{-1}{2} \\cdot (\\sigma^2 + \\epsilon)^{-3/2} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mu} = \\sum_{i=1}^m \\frac{\\partial L}{\\partial \\hat{z}i} \\cdot \\frac{-1}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{-2}{m} \\cdot \\sum_{i=1}^m (x_i - \\mu) \\\\\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat{x}i} \\cdot \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{2}{m} \\cdot (x_i - \\mu) + \\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{1}{m} \\\\\n",
    "\\frac{\\partial L}{\\partial \\gamma} = \\sum_{i=1}^m \\frac{\\partial L}{\\partial y_i} \\cdot \\hat{x}i \\\\\n",
    "\\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^m \\frac{\\partial L}{\\partial y_i}\n",
    "$\n",
    "\n",
    "where $L = L(\\hat{z}_i, \\mu, \\sigma^2,x_i,\\gamma, \\beta )$ is the loss function.\n",
    "\n",
    "\n",
    "The batch normalization layer usage:\n",
    "\n",
    "1. Use between the layer and the activion of the layer\n",
    "\n",
    "```python \n",
    "\n",
    "# NOTE: ok \n",
    "x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# NOTE: not reccommended \n",
    "x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "```\n",
    "\n",
    "1. Not ueful when using Stchastic Gradient descent. (The mean of 1 sample is the sample and the variance is zero) \n",
    "1. Canot be used together with dropout in the same layer block\n",
    "\n",
    "```python \n",
    "\n",
    "# NOTE: ok\n",
    "x = Conv2D(64, kernel_size=3, strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Dropout(0.30)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "\n",
    "# NOTE: not recommended\n",
    "x = Conv2D(64, kernel_size=3, strides=2, padding='same')(x)\n",
    "x = Dropout(0.30)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation forward equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnormalization(x, gamma , beta):\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    mu = np.mean(x,axis=0)\n",
    "    \n",
    "    sigma2 = np.var(x,axis=0)\n",
    "    \n",
    "    z = (x - mu) / np.sqrt(sigma2 + epsilon)\n",
    "    \n",
    "    bn =  gamma * z + beta\n",
    "    \n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([4.00790512e-16, 1.01862963e-15])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.normal(5.0,2.0,size=(100,2))\n",
    "\n",
    "x.shape\n",
    "\n",
    "gamma = 1.0\n",
    "beta = 0.00\n",
    "bn = batchnormalization(x, gamma , beta)\n",
    "\n",
    "bn.shape\n",
    "\n",
    "bn.mean(axis=0)\n",
    "bn.var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\n",
    "    * Authors: Sergey Ioffe (same author of PLDA and works at Google) n Christian Szegedy (google)\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf Paper **TODO** Read the paper. It is simple and easy to understand/ It is a good gain experience in reading paper  \n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization tensprflow doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "301.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
