{
 "cells": [
  {
   "metadata": {},
   "id": "d15fd8ee",
   "cell_type": "markdown",
   "source": "# Intro do Matrix calculus\n\n**WIP**\n\n**TODO**:\n* Still need to review.\n* there is a lot of errors\n* Also it is hard topic. Will save for later\n\n**Plan**\n\n* Keep 80/20 matrix rule (DONE)\n\n* Start with the derivatives I am familiar\n    1. scalar function and x scalar as Df = [f'(x)]\n    2. scalar function of 2 variables and the gradient\n    3. vector fucntion (vector field) and Df using vector bidimensional and 2 variables\n    4. examples to show\n        * Df = x^t A x = 2Ax (scalr = 2ax). For A is symetric\n        * what else ?\n    \n* Introduce derivative respect to a matrix (these ones are the ones it is not clear for me)\n    1. start f(X) = XW = Df =d/dX f = W^\\top\n        * explain denominator notation\n        * explain the intuition\n    1. d/dX X^\\top X = 2X (like 2x)\n    \n * Define the multiple regression problem in terms of matrix (DONE)\n \n * Define the loss MSE and minimize the loss Df (DONE)\n \nRefs:\n* http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf\n* https://towardsdatascience.com/analytical-solution-of-linear-regression-a0e870b038d5\n* https://towardsdatascience.com/vectorization-implementation-in-machine-learning-ca652920c55d\n* https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n\n## 80/20 Matrix Rules (for Machine Learning)\n\n1. $A B \\ne B A$ ‚Äî matrix multiplication is not commutative  \n2. $ A_{n \\times d} \\cdot B_{d \\times m} \\Rightarrow C_{n \\times m}$ ‚Äî inner dimensions must match  \n3. **Only square matrices have inverse (and it is not guaranteed) and identity matrix**\n4. $ (A^\\top)^\\top = A $ ‚Äî double transpose returns original  \n5. $ (AB)^\\top = B^\\top A^\\top $ ‚Äî transpose of a product reverses order  \n6. $ \\frac{d}{dX}(XW) = W^\\top $ ‚Äî key derivative in linear models and deep learning\n7. $\\frac{d}{dX}(X_{nxn}) = I_{nxn} = \\frac{d}{dX^\\top}(X^\\top_{nxn}) = I^\\top = I$\n"
  },
  {
   "metadata": {},
   "id": "e5a742a5",
   "cell_type": "markdown",
   "source": "## Matrix calculus"
  },
  {
   "metadata": {},
   "id": "5c3ab481",
   "cell_type": "markdown",
   "source": "* scalar function of one variable f = f(x)\n\n$\nDf = \\frac{df}{dx} = [f'(x)] \n$\n\n*  scalar function of 2 variablee f = f(x,y)\n\n$\nDf = \\nabla f = \n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n$\n\n* vector function (like vector field like magnetic field) f = f(x,y). where f is a column vector\n\n$\nf(x,y) =\n\\begin{bmatrix}\nf_1(x,y) \\\\\nf_2(x,y)\n\\end{bmatrix}\n$\n\n$\n\\nabla f_1^\\top =\n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y}\n\\end{bmatrix},\n\\quad\n\\nabla f_2^\\top =\n\\begin{bmatrix}\n\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n\\end{bmatrix}\n$\n\n\n$\nDf = \n\\begin{bmatrix}\n\\nabla f_1^\\top \\\\\n\\nabla f_2^\\top\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n\\end{bmatrix}\n$\n\nFor example.\n\n* lets the vector function defined by:\n\n$\nf(v) = Av \n=\n\\begin{bmatrix}\nf_1(x,y) \\\\\nf_2(x,y)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na_1 x & a_2 y \\\\\na_3 x & a_4 y\n\\end{bmatrix}\n$\n\nwhere $v = [x y]^\\top$ and $A$:\n\n$\nA =\n\\begin{bmatrix}\na_1 & a_2 \\\\\na_3 & a_4\n\\end{bmatrix}\n$\n\nLets\n\n$\nDf = \n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n\\end{bmatrix}\n= \n\\begin{bmatrix}\na1 & a2 \\\\\na_3 & a_4\n\\end{bmatrix}\n= A\n$\n\nSimilar to $\\frac{d}{dx} ax = a$ for a scalar function of one variable.\n\n* let $A$ a symetric matric and $f$ a scalar function defined by\n\n$\nA =\n\\begin{bmatrix}\na_1 & a \\\\\na   & a_2\n\\end{bmatrix}\n$\n\n$\nf(v) = v^\\top A v = \nf(x,y) = a_1x^2 + 2 a xy + a_2 y^2\n$\n\n$\nDf = \\nabla f = \n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n=\n=\n\\begin{bmatrix}\n2a_1 x + 2a y \\\\\n2a x + 2a_2 y\n\\end{bmatrix}\n$\n\nIsolating the symetric matrix $A$\n\n$\nDf = \n\\begin{bmatrix}\n2a_1 x + 2a y \\\\\n2a x + 2a_2 y\n\\end{bmatrix}\n$ \n\nRearrange=ing the terms\n$\n=\n2\n\\begin{bmatrix}\na_1  + a  \\\\\na  + a_2 \n\\end{bmatrix}\n\\begin{bmatrix}\nx  \\\\\ny \n\\end{bmatrix} \\\\\n= 2Ax\n$\n\nAgain, similar to $\\frac{d}{dx} ax^2 = 2ax$"
  },
  {
   "metadata": {},
   "id": "d363ddfd",
   "cell_type": "markdown",
   "source": "* matrix funtion $f$ by matrix $X$\n\n**THIS IS THE ONE I AM NOT FAMILIAR**\n**TODO**:\n* consider only in the context of ML where X is data and nxd and d is the\n* Using the results $\\frac{d}{dX} XA = A^\\top$ derive\n    * $\\frac{d}{dX} XA = I$\n    * $\\frac{d}{dX^\\top} AX^\\top = A$\n* examples is DONE:\n    1. $f(X) = XW$ (TODO)\n    1. $f(X) = X^\\top X$ (TODO. Iam not understanding this)"
  },
  {
   "metadata": {},
   "id": "f96a4257",
   "cell_type": "markdown",
   "source": "---\n\nIn the field of Machine Learning, we typically work with:\n\n* $X_{nxd}$: each row is a sample, each column is a feature\n* $W_{dx1}$:  a column vector of weights or parameters\n* $y$: the actual values (targets), one per sample\n\nWe want to introduce the meaning of $\\frac{d}{dX} f(X)$ using this context. \nFirst, let‚Äôs begin with a scalar-valued function. **Loss functions** ($\\mathcal{L}$) in machine learning are scalar functions defined over matrices and vectors like X, W, and y.\n\nLet the scalar function\n$\n \\hat{y} = f(X) = XW \\\\ \n = f(x_1, x_2) = x_1 w_1 + x_2 w_2\n$\n\nWhere $X \\in \\mathbb{R}^{1 \\times 2}$ and $W \\in \\mathbb{R}^{2 \\times 1}$. This is a function of 2 variables. From calculus we have the gradient of this function\n\n$\n\\nabla f =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2}\n\\end{bmatrix}^\\top\n=\n\\begin{bmatrix}\nw_1 & w_2\n\\end{bmatrix}\n= W\n$\n\nIn machine learning, we generalize this gradient and write using the denominator notation:\n\n$\n\\frac{d}{dX} f(X) = W^\\top\n$\n\nBy convention is $W^\\top$ and not $W$ because $X$ is a row vectors and $\\frac{d f}{dX} \\in \\mathbb{R}^{1 \\times 2}$ is **derivative related to a row vector**. This row-gradient convention aligns with standard rules from:\n\n1. Linear algebra (dot product, inner product)\n1. Optimization (e.g. in gradient descent, the update step uses matching dimensions)\n1. Backpropagation (ensuring dimension consistency in **matrix chain rules**)\n\n**The gradient tells us how much $f$ change when we change the matrix $X$ element by small amount.** \n\nSecond, we gonna generalize a little bit the $\\frac{d}{dX}$ for vector valued function $y = f(X)$, where\n$y\\in \\mathbb{R}^{n \\times d}$.\n\nBecause $y$ is the colum vectors and lest assume $X$ only has 2 features. For instance\n**PAREI AQUI**\n\nTODO:\n1. intuition d/dX X^tX = 2X where X is a matrix (nxd)\n1. intuition of d/dX x^tAx = 2Ax, where x is a column vector (important as well)\n    * A is not symetric\n    * A is symetric (SVD and PCA case)\n\nSee: https://atmos.washington.edu/~dennis/MatrixCalculus.pdf\n\n$\nX = \\begin{bmatrix}\nx_{11} & x_{12} \\\\\nx_{21} & x_{22} \\\\\nx_{31} & x_{32}\n\\end{bmatrix}\n\\in \\mathbb{R}^{3 \\times 2}\n$\n\n$\nf(X) = X^\\top X =\n\\begin{bmatrix}\nf_{11}(x_{11}, x_{21}, x_{31}) & f_{12}(x_{11}, x_{12}, x_{21}, x_{22}, x_{31}, x_{32}) \\\\\nf_{21}(x_{11}, x_{12}, x_{21}, x_{22}, x_{31}, x_{32}) & f_{22}(x_{12}, x_{22}, x_{32})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_{11}^2 + x_{21}^2 + x_{31}^2 & x_{11}x_{12} + x_{21}x_{22} + x_{31}x_{32} \\\\\nx_{11}x_{12} + x_{21}x_{22} + x_{31}x_{32} & x_{12}^2 + x_{22}^2 + x_{32}^2\n\\end{bmatrix}\n$\n\nSo lets give meaning to $\\frac{d}{dX} f(X)$\n\n$\n\\frac{d}{dX} f(X) =\n\\begin{bmatrix}\n\\nabla f_{11} \\\\\n\\nabla f_{12} \\\\\n\\nabla f_{21} \\\\\n\\nabla f_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial f_{11}}{\\partial x_1} & \\frac{\\partial f_{11}}{\\partial x_2} \\\\\n\\frac{\\partial f_{12}}{\\partial x_1} & \\frac{\\partial f_{12}}{\\partial x_2} \\\\\n\\frac{\\partial f_{21}}{\\partial x_1} & \\frac{\\partial f_{21}}{\\partial x_2} \\\\\n\\frac{\\partial f_{22}}{\\partial x_1} & \\frac{\\partial f_{22}}{\\partial x_2}\n\\end{bmatrix}\n$\n\n$\n\\frac{d}{dX} f(X) = \\begin{bmatrix}\n2x_1 & 0 \\\\\nx_2 & x_1 \\\\\nx_2 & x_1 \\\\\n0 & 2x_2\n\\end{bmatrix}\n$\n\n**TODO**\n1. correct the equation above\n2. Start with f(X) = XW to introduce the meaning of d/dX\n3. Using example of X_{3x2} and f(X) = X^\\top X give meaning to d/dX.\n4. write the meaning in english. Means change all variables by a small amount. \n5. what is the relation with the gradient operator $\\nabla$\n6. Question. Can d/dX of X^\\top X be derived using the limit derivative definition?\n\nNotes about matrix derivatives\n\n$\n\\frac{\\partial}{ \\partial X} XA = A^\\top \\text{<== most common on machine learning} \\\\\n\\frac{\\partial}{ \\partial Z} AZ = A \\text{<== more common on statistics and math}\n$\n\nAn important relation for $Z = X^\\top$ is: **<== REVIEW**\n\n$\n\\frac{\\partial}{ \\partial X^\\top} AX^\\top = A \\\\\n\\frac{\\partial}{ \\partial X^\\top} AX^\\top = (A^\\top)^\\top \\\\ \n\\frac{\\partial}{ \\partial X^\\top} AX^\\top = \\left (\\frac{\\partial}{ \\partial X} XA \\right)^\\top  = A \n$\n\n$\n\\frac{\\partial}{ \\partial X} Z^{-1} = - Z^{-1} (\\frac{\\partial}{ \\partial x} Z) Z^{-1}  \\text{<== derivative of the inverse}\n$\n\n\nAnalogy with scalar\n\n$\n\\frac{\\partial}{ \\partial x} x^2 = 2x  \\text{ similar to ==> } \\frac{\\partial}{ \\partial x} X^tX \\equiv 2X  \\\\ \n\\frac{\\partial}{ \\partial x} bx = b \\text{ similar to ==> } \\frac{\\partial}{ \\partial x} XB = B^t\\\\\n\\frac{\\partial}{ \\partial x} bx^2 = 2bx \\text{ similar to ==> } \\frac{\\partial}{ \\partial x} X^tBX = 2BX\\\\ \n$\n\nProoving: $\\frac{d}{dX} X^\\top X = 2X$\n\nApplying the tranpose operator $\\frac{d}{dX} X^\\top$ to $X$:\n\n$\n\\frac{d}{dX} X^\\top (X) = \\left (\\frac{d}{dX} (X) X^\\top \\right) \\\\\n= I X^\\top = X^\\top\n$\n\nSo: \n\n$\n\\frac{d}{dX} X^\\top X = \\left (\\frac{d}{dX} X^\\top \\right) X + X^\\top \\frac{d}{dX} X \\\\\n= I X + X^\\top I\n$\n"
  },
  {
   "metadata": {},
   "id": "67a783f1",
   "cell_type": "markdown",
   "source": "## Multiple Linear regression matrix formulation"
  },
  {
   "metadata": {},
   "id": "2f3cb0b5",
   "cell_type": "markdown",
   "source": "* One sample 2 dimension\n\n$$\nf(x_1, x_2) = w_1 x_1 + w_2 x_2 + b\n$$\n\n$$\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1 & x_2\n\\end{bmatrix},\n\\quad\n\\mathbf{w} =\n\\begin{bmatrix}\nw_1 \\\\\nw_2\n\\end{bmatrix}\n$$\n\n$$\n\\quad\nf(\\mathbf{x}) = \\mathbf{x} \\mathbf{w} + b\n$$\n\n* n samples 2 columns (each row is one sample and cols are features)\n\n$$\nX =\n\\begin{bmatrix}\nx_{11} & x_{12} \\\\\nx_{21} & x_{22} \\\\\n\\vdots & \\vdots \\\\\nx_{n1} & x_{n2}\n\\end{bmatrix}\n\\in \\mathbb{R}^{n \\times 2},\n$$\n\n$$\n\\hat{\\mathbf{y}} = X \\mathbf{w} + b\n$$\n\n* n sample and d features:\n\n$$\nX \\in \\mathbb{R}^{n \\times d},\\quad W \\in \\mathbb{R}^{d \\times 1},\n$$\n\n$$\n\\hat{\\mathbf{y}} = XW + b\n$$\n\nWhere $\\hat{\\mathbf{y}}$ is  column vector with all sample estimations\n\n$$\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n\\hat{y}_{1} \\\\\n\\hat{y}_{2} \\\\\n\\vdots \\\\\n\\hat{y}_{n}\n\\end{bmatrix}\n$$\n\n* **Bias trick**: (It is quite common and easy to implement)\n\n$$\nX' = \\begin{bmatrix}\nX & \\mathbf{1}\n\\end{bmatrix}\n\\in \\mathbb{R}^{n \\times (d+1)},\n\\quad\nW' = \\begin{bmatrix}\nW \\\\\nb\n\\end{bmatrix}\n\\in \\mathbb{R}^{(d+1) \\times 1},\n$$\n\n$$\n\\hat{\\mathbf{y}} = X' W'\n$$\n\n* Deep learning (no activation)\n\n$$\nX \\in \\mathbb{R}^{n \\times d},\\quad W \\in \\mathbb{R}^{d \\times h},\n$$\n\n$$\n\\hat{\\mathbf{y}} = XW + b\n$$\n\n| **Layer Type**                | **Weight Shape (W)**                     | **Meaning**                                                    |\n|------------------------------|-------------------------------------------|----------------------------------------------------------------|\n| Output layer (scalar output) | `W ‚àà ‚Ñù^{d √ó 1}`                          | Dot product with feature vector to get scalar output           |\n| Hidden/Input layer                 | `W ‚àà ‚Ñù^{d √ó h}`                          | Projects input of size `d` to hidden size `h`                  |\n"
  },
  {
   "metadata": {},
   "id": "d7534840",
   "cell_type": "markdown",
   "source": "#  Linear Regression: Minimizing the loss function"
  },
  {
   "metadata": {},
   "id": "4081d4b1",
   "cell_type": "markdown",
   "source": "### üéØ Linear Regression: Loss Function and Optimal Weights\n* Loss Function: MSE\n$$\nJ(\\mathbf{w}) = \\frac{1}{2n} (X \\mathbf{w} - \\mathbf{y})^\\top (X \\mathbf{w} - \\mathbf{y})\n$$\n\n* Gradient of the Loss\n\nTaking the derivative w.r.t. $ \\mathbf{w}$:\n\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\frac{d}{dW} \\frac{1}{2n} (X \\mathbf{w} - \\mathbf{y})^\\top (X \\mathbf{w} - \\mathbf{y}) \\\\\n = \\frac{1}{2n} \\frac{d}{dW} (X \\mathbf{w} - \\mathbf{y})^\\top (X \\mathbf{w} - \\mathbf{y}) \\\\\n = \\frac{1}{2n} \\frac{d}{dW} (X \\mathbf{w} - \\mathbf{y}) 2 (X \\mathbf{w} - \\mathbf{y}) \\\\\n = \\frac{1}{n} (X^\\top)(X \\mathbf{w} - \\mathbf{y})  = 0 \\\\\n$$\n\n* solving for $\\mathbf{w}$.\n\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = 0 \\quad \\Rightarrow \\quad X^\\top X \\mathbf{w} = X^\\top \\mathbf{y} \\\\\n\\mathbf{w}^* = (X^\\top X)^{-1} X^\\top \\mathbf{y}\n$$"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}