{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Refs:\n",
    "* Book with good math in deep learning: http://d2l.ai/index.html (Authors from Amazon; I personaly liked the section about MLE)\n",
    "\n",
    "* THis is a good ref form Stanfort: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks\n",
    "\n",
    "* See this discussion how to diagnose deep learning curves: https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\n",
    "\n",
    "\n",
    "Refactor in sectiosn\n",
    "\n",
    "1. DNN main concpets\n",
    "\n",
    "    * neurons and archicteture\n",
    "    * activations (Stanford cheat sheet pictures) \n",
    "\n",
    "1. Loss \n",
    "\n",
    "    * main losses, tf code and when to use\n",
    "    * Add the discussion best constant prediction (regression) that depends of the loss (See coursera how to winn kaggle competition)\n",
    "\n",
    "\n",
    "1. Optmizer and Gradient desctent \n",
    "\n",
    "    **Just the big picture. I will implement the algs in separate notebook**\n",
    "\n",
    "    * Batch gradient descent \n",
    "        * update $\\theta_j$ only after an epoch\n",
    "        * The computation of the gradient $- \\nabla J(\\theta)$ is precise and always point to the direction of fast decrease of the loss function\n",
    "        * Always utse all the data, so is memory hungry\n",
    "        * can explore vectorization\n",
    "    * stochastic gradient descent\n",
    "        * update $\\theta_j$ every single data $X^(i)$.\n",
    "        * The computation of the gradient $- \\nabla J(\\theta)$ is not always good, sometimes point to the wrong direction. Guarantee that in the average the loss function is minimized\n",
    "        * Faster  than Batch Gradient because update more times per epoch \n",
    "        * Do not use too much memorry\n",
    "        * ca NOT explore vectorization\n",
    " \n",
    "    * mini-btach (Review deeplearning.ai videos and notes ) \n",
    "        * update $\\theta_j$ many times during an epoch. \n",
    "        * The computation of the gradient $- \\nabla J(\\theta)$ is good ok but not precise\n",
    "        * Faster than all algorithms, because updates the arameters more often and also explore vectorization \n",
    "        * Do not use too much memorry\n",
    "        * Explore vectorization\n",
    "        * Relations with the others algs:\n",
    "            * batch size = M: batch gradient descent\n",
    "            * batch size = 1: stochastinc gradient descent \n",
    "        \n",
    "https://www.youtube.com/watch?v=-_4Zi8fCZO4\n",
    "\n",
    "Discusion about the algs . maybe better in ds_prgamatic notebook \n",
    "* implement only mini-batch gradient descent. The others you can set bacth size: m for Btach and 1 for Stochastic.\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
    "\n",
    "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\n",
    "\n",
    "https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/\n",
    "\n",
    "https://github.com/bhattbhavesh91/gradient-descent-variants/blob/master/gradient-descent-variants-notebook.ipynb\n",
    "\n",
    "\n",
    "Very good video about NN , back propagation and its intuition\n",
    "At 11:27 also show implementations in pyhton good for references.\n",
    "This code can be found in the book  http://neuralnetworksanddeeplearning.com/\n",
    "https://www.youtube.com/watch?v=Ilg3gGewQ5U\n",
    "\n",
    "book: http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent\n",
    "\n",
    "    * Momentum in some optmizer \n",
    "\n",
    "https://distill.pub/2017/momentum/\n",
    "\n",
    "1. Learning curves diagnostics \n",
    "    \n",
    "    * under fitting , right and overfitting\n",
    "    * Add this Unknown fit - Validation error low, training error 'high' (shold review your data, wrongl labels in train, low diversity in validation, duplicated data in validation n etc)\n",
    "    * train and loss in sync signal that you are goodand not overfitting\n",
    "    * learning rate diagnostics big vs too small and ideal\n",
    "    * Batch size trade off \n",
    "        * loss oscilating \n",
    "\n",
    "Add this discussion with real learning curves\n",
    "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\n",
    "\n",
    "1. Recipe to train\n",
    "\n",
    "    * General advices\n",
    "    * Become one withh data (data descriptive and know your data)\n",
    "    * Train strategy \n",
    "        1. Reduce bias error first \n",
    "        1. Then reduce variance \n",
    "        1. Squezze the juice \n",
    "    * Tunning\n",
    "\n",
    "1. How to verify and check your code\n",
    "\n",
    "    * overfit toy sample as small as 2 sample\n",
    "    * verify initial loss\n",
    "    * verify if you increase regularization the initial loss increase as well\n",
    "    * shuffle labels and make sure now DNN is not learning (learning rate will be very slow compared to no shuffe labels)\n",
    "\n",
    "\n",
    "refs:\n",
    "* Coursera. deeplearning.ai\n",
    "* https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\n",
    "* https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning\n",
    "* https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras  as K\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN main concepts \n",
    "\n",
    "\n",
    "ANN can learning non-linear relationships \n",
    "\n",
    "<img src=\"images/non-linear_and_linear_decision_edge.png\" width=\"400\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "#### Neuron\n",
    "\n",
    "<img src=\"images/neuron_ANN.png\" width=\"400\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation func (allow ANN to learn no-linear relationships) \n",
    "\n",
    "Good activatiobn functions has non-linear shapes, easy to compute the function and the first derivative of the function\n",
    "\n",
    "* **sigmoid function**: most common function. widely used on logiostic regression  \n",
    "     * (range: 0.0 and 1.0)  \n",
    "     * good for binnary classifiers (output layer) \n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z} } \\\\\n",
    "g'(z) = g(z) (1 - g(z)) \\\\\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "$\n",
    "g'(x) = \\frac{d}{dx}(1 + e^{-x})^{-1} \\\\\n",
    "= \\frac{(-1)}{(1 + e^{-x})^{2}}(-1)e^{-x}  \\\\\n",
    "= g(x)\\frac{e^{-x}}{(1 + e^{-x})} \\\\\n",
    "= g(x)\\frac{1+ e^{-x} - 1}{(1 + e^{-x})} \\\\\n",
    "= g(x)(\\frac{1+ e^{-x}}{(1 + e^{-x})} - \\frac{1}{(1 + e^{-x})}) \\\\ \n",
    "g'(x) = g(x)(1-g(x))\n",
    "$\n",
    "\n",
    "\n",
    "<img src=\"images/sigmoid.png\" width=\"200\" align=\"left\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tanh: hyperbolic tangent   \n",
    "\n",
    "* range: -1.0 and 1.0  \n",
    "* mean value is zero. This is good in optimization problems (remember why we should normalize the input features)  \n",
    "*  good for hidden layers  \n",
    "\n",
    "$$\n",
    "g(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z} } \\\\\n",
    "g'(z) = (1 - g(z)^2) \\\\\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Remember hyperboic functions\n",
    "$\n",
    "cosh(x) \\frac{e^x + e^{-x}}{2} \\\\\n",
    "sinh(x) \\frac{e^x - e^{-x}}{2} \\\\\n",
    "cosh^2(x) - sinh^2(x) = 1 \\\\\n",
    "\\frac{d}{dx} cosh(x) = sinh(x) \\\\\n",
    "\\frac{d}{dx} sinh(x) = cosh(x) \\\\\n",
    "$\n",
    "\n",
    "So:\n",
    "\n",
    "$\n",
    "\\frac{d}{dx} tanh(x) = \\frac{d}{dx}\\frac{sinh(x)}{cosh(x)}\n",
    "$\n",
    "\n",
    "Using $(u/v)'= \\frac{u'v - v'u}{v^2}$\n",
    "\n",
    "$\n",
    "= \\frac{sinh'(x)cosh(x) - cosh'(x)sinh(x)}{cosh^2(x)} \\\\\n",
    "= \\frac{cosh^2(x) - sinh^2(x)}{cosh^2(x)} \\\\\n",
    "= \\frac{1 - sinh^2(x)}{cosh^2(x)} \\\\\n",
    "= 1- g^2(x) \n",
    "$\n",
    "\n",
    "This is also true. But not useful for deep learning\n",
    "\n",
    "$\n",
    "\\frac{d}{dx} tanh(x) = \\frac{d}{dx}\\frac{sinh(x)}{cosh(x)} \\\\\n",
    "= \\frac{cosh^2(x) - sinh^2(x)}{cosh^2(x)} \\\\\n",
    "= \\frac{1}{cosh^2(x)} = sech^2(x)\n",
    "$\n",
    "\n",
    "<img src=\"images/tanh.png\" width=\"200\" align=\"left\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another demonstration:\n",
    "\n",
    "$\n",
    "\\frac{d}{dz} tanh(z) = \\frac{d}{dz} \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \\\\\n",
    "$\n",
    "\n",
    "Using $(u/v)'= \\frac{u'v - v'u}{v^2}$\n",
    "\n",
    "$\n",
    "\\frac{d}{dz} tanh(z) = \\frac{(e^{z} + e^{-z})d/dz (e^{z} - e^{-z})+ (e^{z} - e^{-z}d/dz(e^{z} + e^{-z}))}\n",
    "    {(e^{z} + e^{-z})^2} \\\\\n",
    "= \\frac{(e^{z} + e^{-z})(e^{z} + e^{-z})}{(e^{z} + e^{-z})^2)}-\n",
    "\\frac{(e^{z} - e^{-z})(e^{z} - e^{-z})}{(e^{z} + e^{-z})^2) }\\\\\n",
    "= 1 - tanh^2(z)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLu: \n",
    "\n",
    "* range: 0  and inf   \n",
    "* good for hidden layers\n",
    "* very common. make train fast\n",
    "\n",
    "$$\n",
    "g(z) = max(0,z) \\\\\n",
    "$$\n",
    "$$\n",
    "g'(z) =\n",
    "\\begin{cases} \n",
    "0 & \\text{if  }  z < 0 \\\\\n",
    "1 & \\text{if  }  z > 0 \\\\\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "<img src=\"images/relu.png\" width=\"200\" align=\"left\"/>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3 See it in tensorflow\n",
    "x = np.linspace(-1.0,1.0,100)\n",
    "\n",
    "g = tf.nn.relu(x)\n",
    "y = g.numpy()\n",
    "\n",
    "plt.plot(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax: \n",
    "\n",
    "ref: https://cs231n.github.io/neural-networks-case-study/#grad\n",
    "\n",
    "$$\n",
    "\\frac{e^{s_i}}{\\sum_j^m e^{s_j}}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of class\n",
    "\n",
    "* range: vector where each element is between 0.0 and 1.0. There is nclass elements in the vector output\n",
    "* softmax enhance the difference between the score\n",
    "* good for multi-class classificatio (output layer)\n",
    "* emphsaize the most likely class and return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layers  \n",
    "\n",
    "<img src=\"images/layers.jpeg\" width=\"400\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "**cross-entropy loss** or **or log loss**: measure the performance of classifier where the outputs ranging between 0.0 and 1.0 \n",
    "\n",
    "Cross-entropy loss increases as the predicted probability diverges from the actual label\n",
    "Is the average of log likelihood over all the data\n",
    "\n",
    "* Forward Propagation: computes the output given an input. (used in train and prediction phase)\n",
    "\n",
    "* Back-propagation: computes the gradiens in order to train the model while the ANN is learning. Only used in train phase\n",
    "\n",
    "#### Loss types\n",
    "\n",
    "**TODO** \n",
    "1. Intuition and interpretation\n",
    "2. When to use? It is better for?\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Use the loss function (approx. 1 line)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "\n",
    "```\n",
    "\n",
    "$$- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opitmizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train recipe\n",
    "\n",
    "refs: \n",
    "* http://karpathy.github.io/2019/04/25/recipe/\n",
    "* http://cs231n.github.io/neural-networks-3/#baby\n",
    "\n",
    "1. General advices\n",
    "\n",
    "    * fast n furious approach does not work\n",
    "    * patient and pay attention in detail tends to work (correlates with success)\n",
    "    * being defensive and obsessed about visualizations works\n",
    "    * do baby steps and avoid to **introduce a lot of unverified complexity at once**\n",
    "    * Build simple to complex\n",
    "    * ReLu are good for hidden layers\n",
    "        * Positive side learn faster than logistic and tanh due to the slope\n",
    "        * The Negative side can make train stuck caused by dead neurons where the gradient becomes zero\n",
    "    \n",
    "1. Become one with data \n",
    "\n",
    "    * inspect data\n",
    "    * try to see patterns (your brain is good at it)\n",
    "    * always check for:\n",
    "        * duplicated \n",
    "        * corrupted data\n",
    "        * wrong labels (if not systemic may not hurt to much)\n",
    "        * imbalance data\n",
    "        * **outliers** caa cause gradient exploding\n",
    "        \n",
    "1. Set up pipeline for trainning and evaluations and test it\n",
    "\n",
    "    * work with fixed seed \n",
    "    * simplify. does not add any regularization \n",
    "    *  **verify loss init**: Ex.  -log(1/n_classes) on a softmax at initialization. We can be derived for L2 regression, Huber losses, etc. **<= Andrej Karptahy Director of Tesla recommends that**  \n",
    "        \n",
    "        * violation indicates poor weight initialization or data is not normalized properly (double check this about normalization)\n",
    "        * **Also check if you include regularization and dropout layers that increases inital loss** \n",
    "        \n",
    "    * verify if you increase regularization the initial loss increase as well\n",
    "    \n",
    "    * **overfit one batch or small train sample dataset as little as 2**\n",
    "    * **input independent** (shuffles labels) (the DNN should not learn. See the errors in test n val dataset)\n",
    "    * visualize the input of DNN. y_hat = model(X). Vis X.  \n",
    "\n",
    "1. Overfit (reduce bias error)\n",
    "\n",
    "    * overfit\n",
    "        * focus in **train loss** should be close to zero\n",
    "        * if you try with many models  that you increased the complexity can suggest that you have a BUG\n",
    "    * do not be a hero. start with the most related paper and copy and paste their simple architecture.\n",
    "        * for images, ResNet-50 is a good start\n",
    "        * for voice, xvectors\n",
    "    * **Adam is safe with learning rate e3-4** !? but you can try different learning rate.\n",
    "    * **Add complexity only one at time**. If you have multiple features. Suggest to add one by one and unsure you get a performance boost. Or you can try smaller image and the increase the image size \n",
    "    * **do not trust learning rate decay**. He always disable learning rate decays entirely. It is a personal advice. less problematic maybe\n",
    "\n",
    "1. Regularize (reduce variance error)\n",
    "\n",
    "    * **get more data** is by the far preferred way to regularize a model. It is **the only guarantee way to improve the model.**\n",
    "    * **data augmentation**. The next best thing\n",
    "    * **pretrain**. It is really rarely hurts to use a pre-trained network even if you have enough data.\n",
    "        * xvectors\n",
    "        * ResNet-50\n",
    "    * **make smaller input dimensionality**. Remove features that can have spurious signal (Remove ciorrelated features)\n",
    "    * **make smaller model**. Personal advise\n",
    "        * He used to use FC layers after ImageNet, but these days he uses average pooling. eliminating a tons of parameters\n",
    "    * **decrease batch size**. helps with regularization\n",
    "    * **dropout**\n",
    "    * **early stopping**\n",
    "    \n",
    "1. Tune\n",
    "\n",
    "    * **random over grid search** Never use grid serach\n",
    "    * **hyper-parameter optmization**\n",
    "    \n",
    "    \n",
    "1. Squeeze the juice (It is not preference)\n",
    "\n",
    "    * leave it training. One time he forgot one model running and get SOTA (state of the art) !?\n",
    "    * ensembles\n",
    "        * **TODO**: read this paper about hot to use ensemble to build one simple model: https://arxiv.org/abs/1503.02531\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Verify loss initialization\n",
    "\n",
    "\n",
    "You initial loss and metric should be close to the one expected by random guess\n",
    "\n",
    "\n",
    "* categorical cross entropy\n",
    "    \n",
    "\n",
    "For K classes and N samples  \n",
    "\n",
    "$\n",
    "L(\\hat{y},y) = - \\frac{1}{N} \\sum_i^N  \\sum_k^K \\left [ y_i^k ln(\\hat{y}_i^k) \\right ]\n",
    "$\n",
    "\n",
    "where \n",
    "\n",
    "$\n",
    "y_i^k = 1 \\ \\mbox{If sample i belongs to class k } \\\\\n",
    "y_i^k = 0  \\ \\mbox{otherwise } \\\\\n",
    "$\n",
    "\n",
    "and $\\hat{y}_i^k$ is the model probability prediction. For random guess you expect $\\hat{y}_i^k = 1/K$ for all classes\n",
    "\n",
    "So the expected initial loss is $ L_0(\\hat{y},y) = -ln(1/K)$\n",
    "\n",
    ">PS: You need a good sample size in order to see that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train.shape\n",
    "x_test.shape\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/1 - 0s - loss: 2.2788 - accuracy: 0.0640\n",
      "Initial loss: 2.37; Expected loss: 2.30\n"
     ]
    }
   ],
   "source": [
    "## Initila loss\n",
    "sample_size = 5000\n",
    "\n",
    "K = 10.00 # 10 digtis\n",
    "expected_loss = - np.log(1.0/K)\n",
    "\n",
    "res = model.evaluate(x_test[0:sample_size],  y_test[0:sample_size], verbose=2)\n",
    "loss0 = res[0]\n",
    "acc0 = res[1]\n",
    "\n",
    "\n",
    "print('Initial loss: {0:.2f}; Expected loss: {1:.2f}'.format(loss0, expected_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate diagnostics \n",
    "\n",
    "* refs:\n",
    "    \n",
    "    * https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10   (approach to detrmine best lr)\n",
    "    * https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance trade-off \n",
    "\n",
    "**Train error still to much high for the application**\n",
    "\n",
    "<img src=\"images/biasvariance.png\" height=\"250\" width=\"400\">\n",
    "\n",
    "**variance error is related to gap between train and error** \n",
    "\n",
    "There is a minimum total error\n",
    "\n",
    "<img src=\"images/irr_error.png\" height=\"250\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High bias\n",
    "\n",
    "\n",
    "* left: high bias and low variance \n",
    "* right: high bias and high variance\n",
    "\n",
    "What to do:\n",
    "\n",
    "* Adding more training instances.\n",
    "* Adding more features.\n",
    "* Feature selection.\n",
    "* Hyperparameter optimization\n",
    "* train longer (deep learning)\n",
    "\n",
    "<img src=\"images/add_data.png\" height=\"400\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low bias high variance error\n",
    "\n",
    "\n",
    "* left: low variance \n",
    "* right: high variance\n",
    "\n",
    "What to do?\n",
    "\n",
    "* Adding more training instances.  \n",
    "\n",
    "* Increase the regularization for our current learning algorithm. This should decrease the variance and increase the bias.  \n",
    "\n",
    "    * L1 or L2\n",
    "    * dropout\n",
    "\n",
    "* Reducing the numbers of features in the training data we currently use. The algorithm will still fit the training data very well, but due to the decreased number of features, it will build less complex models. This should increase the bias and decrease the variance. \n",
    "\n",
    "\n",
    "<img src=\"images/low_high_var.png\" height=\"400\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates \n",
    "\n",
    "Learning rate controls how much we are adjusting the weights of our network with respect the gradient of the loss function. \n",
    "\n",
    "\n",
    "<img src=\"images/learning_rate.png\" height=\"400\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Too small: **Less training time, lesser money spent on GPU cloud compute. :)**\n",
    "* Too large: does not converge\n",
    "\n",
    "<img src=\"images/learning_rate2.png\" height=\"200\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a methodology to detrmine best learning rate?\n",
    "\n",
    "In the article **Cyclical Learning Rates for Training Neural Networks\"\"** Leslie N. Smith argued that you could estimate a good learning rate by training the model initially with a very low learning rate and increasing it (either linearly or exponentially) at each iteration.\n",
    "\n",
    "\n",
    "1. change the learningrate at each minibatch (lienarlly or exponentially)\n",
    "1. plot the learning rate (log) against loss; (choose the one close to the minumum)\n",
    "\n",
    "\n",
    "**The python package fastai has function to do that** fastai is like keras for pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips n learning curve diagnostics\n",
    "\n",
    "* https://stats.stackexchange.com/questions/345990/why-does-the-loss-accuracy-fluctuate-during-the-training-keras-lstm\n",
    "\n",
    "* https://stats.stackexchange.com/questions/187335/validation-error-less-than-training-error \n",
    "\n",
    "**Summary**\n",
    "\n",
    "* val loss or error smaller than train reasons\n",
    "    * diff in train and val data distributions. \n",
    "        * Maybe train has harder case while validation has more easy cases.\n",
    "        * data wrongly labeledin train datasets\n",
    "        * dropouts (highe level) can cause that sometimes\n",
    "\n",
    "* loss oscilation reasons\n",
    "    1. batch_size is too small\n",
    "    1. large neural network and small data  (**always compare #parmeters and #samples**)\n",
    "    \n",
    "    \n",
    "* Batch size trade off (alsoe related to previous one)\n",
    "    * too large make training slow\n",
    "    * too small loss oscilation and takes more epoch to converge\n",
    "    * large batch (**small number of mini batches** per epoch) size can make the DNN not learn\n",
    "\n",
    "You can think of model evaluation in four different categories:\n",
    "\n",
    "1. Underfitting – Validation and training error high\n",
    "\n",
    "1. Overfitting – Validation error is high, training error low\n",
    "\n",
    "1. Good fit – Validation error low, slightly higher than the training error\n",
    "\n",
    "1. Unknown fit - Validation error low, training error 'high'\n",
    "\n",
    "\n",
    "I say 'unknown' fit because the result is counter intuitive to how machine learning works. The essence of ML is to predict the unknown. If you are better at predicting the unknown than what you have 'learned', AFAIK the data between training and validation must be different in some way. \n",
    "\n",
    "\n",
    "\n",
    ">There are several reasons that can cause fluctuations in training loss over epochs. The main one though is the fact that almost all neural nets are trained with different forms of stochastic gradient decent. This is why batch_size parameter exists which determines how many samples you want to use to make one update to the model parameters. If you use all the samples for each update, you should see it decreasing and finally reaching a limit. Note that there are other reasons for the loss having some stochastic behavior.\n",
    ">\n",
    ">This explains why we see oscillations. But in your case, it is more that normal I would say. Looking at your code, I see two possible sources.\n",
    ">\n",
    ">Large network, small dataset: It seems you are training a relatively large network with 200K+ parameters with a very small number of samples, ~100. To put this into perspective, you want to learn 200K parameters or find a good local minimum in a 200K-D space using only 100 samples. Thus, you might end up just wandering around rather than locking down on a good local minima. (The wandering is also due to the second reason below).\n",
    ">\n",
    ">Very small batch_size. You use very small batch_size. So it's like you are trusting every small portion of the data points. Let's say within your data points, you have a mislabeled sample. This sample when combined with 2-3 even properly labeled samples, can result in an update which does not decrease the global loss, but increase it, or throw it away from a local minima. When the batch_size is larger, such effects would be reduced. Along with other reasons, it's good to have batch_size higher than some minimum. Having it too large would also make training go slow. Therefore, batch_size is treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithms\n",
    "\n",
    "refs: https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent\n",
    "\n",
    "$\n",
    "J(\\theta)=\\frac{1}{2}\\sum_{i=1}^N(y_i−h_{\\theta}(x_i)^2\n",
    "$\n",
    "\n",
    "$\n",
    "\\theta_j = \\theta_j − \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} \n",
    "$\n",
    "\n",
    "The update is given by \n",
    "\n",
    "**REVIEW. THIS EQUATION IS NOT RIGHT**   \n",
    "$\n",
    "\\alpha \\nabla_{\\theta} J(\\theta) = \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} \\equiv \\alpha \\sum_{i=1}^N(y_i−h_{\\theta}(x_i))x_i\n",
    "$\n",
    "\n",
    "\n",
    "where $\\alpha$ is learning rate.\n",
    "\n",
    "Comparison between Gradient, Sthocasting Gradient and mini-batch Gradient descent:\n",
    "\n",
    "Let think how to estimate the gradient \n",
    "$\n",
    "\\nabla_{\\theta}J(\\theta|X) = f(\\theta)\n",
    "$\n",
    "\n",
    "\n",
    "1. **Gradient descent**  \n",
    "\n",
    "\n",
    "$\n",
    "\\nabla_{\\theta}J(\\theta|X) = f(\\theta)\n",
    "$    \n",
    "\n",
    "$X$ is all data, Utilizes a lot of memory ans it is slow. \n",
    "\n",
    "\n",
    "    * Compute the gradient of the cost function using the entire dataset \n",
    "    * Update the weights.\n",
    "    \n",
    "    \n",
    "    Pros n cons  \n",
    "    \n",
    "    * **Computational slow and utilizes a lot of memory**  \n",
    "    * Guarantee that loss func always will reduce   \n",
    "    \n",
    "\n",
    "1. **Stochastic Gradient Descent**\n",
    "\n",
    "$\n",
    "\\nabla_{\\theta}J(\\theta|X) = f(\\theta)\n",
    "$    \n",
    "\n",
    "$X$ is one sample data $X ={x_i}$. Where $bs$ is batch size. Consume more memory than Sthocastica but less than Gradient. Easy pararlelization. Up to now is the faster and most popular implementation.\n",
    "\n",
    "    * Compute gradient for each sample\n",
    "    \n",
    "    Pros n cons  \n",
    "    \n",
    "    * More sensible to noisy  \n",
    "    * Faster than Gradient decsent  \n",
    "    * Use less memmory   \n",
    "\n",
    "\n",
    "1. **Mini batch Gradient**  \n",
    "\n",
    "$\n",
    "\\nabla_{\\theta}J(\\theta|X) = f(\\theta)\n",
    "$    \n",
    "\n",
    "$X$ is a subset of the $X = \\{x_1,x_2,x_3, ..., x_{bs} \\}$. Does not use too much memory and tends to be fast then Gradient Descent\n",
    "\n",
    "    * Compute gradient for each mini batch (This is a estimation of the true Gradient )  \n",
    "\n",
    "    Pros n Cons  \n",
    "    \n",
    "    * More robust to noisys data\n",
    "    * Faster than all methods\n",
    "    * Use less memory than Gradient but more than Stochastic\n",
    "\n",
    "\n",
    "See this discusison for batches sizes:\n",
    "* https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why\n",
    "\n",
    ">  the minibatch size gets larger the convergence of SGD actually gets harder/worse,\n",
    "\n",
    "* Paper: https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/\n",
    "\n",
    ">  large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization.  \n",
    "\n",
    "* https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network/236393#236393 \n",
    "\n",
    "> It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
